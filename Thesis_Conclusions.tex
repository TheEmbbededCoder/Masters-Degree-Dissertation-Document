%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                      %
%     File: Thesis_Conclusions.tex                                     %
%     Tex Master: Thesis.tex                                           %
%                                                                      %
%     Author: Francisco Mendes                                         %
%     Last modified :  3 Oct 2020                                      %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusions}
\label{chapter:conclusions}

The main question that steered this master thesis's development was "How can we improve the energy-efficiency of already deployed \acrshort{gpu}s?". This question guided all possible solutions to creating and improving the software stack that controls these devices, considering that introducing or modifying already deployed hardware is generally not possible.

The current literature on this topic presents a panoply of strategies to achieve this objective, ranging from improving the device execution units utilization, optimizing the number of threads per block, or performing the computations with reduced precision operands, to tunning the applied voltage-frequency levels to the current \acrshort{gpu} state and running application. The followed path was to further investigate this last option, targetting the improvement of current \acrshort{gpu}s \acrshort{dvfs} systems implementations. In fact, enhancing the \acrshort{dvfs} systems allows to benefit a broader range of both devices and applications, since the improvement does not come from optimizing the application to the target hardware, but instead, from better matching the hardware to the running application.

At this point, the state of the art implementations of \acrshort{dvfs} systems make use of many techniques to choose the most appropriate \textit{default} V-F pair for the current device state. However, one could go one step back and argue that the manufacturer's chosen V-F configurations do not allow for extracting the best energy-efficiency of \acrshort{gpu}s. Having this premise in mind, the first developed task devised a methodology to analyze the non-conventional V-F configuration's impact on the different architectural components and \acrshort{dvfs} domains. The methodology was tested on two architectural different AMD \acrshort{gpu}s with great success. Both devices allow for up to 21\% voltage reduction, depending on the tested frequency and stressed component. Hence, by running all the benchmarks that compose the methodology was possible to define an usable execution space, consisting of a new set of V-F pairs that, even though they are not the default ones, they do not compromise the \acrshort{gpu} architecture. Moreover, the use of those non-conventional V-F configurations allowed for a decreasing the device's energy consumption of up to 40\% without (in some cases, it can even improve) penalizing the performance. This also resulted in energy-efficiency gains, measured through the \acrshort{edp} metric, of up to 43\%.

After demonstrating that such configurations are so beneficial, the next step was to create a V-F optimization mechanism that explores all those new V-F pairs and, without testing all of them, find and set the most appropriate one for the running application and current \acrshort{gpu} state. This iterative optimization mechanism takes advantage of the natural code repetition pattern found in most \acrshort{gpgpu} applications to be continually monitoring, adjusting and selecting the fittest V-F configuration.

Deep learning applications, and more specifically, convolutional neural networks (\acrshort{cnn}s), were chosen as a target application to demonstrate the added benefits of defining a usable execution space that comprises non-conventional V-F pairs, with the devised iterative V-F optimization mechanism.
This application was first demonstrated with the execution of the two most energy-consuming \acrshort{cnn} layers, to assess the validity of non-conventional V-F pairs in concrete applications. Naturally, this algorithms' usable execution space corresponds to the one exhibited by the component that they stress the most. Second, the computational errors that emerge when running at the minimum allowed voltage, $V_{min}$, were studied by comparing the layer output when running conventional V-F pairs versus the non-conventional ones. It was observed that the single output maximum relative error did not exceed 0.14\% and that, even in these circumstances, up to 99.99\% of the results are shown to be accurate. Finally, the training procedure of four \acrshort{cnn} models were executed while exploring the usable execution space, observing no relevant effect on the final testing accuracy being measured when undervolting the \acrshort{gpu} core. Moreover, it was observed that using the novel V-F pairs yields an \acrshort{edp} improvement of up to 44\% over the regular automatic  \acrshort{dvfs} system of the Vega 10  \acrshort{gpu}. 
With the adaptation of the V-F mechanism to the training procedure of the \acrshort{cnn}, the discovery of the configuration that achieves the best energy efficiency is performed automatically. 

The adoption of the devised automatic optimization mechanism, which builds on the benefits of using non-conventional V-F pairs, is this dissertation answer to the objective of improving the energy-efficiency of already deployed \acrshort{gpu}s.






% ----------------------------------------------------------------------
\section{Future Work}
\label{section:future}

Two significant advancements to the state of the art are put forward by this thesis, contributing positively to increase the energy-efficiency of \acrshort{gpu}s. However, the devised methodologies still have room to be further explored.

Even though the current AMD software stack, ROCm, allows for independent control of frequency and voltage, the tool is not made to allow a proper dynamic voltage and frequency scaling controlled by the user. Instead, it is targeted to allow some degree of freedom, but to still be the automatic system in control of those two variables. If other manufacturers (for example, NVIDIA) start allowing what AMD already offers, a substantial benefit could already be put forward. Nevertheless, one further step could be made by developing a control \acrshort{api} that allows developers to hint the \acrshort{dvfs} system through CUDA, HIP or OpenCL directives. By doing so, researchers could have more freedom to experiment and create novel solutions.

The proposed methodology to characterize the architecture to non-conventional V-F scaling could be further expanded by going one step below and one step above. Going one step below, other new kernels to characterize the architecture in more detail could be added to the methodology. These would use Assembly calls to determine which operations inside each architectural component are actually limiting the usable exploration space. Going one step above, the exploration of the V-F configurations for each benchmark could be done more intelligently, by creating an algorithm that would indicate which configurations to test (in the current implementation, it is necessary to examine all possible configurations).

The devised V-F execution mechanism already considers the application performance and energy-consuming characteristics to choose the most appropriate V-F pair. However, the generation of the V-F pair to be tested is randomly determined. By monitoring each iteration of the user application's performance counters, such information could be used to guide the generation of the new V-F configuration. To allow that, a model that relates the performance counters results with the observed performance and energy-consuming behavior to non-conventional V-F scaling needed to be added to the current optimization mechanism.

