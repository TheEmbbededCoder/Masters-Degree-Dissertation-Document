
@inproceedings{mei_measurement_2013,
	title = {A measurement study of {GPU} {DVFS} on energy conservation},
	doi = {10.1145/2525526.2525852},
	abstract = {Nowadays, GPUs are widely used to accelerate many high performance computing applications. Energy conservation of such computing systems has become an important research topic. Dynamic voltage/frequency scaling (DVFS) is proved to be an appealing method for saving energy for traditional computing centers. However, there is still a lack of firsthand study on the effectiveness of GPU DVFS. This paper presents a thorough measurement study that aims to explore how GPU DVFS affects the system energy consumption. We conduct experiments on a real GPU platform with 37 benchmark applications. Our results show that GPU voltage/frequency scaling is an effective approach to conserving energy. For example, by scaling down the GPU core voltage and frequency, we have achieved an average of 19.28\% energy reduction compared with the default setting, while giving up no more than 4\% of performance. For all tested GPU applications, core voltage scaling is significantly effective to reduce system energy consumption. Meanwhile the effects of scaling core frequency and memory frequency depend on the characteristics of GPU applications.},
	booktitle = {Proceedings of the {Workshop} on {Power}-{Aware} {Computing} and {Systems}, {HotPower} 2013},
	author = {Mei, Xinxin and Yung, Ling and Zhao, Kaiyong and Chu, Xiaowen},
	month = nov,
	year = {2013}
}

@inproceedings{abe_power_2012,
	address = {Berkeley, CA, USA},
	series = {{HotPower}'12},
	title = {Power and {Performance} {Analysis} of {GPU}-accelerated {Systems}},
	abstract = {Graphics processing units (GPUs) provide significant improvements in performance and performance-per-watt as compared to traditional multicore CPUs. This energy-efficiency of GPUs has facilitated the use of GPUs in many application domains. Albeit energy efficient, GPUs consume non-trivial power independently of CPUs. Therefore, we need to analyze the power and performance characteristic of GPUs and their causal relation with CPUs in order to reduce the total energy consumption of the system while sustaining high performance. In this paper, we provide a power and performance analysis of GPU-accelerated systems for better understandings of these implications. Our analysis on a real system discloses that system energy can be reduced by 28\% retaining a decrease in performance within 1\% by controlling the voltage and frequency levels of GPUs. We show that energy savings can be achieved when GPU core and memory clock frequencies are appropriately scaled considering the workload characteristics. Another interesting finding is that voltage and frequency scaling of CPUs is trivial for total system energy reduction, and even should not be applied in state-of-the-art GPU-accelerated systems. We believe that these findings are useful to develop dynamic voltage and frequency scaling (DVFS) algorithms for GPU-accelerated systems.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2012 {USENIX} {Conference} on {Power}-{Aware} {Computing} and {Systems}},
	publisher = {USENIX Association},
	author = {Abe, Yuki and Sasaki, Hiroshi and Peres, Martin and Inoue, Koji and Murakami, Kazuaki and Kato, Shinpei},
	year = {2012},
	pages = {10--10}
}

@inproceedings{leng_gpuwattch_2013,
	address = {New York, NY, USA},
	series = {{ISCA} '13},
	title = {{GPUWattch}: {Enabling} {Energy} {Optimizations} in {GPGPUs}},
	isbn = {978-1-4503-2079-5},
	shorttitle = {{GPUWattch}},
	doi = {10.1145/2485922.2485964},
	abstract = {General-purpose GPUs (GPGPUs) are becoming prevalent in mainstream computing, and performance per watt has emerged as a more crucial evaluation metric than peak performance. As such, GPU architects require robust tools that will enable them to quickly explore new ways to optimize GPGPUs for energy efficiency. We propose a new GPGPU power model that is configurable, capable of cycle-level calculations, and carefully validated against real hardware measurements. To achieve configurability, we use a bottom-up methodology and abstract parameters from the microarchitectural components as the model's inputs. We developed a rigorous suite of 80 microbenchmarks that we use to bound any modeling uncertainties and inaccuracies. The power model is comprehensively validated against measurements of two commercially available GPUs, and the measured error is within 9.9\% and 13.4\% for the two target GPUs (GTX 480 and Quadro FX5600). The model also accurately tracks the power consumption trend over time. We integrated the power model with the cycle-level simulator GPGPU-Sim and demonstrate the energy savings by utilizing dynamic voltage and frequency scaling (DVFS) and clock gating. Traditional DVFS reduces GPU energy consumption by 14.4\% by leveraging within-kernel runtime variations. More finer-grained SM cluster-level DVFS improves the energy savings from 6.6\% to 13.6\% for those benchmarks that show clustered execution behavior. We also show that clock gating inactive lanes during divergence reduces dynamic power by 11.2\%.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 40th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Leng, Jingwen and Hetherington, Tayler and ElTantawy, Ahmed and Gilani, Syed and Kim, Nam Sung and Aamodt, Tor M. and Reddi, Vijay Janapa},
	year = {2013},
	keywords = {CUDA, GPU architecture, energy, power, power estimation},
	pages = {487--498}
}

@inproceedings{jiao_power_2010,
	title = {Power and {Performance} {Characterization} of {Computational} {Kernels} on the {GPU}},
	doi = {10.1109/GreenCom-CPSCom.2010.143},
	abstract = {Nowadays Graphic Processing Units (GPU) are gaining increasing popularity in high performance computing (HPC). While modern GPUs can offer much more computational power than CPUs, they also consume much more power. Energy efficiency is one of the most important factors that will affect a broader adoption of GPUs in HPC. In this paper, we systematically characterize the power and energy efficiency of GPU computing. Specifically, using three different applications with various degrees of compute and memory intensiveness, we investigate the correlation between power consumption and different computational patterns under various voltage and frequency levels. Our study revealed that energy saving mechanisms on GPUs behave considerably different than CPUs. The characterization results also suggest possible ways to improve the 'greenness' of GPU computing.},
	booktitle = {2010 {IEEE}/{ACM} {Int}'l {Conference} on {Green} {Computing} and {Communications} {Int}'l {Conference} on {Cyber}, {Physical} and {Social} {Computing}},
	author = {Jiao, Y. and Lin, H. and Balaji, P. and Feng, W.},
	month = dec,
	year = {2010},
	keywords = {CPU, Clocks, DVFS, Energy-Efficient Computing, GPU, GPUs, Graphics processing unit, Kernel, Memory management, Performance evaluation, Power demand, computational power, computer graphic equipment, coprocessors, energy conservation, energy efficiency, energy saving mechanism, graphic processing unit, high performance computing, low-power electronics, mainframes, power consumption},
	pages = {221--228}
}

@article{guerreiro_dvfs-aware_2019,
	title = {{DVFS}-aware application classification to improve {GPGPUs} energy efficiency},
	volume = {83},
	issn = {0167-8191},
	doi = {10.1016/j.parco.2018.02.001},
	abstract = {The increasing importance of GPUs as high-performance accelerators and the power and energy constraints of computing systems, make it fundamental to develop techniques for energy efficiency maximization of GPGPU applications. Among several potential techniques, dynamic voltage and frequency scaling (DVFS) stands out as one of the most promising approaches. Hence, novel DVFS-aware performance and power classification models are herein proposed that correlate application characteristics and GPU architecture features. In particular, by analysing the utilization of graphics and memory components at a single voltage and frequency levels, the proposed classification methodologies are able to predict the impact of DVFS on GPGPU applications execution time and power and energy consumption. The accuracy of the proposed approach is validated on two modern NVIDIA GPUs from the Maxwell and Pascal generations, by relying on 35 benchmarks from the Rodinia, Polybench, Parboil, SHOC and CUDA SDK suites. Experimental results show that the proposed approach can typically predict the optimal operating frequencies of graphics and memory subsystems, attaining up to 36\% energy savings (average of 16\%), which correspond to an average deviation of 0.74\% regarding the optimal case. Moreover, when considering a maximum performance penalty of 10\%, up to 26\% energy savings are still attained.},
	language = {en},
	urldate = {2020-05-24},
	journal = {Parallel Computing},
	author = {Guerreiro, João and Ilic, Aleksandar and Roma, Nuno and Tomás, Pedro},
	year = {2019},
	note = {Publisher: Elsevier},
	keywords = {Application classification, DVFS, Energy savings, GPGPU, Optimal frequency},
	pages = {93--117}
}

@inproceedings{kim_racing_2015,
	title = {Racing and {Pacing} to {Idle}: {Theoretical} and {Empirical} {Analysis} of {Energy} {Optimization} {Heuristics}},
	shorttitle = {Racing and {Pacing} to {Idle}},
	doi = {10.1109/CPSNA.2015.23},
	abstract = {The problem of minimizing energy for a performance constraint (e.g., Real-time deadline or quality-of-service requirement) has been widely studied, both in theory and in practice. Theoretical models have indicated large potential energy savings, but practical concerns have made these savings hard to realize. Instead, practitioners often rely on heuristic solutions, which achieve good results in practice but tend to be system-specific in efficacy. An example is the race-to-idle heuristic, which makes all resources available until a task completes and then idles. Theory predicts poor energy savings, but practitioners have reported good empirical results. To help bridge the gap between theory and practice, this paper presents a geometrical framework for analyzing the energy optimality of resource allocation under performance constraints. The geometry of the problem allows us to derive an optimal strategy and three commonly used heuristics: 1) race-to-idle, 2) pace-to-idle a near-optimal idling strategy, and 3) no-idle which never idles. We then implement all strategies and test them empirically for seven benchmarks on four different multicore systems, including both x86 and ARM. We find that race-to-idle is near optimal on older systems, but can consume as much as 3× more energy than the optimal strategy. In contrast, pace-to-idle is never more than 12\% worse than optimal.},
	booktitle = {2015 {IEEE} 3rd {International} {Conference} on {Cyber}-{Physical} {Systems}, {Networks}, and {Applications}},
	author = {Kim, David H.K. and Imes, Connor and Hoffmann, Henry},
	month = aug,
	year = {2015},
	keywords = {ARM, Energy consumption, Heuristic algorithms, Multicore processing, Optimization, Power demand, Program processors, Resource management, energy minimization, energy optimization heuristics, energy savings, geometrical framework, heuristic solutions, multicore systems, near-optimal idling strategy, no-idle heuristic, optimal strategy, pace-to-idle heuristic, performance constraints, power aware computing, race-to-idle heuristic, resource allocation, x86},
	pages = {78--85}
}

@inproceedings{hoffmann_racing_2013,
	address = {New York, NY, USA},
	series = {{HotPower} '13},
	title = {Racing and {Pacing} to {Idle}: {An} {Evaluation} of {Heuristics} for {Energy}-aware {Resource} {Allocation}},
	isbn = {978-1-4503-2458-8},
	shorttitle = {Racing and {Pacing} to {Idle}},
	doi = {10.1145/2525526.2525854},
	abstract = {We examine the problem of assigning computing resources to an application to meet a performance goal while minimizing energy consumption. We present a general formulation of this problem as a linear program, discuss several potential heuristic solutions, and evaluate these heuristics on two real systems (one purchased in 2010, the other in 2013). We find that the well-known race-to-idle heuristic is close to the optimal solution on the older machine. On the newer machine, however, the optimal solution outperforms race-to-idle by over 35\%. A generalization of race-to-idle, called pace-to-idle, is found to provide better results in a wider range of scenarios.},
	urldate = {2019-11-29},
	booktitle = {Proceedings of the {Workshop} on {Power}-{Aware} {Computing} and {Systems}},
	publisher = {ACM},
	author = {Hoffmann, Henry},
	year = {2013},
	keywords = {energy-aware computing, optimization, power-aware computing, resource allocation},
	pages = {13:1--13:5}
}

@article{da-ren_time_2014,
	title = {Time and {Energy} {Efficient} {DVS} {Scheduling} for {Real}-{Time} {Pinwheel} {Tasks}},
	volume = {12},
	issn = {1665-6423},
	doi = {https://doi.org/10.1016/S1665-6423(14)71663-3},
	abstract = {Dynamic voltage/frequency scaling (DVFS) is one of the most effective techniques for reducing energy use. In this paper, we focus on the pinwheel task model to develop a variable voltage processor with d discrete voltage/speed levels. Depending on the granularity of execution unit to which voltage scaling is applied, DVFS scheduling can be defined in two categories: (i) inter-task DVFS and (ii) intra-task DVFS. In the periodic pinwheel task model, we modified the definitions of both intra- and inter-task and design their DVFS scheduling to reduce the power consumption of DVFS processors. Many previous approaches have solved DVFS problems by generating a canonical schedule in advance and thus require pseudo polynomial time and space because the length of a canonical schedule depends on the hyperperiod of the task periods and is generally of exponential length. To limit the length of the canonical schedules and predict their task execution, tasks with arbitrary periods are first transformed into harmonic periods and their key features are profiled. The proposed methods have polynomial time and space complexities, and experimental results show that, under identical assumptions, the proposed methods achieve more energy savings than the previous methods.},
	number = {6},
	journal = {Journal of Applied Research and Technology},
	author = {Da-Ren, Chen and Young-Long, Chen and You-Shyang, Chen},
	year = {2014},
	keywords = {Dynamic voltage scaling, Hard real-time systems, Pinwheel tasks, Power-aware scheduling},
	pages = {1025 -- 1039}
}

@misc{chen_da-ren_time_nodate,
	title = {Time and {Energy} {Efficient} {DVS} {Scheduling} for {Real}-{Time} {Pinwheel} {Tasks} {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1665642314716633?token=A320D8E6AEACD94E4D0F78E52A78812476B17F7A403EEC03DC8E707CDC594C9935381917052C84FC19C8BABE752351B8},
	language = {en},
	urldate = {2019-12-04},
	author = {{Chen Da-Ren} and {Chen Young-Long} and {Chen You-Shyang}},
	doi = {10.1016/S1665-6423(14)71663-3}
}

@article{mei_survey_2017,
	title = {A {Survey} and {Measurement} {Study} of {GPU} {DVFS} on {Energy} {Conservation}},
	volume = {3},
	issn = {2352-8648},
	doi = {https://doi.org/10.1016/j.dcan.2016.10.001},
	abstract = {Energy efficiency has become one of the top design criteria for current computing systems. The dynamic voltage and frequency scaling (DVFS) has been widely adopted by laptop computers, servers, and mobile devices to conserve energy, while the GPU DVFS is still at a certain early age. This paper aims at exploring the impact of GPU DVFS on the application performance and power consumption, and furthermore, on energy conservation. We survey the state-of-the-art GPU DVFS characterizations, and then summarize recent research works on GPU power and performance models. We also conduct real GPU DVFS experiments on NVIDIA Fermi and Maxwell GPUs. According to our experimental results, GPU DVFS has significant potential for energy saving. The effect of scaling core voltage/frequency and memory voltage/frequency depends on not only the GPU architectures, but also the characteristic of GPU applications.},
	number = {2},
	urldate = {2019-10-23},
	journal = {Digital Communications and Networks},
	author = {Mei, Xinxin and Wang, Qiang and Chu, Xiaowen},
	year = {2017},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Dynamic voltage and frequency scaling, Energy efficiency, Graphics processing unit},
	pages = {89 -- 100}
}

@phdthesis{boyer_improving_2013,
	title = {Improving {Resource} {Utilization} in {Heterogeneous} {CPU}-{GPU} {Systems}},
	copyright = {All rights reserved (no additional license for public reuse)},
	abstract = {Graphics processing units (GPUs) have attracted enormous interest over the past decade due to substantial increases in both performance and programmability. Programmers can potentially leverage GPUs for substantial performance gains, but at the cost of significant software engineering effort. In practice, most GPU applications do not effectively utilize all of the available resources in a system: they either fail to use use a resource at all or use a resource to less than its full potential. This underutilization can hurt both performance and energy efficiency. In this dissertation, we address the underutilization of resources in heterogeneous CPU-GPU systems in three different contexts.

First, we address the underutilization of a single GPU by reducing CPU-GPU interaction to improve performance. We use as a case study a computationally-intensive video-tracking application from systems biology. Because of the high cost of CPU-GPU coordination, our initial, straightforward attempts to accelerate this application failed to effectively utilize the GPU. By leveraging some non-obvious optimization strategies, we significantly decreased the amount of CPU-GPU interaction and improved the performance of the GPU implementation by 26x relative to the best CPU implementation. Based on the lessons we learned, we present general guidelines for optimizing GPU applications as well as recommendations for system-level changes that would simplify the development of high-performance GPU applications.

Next, we address underutilization at the system level by using load balancing to improve performance. We propose a dynamic scheduling algorithm that automatically and efficiently divides the execution of a data-parallel kernel across multiple, possibly heterogeneous GPUs. We show that our scheduler can nearly match the performance of an unrealistic static scheduler when device performance is fixed, and can provide better performance when device performance varies.

Finally, we address underutilization within a GPU by using frequency scaling to improve energy efficiency. We propose a novel algorithm for predicting the energy-optimal GPU clock frequencies for an arbitrary kernel. Using power measurements from real systems, we demonstrate that our algorithm improves significantly on the state of the art across multiple generations of GPUs. We also propose and evaluate techniques for decreasing the CPU's energy consumption during GPU computation.

Many of the techniques presented in this dissertation can be used to improve the performance and energy efficiency of GPU applications with no programmer effort or software modifications required. As the diversity of available hardware systems continues to increase, automatic techniques such as these will become critical for software to fully realize the benefits of future hardware improvements.},
	language = {en},
	urldate = {2019-12-04},
	school = {University of Virginia},
	author = {Boyer, Michael},
	collaborator = {Skadron, Kevin},
	month = apr,
	year = {2013},
	doi = {10.18130/V3TJ7X}
}

@inproceedings{hong_integrated_2010,
	address = {New York, NY, USA},
	series = {{ISCA} '10},
	title = {An {Integrated} {GPU} {Power} and {Performance} {Model}},
	isbn = {978-1-4503-0053-7},
	doi = {10.1145/1815961.1815998},
	abstract = {GPU architectures are increasingly important in the multi-core era due to their high number of parallel processors. Performance optimization for multi-core processors has been a challenge for programmers. Furthermore, optimizing for power consumption is even more difficult. Unfortunately, as a result of the high number of processors, the power consumption of many-core processors such as GPUs has increased significantly. Hence, in this paper, we propose an integrated power and performance (IPP) prediction model for a GPU architecture to predict the optimal number of active processors for a given application. The basic intuition is that when an application reaches the peak memory bandwidth, using more cores does not result in performance improvement. We develop an empirical power model for the GPU. Unlike most previous models, which require measured execution times, hardware performance counters, or architectural simulations, IPP predicts execution times to calculate dynamic power events. We then use the outcome of IPP to control the number of running cores. We also model the increases in power consumption that resulted from the increases in temperature. With the predicted optimal number of active cores, we show that we can save up to 22.09\%of runtime GPU energy consumption and on average 10.99\% of that for the five memory bandwidth-limited benchmarks.},
	urldate = {2019-11-28},
	booktitle = {Proceedings of the 37th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Hong, Sunpyo and Kim, Hyesoon},
	year = {2010},
	keywords = {CUDA, GPU architecture, analytical model, energy, performance, power estimation},
	pages = {280--289}
}

@phdthesis{s_hong_modeling_2012,
	type = {{PhD} thesis},
	title = {Modeling performance and power for energy-efficient {GPGPU} computing},
	abstract = {The objective of the proposed research is to develop an analytical model that predicts performance and power for many-core architecture and further propose a mechanism, which leverages the analytical model, to enable energy-efficient execution of an application. The key insight of the model is to investigate and quantify a complex relationship that exists between the thread-level parallelism and memory-level parallelism for an application on a given many-core architecture. Two metrics are proposed: memory warp parallelism (MWP), which refers to the number of overlapping memory accesses per core, and computation warp parallelism (CWP), which characterizes an application type. By using these metrics in addition to the architectural and application parameters, the overall application performance is produced. The model uses statically-available parameters such as instruction-mixture information and input-data size, and the prediction accuracy is 13.3\% for the GPU-computing benchmarks. Another important aspect of using many-core architecture is reducing peak power and achieving energy savings. By using the proposed integrated power and performance (IPP) framework, the results showed that different optimization points exist for GPU architecture depending on the application type. The work shows that by activating fewer cores, 10.99\% of run-time energy consumption can be saved for the bandwidth-limited benchmarks, and a projection of 25.8\% energy savings is predicted when power-gating at core level is employed. Finally, the model is shifted to throughput using OpenCL for targeting more variety of processors. First, multiple outputs relating to performance are predicted, including upper-bound and lower-bound values. Second, by using the model parameters, an application can be categorized into a different category, each with its own suggestions for improving performance and energy efficiency. Third, the bandwidth saturation point accuracy is significantly improved by considering independent memory accesses and updating the performance model. Furthermore, a trade-off analysis using architectural and application parameters is straightforward, which provides more insights to improve energy efficiency. In the future, a computer system will contain hundreds of heterogeneous cores. Hence, it is mandatory that a workload gets scheduled to an efficient core or distributed on both types of cores. A preliminary work by using the analytical model to do scheduling between CPU and GPU is demonstrated in the appendix. Since profiling phase is not required, the kernel code can be transformed to run more efficiently on the specific architecture. Another extension of the work regarding the relationship between the speed-up and energy efficiency is mathematically derived. Finally, future research ideas are presented regarding the usage of the model for programmer, compiler, and runtime for future heterogeneous systems.},
	urldate = {2019-11-28},
	school = {Georgia Institute of Technology},
	author = {{S. Hong}},
	year = {2012}
}

@article{freijedo_modeling_2012,
	title = {Modeling the {Effect} of {Process}, {Power}-{Supply} {Voltage} and {Temperature} {Variations} on the {Timing} {Response} of {Nanometer} {Digital} {Circuits}},
	volume = {28},
	issn = {0923-8174, 1573-0727},
	doi = {10.1007/s10836-012-5297-0},
	language = {en},
	number = {4},
	urldate = {2020-08-06},
	journal = {Journal of Electronic Testing},
	author = {Freijedo, Judit F. and Semião, Jorge and Rodriguez-Andina, Juan J. and Vargas, Fabian and Teixeira, Isabel C. and Teixeira, J. Paulo},
	month = aug,
	year = {2012},
	pages = {421--434}
}

@incollection{wolpert_temperature_2012,
	address = {New York, NY},
	title = {Temperature {Effects} in {Semiconductors}},
	isbn = {978-1-4614-0747-8 978-1-4614-0748-5},
	shorttitle = {power},
	language = {en},
	urldate = {2020-08-06},
	booktitle = {Managing {Temperature} {Effects} in {Nanoscale} {Adaptive} {Systems}},
	publisher = {Springer New York},
	author = {Wolpert, David and Ampadu, Paul},
	collaborator = {Wolpert, David and Ampadu, Paul},
	year = {2012},
	doi = {10.1007/978-1-4614-0748-5_2},
	pages = {15--33}
}

@article{sapatnekar_what_2013,
	title = {What happens when circuits grow old: {Aging} issues in {CMOS} design},
	doi = {10.1109/VLSI-TSA.2013.6545621},
	abstract = {As CMOS technologies have shrunk to tens of nanometers, aging problems have emerged as a major challenge. There has been tremendous progress in developing new methods for modeling and diagnosing reliability at the level of individual transistors, but much less work on propagating these models to higher levels of abstraction to analyze and optimize the reliability of larger circuits. This talk will provide an introduction to various circuit aging mechanisms and will then discuss research that develops computer-aided design techniques for estimating and enhancing the reliability of large digital circuits, examining solutions that could practically be applied to analyze or improve the lifetime of a design while maintaining consistency to accurate device-level models and the associated physics.},
	language = {en},
	journal = {2013 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA)},
	author = {Sapatnekar, Sachin S},
	year = {2013},
	pages = {1--2}
}

@article{zhang_metal_2003,
	title = {Metal {Silicides} in {CMOS} {Technology}: {Past}, {Present}, and {Future} {Trends}},
	volume = {28},
	issn = {1040-8436},
	shorttitle = {Metal {Silicides} in {CMOS} {Technology}},
	url = {https://doi.org/10.1080/10408430390802431},
	doi = {10.1080/10408430390802431},
	abstract = {Metal silicides have played an indispensable role in the raped development of microelectronics since PtSi was first used to improve the rectifying characteristics of diodes in early 1960s. This work first provides a brief historical overview of the many different silicides, and, correspondingly, the different processing methodologies used in the past.With regard to the present use of silicides in CMOS technologies a convergence becomes clear with the self aligned technology using only a limited number of silicides, namely, TiSi2, CoSi 2, and NiSi. A section on fundamental aspects is included to cover thermodynamics and kinetics, which are essential for understanding the silicide formation processes. The advantage and disadvantages of TiSi2, CoSi 2, and NiSi are analyzed with the development trend of CMOS technologies as a measure. Specifically, the reactive diffusion and phase formation of these silicades in the three terminals of a metal-oxides-semiconductor device, that is, gate, source, and drain are scrutinized. The review ends with an extended discussion of about future trends of metal-silicides in micro/nanoelectronics, with reference to the potential material aspects and device structures outlined in the International Technologies Roadmap for Semiconductors.},
	number = {1},
	urldate = {2020-10-04},
	journal = {Critical Reviews in Solid State and Materials Sciences},
	author = {Zhang, Shi-Li and Östling, Mikael},
	month = nov,
	year = {2003},
	pages = {1--129}
}

@inproceedings{jing_energy-efficient_2013,
	address = {New York, NY, USA},
	series = {{ISCA} '13},
	title = {An {Energy}-efficient and {Scalable} {eDRAM}-based {Register} {File} {Architecture} for {GPGPU}},
	isbn = {978-1-4503-2079-5},
	url = {http://doi.acm.org/10.1145/2485922.2485952},
	doi = {10.1145/2485922.2485952},
	abstract = {The heavily-threaded data processing demands of streaming multiprocessors (SM) in a GPGPU require a large register file (RF). The fast increasing size of the RF makes the area cost and power consumption unaffordable for traditional SRAM designs in the future technologies. In this paper, we propose to use embedded-DRAM (eDRAM) as an alternative in future GPGPUs. Compared with SRAM, eDRAM provides higher density and lower leakage power. However, the limited data retention time in eDRAM poses new challenges. Periodic refresh operations are needed to maintain data integrity. This is exacerbated with the scaling of eDRAM density, process variations and temperature. Unlike conventional CPUs which make use of multi-ported RF, most of the RFs in modern GPGPU are heavily banked but not multi-ported to reduce the hardware cost. This provides a unique opportunity to hide the refresh overhead. We propose two different eDRAM implementations based on 3T1D and 1T1C memory cells. To mitigate the impact of periodic refresh, we propose two novel refresh solutions using bank bubble and bank walk-through. Plus, for the 1T1C RF, we design an interleaved bank organization together with an intelligent warp scheduling strategy to reduce the impact of the destructive reads. The analysis shows that our schemes present better energy efficiency, scalability and variation tolerance than traditional SRAM-based designs.},
	urldate = {2019-11-28},
	booktitle = {Proceedings of the 40th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Jing, Naifeng and Shen, Yao and Lu, Yao and Ganapathy, Shrikanth and Mao, Zhigang and Guo, Minyi and Canal, Ramon and Liang, Xiaoyao},
	year = {2013},
	pages = {344--355}
}

@inproceedings{zhang_approxann_2015,
	title = {{ApproxANN}: {An} approximate computing framework for artificial neural network},
	shorttitle = {{ApproxANN}},
	abstract = {Artificial Neural networks (ANNs) are one of the most well-established machine learning techniques and have a wide range of applications, such as Recognition, Mining and Synthesis (RMS). As many of these applications are inherently error-tolerant, in this work, we propose a novel approximate computing framework for ANN, namely ApproxANN. When compared to existing solutions, ApproxANN considers approximation for both computation and memory accesses, thereby achieving more energy savings. To be specific, ApproxANN characterizes the impact of neurons on the output quality in an effective and efficient manner, and judiciously determine how to approximate the computation and memory accesses of certain less critical neurons to achieve the maximum energy efficiency gain under a given quality constraint. Experimental results on various ANN applications with different datasets demonstrate the efficacy of the proposed solution.},
	booktitle = {Design, {Automation} {Test} in {Europe} {Conference} {Exhibition} 2015},
	author = {Zhang, Qian and Wang, Ting and Tian, Ye and Yuan, Feng and Xu, Qiang},
	month = mar,
	year = {2015},
	note = {ISSN: 1558-1101},
	keywords = {ApproxANN, Approximation methods, Artificial neural networks, Biological neural networks, Degradation, Energy consumption, Hardware, Neurons, approximate computing framework, artificial neural network, energy efficiency gain, neural nets, power aware computing, quality constraint},
	pages = {701--706}
}

@article{shrestha_review_2019,
	title = {Review of {Deep} {Learning} {Algorithms} and {Architectures}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2912200},
	abstract = {Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
	journal = {IEEE Access},
	author = {Shrestha, Ajay and Mahmood, Ausif},
	year = {2019},
	keywords = {Computer architecture, DL, Deep learning, Feature extraction, Feedforward neural networks, Machine learning algorithm, Recurrent neural networks, Training, artificial intelligence, backpropagation, cancer diagnosis, classification systems, convolution neural network, deep architectures, deep convolution networks, deep learning algorithms, deep neural network, deep neural network architectures, deep residual networks, large-sized data sets, learning (artificial intelligence), multidimensional training data, neural nets, optimisation, optimization, optimization methods, painstakingly handcrafted feature extractors, pattern recognition systems, precision medicine, predictive forecasting, recurrent neural networks, reinforcement learning, self-driving cars, speech recognition, supervised and unsupervised learning, variational autoencoders},
	pages = {53040--53065}
}

@article{zagoruyko_wide_2017,
	title = {Wide {Residual} {Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
	urldate = {2020-10-12},
	journal = {arXiv:1605.07146 [cs]},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = jun,
	year = {2017},
	note = {arXiv: 1605.07146},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105}
}

@incollection{krizhevsky_imagenet_2012-1,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2020-10-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {2D shape variability, Character recognition, Feature extraction, GTN, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis, back-propagation, backpropagation, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, performance measure minimization, segmentation recognition},
	pages = {2278--2324}
}

@misc{noauthor_gradient-based_nodate,
	title = {Gradient-based learning applied to document recognition - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore.ieee.org/document/726791},
	urldate = {2020-10-12}
}

@article{lecun_gradient-based_1998-1,
	title = {Gradient-{Based} {Learning} {Applied} to {Document} {Recognition}},
	language = {en},
	author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
	year = {1998},
	pages = {46}
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2020-10-05},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2020-10-05},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536}
}

@article{rumelhart_learning_1986-1,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2020-10-05},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Number: 6088
Publisher: Nature Publishing Group},
	pages = {533--536}
}

@article{rumelhart_learning_1986-2,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2020-10-05},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Number: 6088
Publisher: Nature Publishing Group},
	pages = {533--536}
}

@article{vaughan_simultaneous_2005,
	title = {Simultaneous {Generalized} {Hill}-{Climbing} {Algorithms} for {Addressing} {Sets} of {Discrete} {Optimization} {Problems}},
	volume = {17},
	issn = {1091-9856, 1526-5528},
	url = {http://pubsonline.informs.org/doi/10.1287/ijoc.1040.0064},
	doi = {10.1287/ijoc.1040.0064},
	language = {en},
	number = {4},
	urldate = {2020-10-01},
	journal = {INFORMS Journal on Computing},
	author = {Vaughan, Diane E. and Jacobson, Sheldon H. and Hall, Shane N. and McLay, Laura A.},
	month = nov,
	year = {2005},
	pages = {438--450}
}

@article{kirkpatrick_optimization_1983,
	title = {Optimization by {Simulated} {Annealing}},
	volume = {220},
	url = {http://science.sciencemag.org/content/220/4598/671.abstract},
	doi = {10.1126/science.220.4598.671},
	abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
	number = {4598},
	journal = {Science},
	author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
	month = may,
	year = {1983},
	pages = {671}
}

@misc{noauthor_optimization_nodate,
	title = {Optimization by {Simulated} {Annealing} {\textbar} {Science}},
	url = {https://science.sciencemag.org/content/220/4598/671},
	urldate = {2020-09-30}
}

@inproceedings{jain_performance_2019,
	title = {Performance {Characterization} of {DNN} {Training} using {TensorFlow} and {PyTorch} on {Modern} {Clusters}},
	doi = {10.1109/CLUSTER.2019.8891042},
	abstract = {The recent surge of Deep Learning (DL) models and applications can be attributed to the rise in computational resources, availability of large-scale datasets, and accessible DL frameworks such as TensorFlow and PyTorch. Because these frameworks have been heavily optimized for NVIDIA GPUs, several performance characterization studies exist for GPU-based Deep Neural Network (DNN) training. However, there exist very few research studies that focus on CPU-based DNN training. In this paper, we provide an in-depth performance characterization of state-of-the-art DNNs such as ResNet(s) and Inception-v3/v4 on multiple CPU architectures including Intel Xeon Broadwell, three variants of the Intel Xeon Skylake, AMD EPYC, and NVIDIA GPUs like K80, P100, and V100. We provide three key insights: 1) Multi-process (MP) training should be used even for a single-node, because the single-process (SP) approach cannot fully exploit all the cores, 2) Performance of both SP and MP depend on various features such as the number of cores, the processes per node (ppn), and DNN architecture, and 3) There is a non-linear and complex relationship between CPU/system characteristics (core-count, ppn, hyper-threading, etc) and DNN specifications such as inherent parallelism between layers. We further provide a comparative analysis for CPU and GPU-based training and profiling analysis for Horovod. The fastest Skylake we had access to is up to 2.35× better than a K80 GPU but up to 3.32× slower than a V100 GPU. For ResNet-152 training, we observed that MP is up to 1.47× faster than SP and achieves 125× speedup on 128 Skylake nodes.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Jain, Arpan and Awan, Ammar Ahmad and Anthony, Quentin and Subramoni, Hari and Panda, Dhableswar K. DK},
	month = sep,
	year = {2019},
	note = {ISSN: 2168-9253},
	keywords = {AMD EPYC, CPU-based DNN training, Central Processing Unit, Computational modeling, DNN Training, DNN architecture, GPU-based deep neural network training, Graphics processing units, Horovod, Intel Xeon Broadwell, Intel Xeon Skylake, K80 GPU, MP, MVAPICH2 MPI, Multicore processing, NVIDIA GPU, Parallel processing, Performance Characterization, PyTorch, ResNet-152 training, SP, Skylake nodes, TensorFlow, Training, V100 GPU, computational resources, graphics processing units, hyper-threading, in-depth performance characterization, large-scale datasets, learning (artificial intelligence), modern clusters, multiple CPU architectures, multiprocess training, multiprocessing systems, neural nets, parallel architectures, performance characterization studies, single-node, single-process approach, state-of-the-art DNNs},
	pages = {1--11}
}

@misc{lecun_yann_and_cortes_corinna_mnist_1999,
	title = {{MNIST} handwritten digit database, {Yann} {LeCun}, {Corinna} {Cortes} and {Chris} {Burges}},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2019-12-25},
	author = {LeCun, Yann {and} Cortes, Corinna},
	year = {1999}
}

@article{noauthor_notitle_nodate
}

@article{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	language = {en},
	author = {Krizhevsky, Alex},
	year = {2009},
	pages = {32--35}
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, ImageNet database, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine, computer vision, image resolution, image retrieval, large-scale hierarchical image database, large-scale ontology, multimedia computing, multimedia data, ontologies (artificial intelligence), subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255}
}

@inproceedings{kim_flipping_2014,
	address = {Minneapolis, MN, USA},
	title = {Flipping bits in memory without accessing them: {An} experimental study of {DRAM} disturbance errors},
	isbn = {978-1-4799-4394-4 978-1-4799-4396-8},
	shorttitle = {Flipping bits in memory without accessing them},
	url = {http://ieeexplore.ieee.org/document/6853210/},
	doi = {10.1109/ISCA.2014.6853210},
	abstract = {Memory isolation is a key property of a reliable and secure computing system — an access to one memory address should not have unintended side eﬀects on data stored in other addresses. However, as DRAM process technology scales down to smaller dimensions, it becomes more diﬃcult to prevent DRAM cells from electrically interacting with each other. In this paper, we expose the vulnerability of commodity DRAM chips to disturbance errors. By reading from the same address in DRAM, we show that it is possible to corrupt data in nearby addresses. More speciﬁcally, activating the same row in DRAM corrupts data in nearby rows. We demonstrate this phenomenon on Intel and AMD systems using a malicious program that generates many DRAM accesses. We induce errors in most DRAM modules (110 out of 129) from three major DRAM manufacturers. From this we conclude that many deployed systems are likely to be at risk. We identify the root cause of disturbance errors as the repeated toggling of a DRAM row’s wordline, which stresses inter-cell coupling eﬀects that accelerate charge leakage from nearby rows. We provide an extensive characterization study of disturbance errors and their behavior using an FPGA-based testing platform. Among our key ﬁndings, we show that (i) it takes as few as 139K accesses to induce an error and (ii) up to one in every 1.7K cells is susceptible to errors. After examining various potential ways of addressing the problem, we propose a low-overhead solution to prevent the errors.},
	language = {en},
	urldate = {2020-08-11},
	booktitle = {2014 {ACM}/{IEEE} 41st {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Kim, Yoongu and Daly, Ross and Kim, Jeremie and Fallin, Chris and Lee, Ji Hye and Lee, Donghyuk and Wilkerson, Chris and Lai, Konrad and Mutlu, Onur},
	month = jun,
	year = {2014},
	pages = {361--372}
}

@inproceedings{leng_gpuvolt_2014,
	address = {La Jolla, California, USA},
	title = {{GPUVolt}: modeling and characterizing voltage noise in {GPU} architectures},
	isbn = {978-1-4503-2975-0},
	shorttitle = {{GPUVolt}},
	url = {http://dl.acm.org/citation.cfm?doid=2627369.2627605},
	doi = {10.1145/2627369.2627605},
	abstract = {Voltage noise is a major obstacle in improving processor energy e ciency because it necessitates large operating voltage guardbands that increase overall power consumption and limit peak performance. Identifying the leading root causes of voltage noise is essential to minimize the unnecessary guardband and maximize the overall energy e ciency. We provide the ﬁrst-ever modeling and characterization of voltage noise in GPUs based on a new simulation infrastructure called GPUVolt. Using it, we identify the key intracore microarchitectural components (e.g., the register ﬁle and special functional units) that signiﬁcantly impact the GPU’s voltage noise. We also demonstrate that intercore-aligned microarchitectural activity detrimentally impacts the chipwide worst-case voltage droops. On the basis of these ﬁndings, we propose a combined register-ﬁle and execution-unit throttling mechanism that smooths GPU voltage noise and reduces the guardband requirement by as much as 29\%.},
	language = {en},
	urldate = {2020-08-08},
	booktitle = {Proceedings of the 2014 international symposium on {Low} power electronics and design - {ISLPED} '14},
	publisher = {ACM Press},
	author = {Leng, Jingwen and Zu, Yazhou and Rhu, Minsoo and Gupta, Meeta and Reddi, Vijay Janapa},
	year = {2014},
	pages = {141--146}
}

@article{chen_predicting_1997,
	title = {Predicting {CMOS} speed with gate oxide and voltage scaling and interconnect loading effects},
	volume = {44},
	issn = {00189383},
	url = {http://ieeexplore.ieee.org/document/641365/},
	doi = {10.1109/16.641365},
	number = {11},
	urldate = {2020-08-06},
	journal = {IEEE Transactions on Electron Devices},
	author = {Chen, K. and Hu, C. and Fang, P. and Lin, M.R. and Wollesen, D.L.},
	month = nov,
	year = {1997},
	pages = {1951--1957}
}

@article{schemmert_threshold-voltage_1974,
	title = {Threshold-voltage sensitivity of ion-implanted m.o.s. transistors due to process variations},
	volume = {10},
	issn = {00135194},
	url = {https://digital-library.theiet.org/content/journals/10.1049/el_19740115},
	doi = {10.1049/el:19740115},
	language = {en},
	number = {9},
	urldate = {2020-08-06},
	journal = {Electronics Letters},
	author = {Schemmert, W. and Zimmer, G.},
	year = {1974},
	pages = {151}
}

@article{schemmert_threshold-voltage_1974-1,
	title = {Threshold-voltage sensitivity of ion-implanted m.o.s. transistors due to process variations},
	volume = {10},
	issn = {0013-5194},
	doi = {10.1049/el:19740115},
	abstract = {Adjustment of the threshold voltage VT by ion implantation yields a certain distribution of threshold voltages determined by different process parameters. A procedure is presented for minimising the threshold-voltage sensitivity of implanted m.o.s. transistors due to these parameters for a typical set of process parameters.},
	number = {9},
	journal = {Electronics Letters},
	author = {Schemmert, W. and Zimmer, G.},
	month = may,
	year = {1974},
	note = {Conference Name: Electronics Letters},
	keywords = {FET, MOS, field effect transistors, ion implantation, sensitivity, threshold voltage sensitivity},
	pages = {151--152}
}

@article{papadimitriou_exceeding_2020,
	title = {Exceeding {Conservative} {Limits}: {A} {Consolidated} {Analysis} on {Modern} {Hardware} {Margins}},
	volume = {20},
	issn = {1558-2574},
	shorttitle = {Exceeding {Conservative} {Limits}},
	doi = {10.1109/TDMR.2020.2989813},
	abstract = {Modern large-scale computing systems (data centers, supercomputers, cloud and edge setups and high-end cyber-physical systems) employ heterogeneous architectures that consist of multicore CPUs, general-purpose many-core GPUs, and programmable FPGAs. The effective utilization of these architectures poses several challenges, among which a primary one is power consumption. Voltage reduction is one of the most efficient methods to reduce power consumption of a chip. With the galloping adoption of hardware accelerators (i.e., GPUs and FPGAs) in large datacenters and other large-scale computing infrastructures, a comprehensive evaluation of the safe voltage reduction levels for each different chip can be employed for efficient reduction of the total power. We present a survey of recent studies in voltage margins reduction at the system level for modern CPUs, GPUs and FPGAs. The pessimistic voltage guardbands inserted by the silicon vendors can be exploited in all devices for significant power savings. On average, voltage reduction can reach 12\% in multicore CPUs, 20\% in manycore GPUs and 39\% in FPGAs.},
	number = {2},
	journal = {IEEE Transactions on Device and Materials Reliability},
	author = {Papadimitriou, George and Chatzidimitriou, Athanasios and Gizopoulos, Dimitris and Reddi, Vijay Janapa and Leng, Jingwen and Salami, Behzad and Unsal, Osman Sabri and Kestelman, Adrian Cristal},
	month = jun,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Device and Materials Reliability},
	keywords = {6G mobile communication, Artificial intelligence, Benchmark testing, FAA, FPGA, Iron, Materials reliability, Three-dimensional displays, Voltage margins, accelerators, energy efficiency, many-core GPU, multicore CPU, power consumption},
	pages = {341--350}
}

@article{stachowski_autotuning_2020,
	title = {Autotuning based on frequency scaling toward energy efficiency of blockchain algorithms on graphics processing units},
	issn = {0920-8542, 1573-0484},
	url = {http://link.springer.com/10.1007/s11227-020-03263-5},
	doi = {10.1007/s11227-020-03263-5},
	abstract = {Energy-efficient computing is especially important in the field of high-performance computing (HPC) on supercomputers. Therefore, automated optimization of energy efficiency during the execution of a compute-intensive program is desirable. In this article, a framework for the automatic improvement of the energy efficiency on NVIDIA GPUs (graphics processing units) using dynamic voltage and frequency scaling is presented. As application, the mining of crypto-currencies is used, since in this area energy efficiency is of particular importance. The framework first determines the energy-optimal frequencies for each available currency on each GPU of a computer automatically. Then, the mining is started, and during a monitoring phase it is ensured that always the most profitable currency is mined on each GPU, using optimal frequencies. Tests with different GPUs show that the energy efficiency, depending on the GPU and the currency, can be increased by up to 84\% compared to the usage of the default frequencies. This in turn almost doubles the mining profit.},
	language = {en},
	urldate = {2020-06-09},
	journal = {The Journal of Supercomputing},
	author = {Stachowski, Matthias and Fiebig, Alexander and Rauber, Thomas},
	month = apr,
	year = {2020}
}

@article{guerreiro_gpu_2019,
	title = {{GPU} {Static} {Modeling} {Using} {PTX} and {Deep} {Structured} {Learning}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2951218},
	abstract = {In the quest for exascale computing, energy-efficiency is a fundamental goal in high-performance computing systems, typically achieved via dynamic voltage and frequency scaling (DVFS). However, this type of mechanism relies on having accurate methods of predicting the performance and power/energy consumption of such systems. Unlike previous works in the literature, this research focuses on creating novel GPU predictive models that do not require run-time information from the applications. The proposed models, implemented using recurrent neural networks, take into account the sequence of GPU assembly instructions (PTX) and can accurately predict changes in the execution time, power and energy consumption of applications when the frequencies of different GPU domains (core and memory) are scaled. Validated with 24 applications on GPUs from different NVIDIA microarchitectures (Turing, Volta, Pascal and Maxwell), the proposed models attain a significant accuracy. Particularly, the obtained power consumption scaling model provides an average error rate of 7.9\% (Tesla T4), 6.7\% (Titan V), 5.9\% (Titan Xp) and 5.4\% (GTX Titan X), which is comparable to state-of-the-art run-time counter-based models. When using the models to select the minimum-energy frequency configuration, significant energy savings can be attained: 8.0\% (Tesla T4), 6.0\% (Titan V), 29.0\% (Titan Xp) and 11.5\% (GTX Titan X).},
	journal = {IEEE Access},
	author = {Guerreiro, João and Ilic, Aleksandar and Roma, Nuno and Tomás, Pedro},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Benchmark testing, DVFS, Energy consumption, GPU, GPU assembly instructions, GPU predictive models, GPU static modeling, GTX Titan X, Graphics processing units, Kernel, NVIDIA microarchitectures, PTX, Performance evaluation, Power demand, Predictive models, Tesla T4, Titan V, Titan Xp, deep structured learning, dynamic voltage and frequency scaling, electronic engineering computing, energy conservation, energy consumption, energy savings, energy-efficiency, graphics processing units, high-performance computing systems, learning (artificial intelligence), microprocessor chips, minimum-energy frequency configuration, modeling, parallel processing, power consumption, recurrent neural nets, recurrent neural networks, run-time counter-based models, run-time information, scaling-factors},
	pages = {159150--159161}
}

@inproceedings{wang_gpgpu_2020,
	address = {San Diego, California},
	series = {{GPGPU} '20},
	title = {{GPGPU} performance estimation for frequency scaling using cross-benchmarking},
	isbn = {978-1-4503-7025-7},
	url = {https://doi.org/10.1145/3366428.3380767},
	doi = {10.1145/3366428.3380767},
	abstract = {Dynamic Voltage and Frequency Scaling (D VFS) on General-Purpose Graphics Processing Units (GPGPUs) is now becoming one of the most significant techniques to balance computational performance and energy consumption. However, there are still few fast and accurate models for predicting GPU kernel execution time under different core and memory frequency settings, which is important to determine the best frequency configuration for energy saving. Accordingly, a novel GPGPU performance estimation model with both core and memory frequency scaling is herein proposed. We design a cross-benchmarking suite, which simulates kernels with a wide range of instruction distributions. The synthetic kernels generated by this suite can be used for model pre-training or as supplementary training samples. Then we apply two different machine learning algorithms, Support Vector Regression (SVR) and Gradient Boosting Decision Tree (GBDT), to study the correlation between kernel performance counters and kernel performance. The models trained only with our cross-benchmarking suite achieve satisfying accuracy (16\%{\textasciitilde}22\% mean absolute error) on 24 unseen real application kernels. Validated on three modern GPUs with a wide frequency scaling range, by using a collection of 24 real application kernels, the proposed model is able to achieve accurate results (5.1\%, 2.8\%, 6.5\% mean absolute error) for the target GPUs (GTX 980, Titan X Pascal and Tesla P100).},
	urldate = {2020-05-24},
	booktitle = {Proceedings of the 13th {Annual} {Workshop} on {General} {Purpose} {Processing} using {Graphics} {Processing} {Unit}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qiang and Liu, Chengjian and Chu, Xiaowen},
	month = feb,
	year = {2020},
	keywords = {GPU performance modeling, dynamic voltage and frequency scaling, graphics processing units, machine learning},
	pages = {31--40}
}

@article{fan_accurate_2020,
	title = {Accurate {Energy} and {Performance} {Prediction} for {Frequency}-{Scaled} {GPU} {Kernels}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	abstract = {Energy optimization is an increasingly important aspect of today\&rsquo;s high-performance computing applications. In particular, dynamic voltage and frequency scaling (DVFS) has become a widely adopted solution to balance performance and energy consumption, and hardware vendors provide management libraries that allow the programmer to change both memory and core frequencies manually to minimize energy consumption while maximizing performance. This article focuses on modeling the energy consumption and speedup of GPU applications while using different frequency configurations. The task is not straightforward, because of the large set of possible and uniformly distributed configurations and because of the multi-objective nature of the problem, which minimizes energy consumption and maximizes performance. This article proposes a machine learning-based method to predict the best core and memory frequency configurations on GPUs for an input OpenCL kernel. The method is based on two models for speedup and normalized energy predictions over the default frequency configuration. Those are later combined into a multi-objective approach that predicts a Pareto-set of frequency configurations. Results show that our approach is very accurate at predicting extema and the Pareto set, and finds frequency configurations that dominate the default configuration in either energy or performance.},
	number = {2},
	urldate = {2020-05-24},
	journal = {Computation},
	author = {Fan, Kaijie and Cosenza, Biagio and Juurlink, Ben},
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {GPU, energy efficiency, frequency scaling, modeling},
	pages = {37}
}

@inproceedings{fan_predictable_2019,
	address = {Kyoto, Japan},
	series = {{ICPP} 2019},
	title = {Predictable {GPUs} {Frequency} {Scaling} for {Energy} and {Performance}},
	isbn = {978-1-4503-6295-5},
	doi = {10.1145/3337821.3337833},
	abstract = {Dynamic voltage and frequency scaling (DVFS) is an important solution to balance performance and energy consumption, and hardware vendors provide management libraries that allow the programmer to change both memory and core frequencies. The possibility to manually set these frequencies is a great opportunity for application tuning, which can focus on the best application-dependent setting. However, this task is not straightforward because of the large set of possible configurations and because of the multi-objective nature of the problem, which minimizes energy consumption and maximizes performance. This paper proposes a method to predict the best core and memory frequency configurations on GPUs for an input OpenCL kernel. Our modeling approach, based on machine learning, first predicts speedup and normalized energy over the default frequency configuration. Then, it combines the two models into a multi-objective one that predicts a Pareto-set of frequency configurations. The approach uses static code features, is built on a set of carefully designed micro-benchmarks, and can predict the best frequency settings of a new kernel without executing it. Test results show that our modeling approach is very accurate on predicting extrema points and Pareto set for ten out of twelve test benchmarks, and discover frequency configurations that dominate the default configuration in either energy or performance.},
	urldate = {2020-05-24},
	booktitle = {Proceedings of the 48th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Fan, Kaijie and Cosenza, Biagio and Juurlink, Ben},
	year = {2019},
	keywords = {Energy efficiency, Frequency scaling, GPUs, Modeling},
	pages = {1--10}
}

@inproceedings{wang_gpgpu_2018,
	title = {{GPGPU} {Performance} {Estimation} with {Core} and {Memory} {Frequency} {Scaling}},
	abstract = {Graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, simple and accurate performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is important to decide the best frequency configuration for energy saving. We reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Over a 2× range of both core and memory frequencies among 20 GPU kernels, our model achieves accurate results (4.83 \% error on average) with real hardware. Compared to the cycle-level simulators, our model only needs simple micro-benchmarks to extract a set of hardware parameters and kernel performance counters to produce such high accuracy without kernel source analysis.},
	booktitle = {2018 {IEEE} 24th {International} {Conference} on {Parallel} and {Distributed} {Systems} ({ICPADS})},
	author = {Wang, Qiang and Chu, Xiaowen},
	year = {2018},
	keywords = {Analytical models, Computational modeling, Dynamic Voltage and Frequency Scaling, GPGPU performance estimation, GPU Performance Modeling, GPU kernels, Graphics Processing Units, Graphics processing units, Hardware, Instruction sets, Kernel, Random access memory, accurate performance estimation, cycle-level simulators, energy consumption, fine-grained analytical model, frequency scaling, graphics processing units, memory frequency, power aware computing, voltage scaling},
	pages = {417--424}
}

@inproceedings{guerreiro_gpgpu_2018,
	title = {{GPGPU} {Power} {Modeling} for {Multi}-domain {Voltage}-{Frequency} {Scaling}},
	doi = {10.1109/HPCA.2018.00072},
	abstract = {Dynamic Voltage and Frequency Scaling (DVFS) on Graphics Processing Units (GPUs) components is one of the most promising power management strategies, due to its potential for significant power and energy savings. However, there is still a lack of simple and reliable models for the estimation of the GPU power consumption under a set of different voltage and frequency levels. Accordingly, a novel GPU power estimation model with both core and memory frequency scaling is herein proposed. This model combines information from both the GPU architecture and the executing GPU application and also takes into account the non-linear changes in the GPU voltage when the core and memory frequencies are scaled. The model parameters are estimated using a collection of 83 microbenchmarks carefully crafted to stress the main GPU components. Based on the hardware performance events gathered during the execution of GPU applications on a single frequency configuration, the proposed model allows to predict the power consumption of the application over a wide range of frequency configurations, as well as to decompose the contribution of different parts of the GPU pipeline to the overall power consumption. Validated on 3 GPU devices from the most recent NVIDIA microarchitectures (Pascal, Maxwell and Kepler), by using a collection of 26 standard benchmarks, the proposed model is able to achieve accurate results (7\%, 6\% and 12\% mean absolute error) for the target GPUs (Titan Xp, GTX Titan X and Tesla K40c).},
	booktitle = {2018 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Guerreiro, Joao and Ilic, Aleksandar and Roma, Nuno and Tomas, Pedro},
	year = {2018},
	keywords = {DVFS, Dynamic Voltage, Frequency estimation, Frequency-domain analysis, GPGPU power modeling, GPU Power Modeling, GPU Utilization Metrics, GPU application, GPU applications, GPU architecture, GPU components, GPU devices, GPU pipeline, GPU power consumption, GPU power estimation model, GPU voltage, Graphics Processing Units components, Graphics processing units, Memory management, Microbenchmarking, Power Decoupling, Power demand, Predictive models, energy savings, frequency configurations, frequency levels, graphics processing units, memory frequencies, memory frequency scaling, model parameters, multidomain Voltage-Frequency Scaling, parallel architectures, power aware computing, power consumption, power management strategies, reliable models, simple models, single frequency configuration},
	pages = {789--800}
}

@inproceedings{papadimitriou_harnessing_2017,
	address = {Cambridge, Massachusetts},
	series = {{MICRO}-50 '17},
	title = {Harnessing voltage margins for energy efficiency in multicore {CPUs}},
	isbn = {978-1-4503-4952-9},
	url = {https://doi.org/10.1145/3123939.3124537},
	doi = {10.1145/3123939.3124537},
	abstract = {In this paper, we present the first automated system-level analysis of multicore CPUs based on ARMv8 64-bit architecture (8-core, 28nm X-Gene 2 micro-server by AppliedMicro) when pushed to operate in scaled voltage conditions. We report detailed system-level effects including SDCs, corrected/uncorrected errors and application/system crashes. Our study reveals large voltage margins (that can be harnessed for energy savings) and also large Vmin variation among the 8 cores of the CPU chip, among 3 different chips (a nominal rated and two sigma chips), and among different benchmarks. Apart from the Vmin analysis we propose a new composite metric (severity) that aggregates the behavior of cores when undervolted and can support system operation and design protection decisions. Our undervolting characterization findings are the first reported analysis for an enterprise class 64-bit ARMv8 platform and we highlight key differences with previous studies on x86 platforms. We utilize the results of the system characterization along with performance counters information to measure the accuracy of prediction models for the behavior of benchmarks running in particular cores. Finally, we discuss how the detailed characterization and the prediction results can be effectively used to support design and system software decisions to harness voltage margins for energy efficiency while preserving operation correctness. Our findings show that, on average, 19.4\% energy saving can be achieved without compromising the performance, while with 25\% performance reduction, the energy saving raises to 38.8\%.},
	urldate = {2020-05-20},
	booktitle = {Proceedings of the 50th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {Association for Computing Machinery},
	author = {Papadimitriou, George and Kaliorakis, Manolis and Chatzidimitriou, Athanasios and Gizopoulos, Dimitris and Lawthers, Peter and Das, Shidhartha},
	month = oct,
	year = {2017},
	keywords = {energy efficiency, error detection and correction, micro-servers, multicore CPUs characterization, power consumption, voltage and frequency scaling},
	pages = {503--516}
}

@misc{noauthor_harnessing_nodate,
	title = {Harnessing voltage margins for energy efficiency in multicore {CPUs} {\textbar} {Proceedings} of the 50th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	url = {https://dl.acm.org/doi/10.1145/3123939.3124537},
	urldate = {2020-05-20}
}
@inproceedings{kalogirou_exploiting_2019,
	title = {Exploiting {CPU} {Voltage} {Margins} to {Increase} the {Profit} of {Cloud} {Infrastructure} {Providers}},
	doi = {10.1109/CCGRID.2019.00044},
	abstract = {Energy efficiency is a major concern for cloud computing, with CPUs accounting a significant fraction of datacenter nodes power consumption. CPU manufacturers introduce voltage margins to guarantee correct operation. However, these are unnecessarily wide for real-world execution scenarios, and translate to increased power consumption. In this paper, we investigate how such margins can be exploited by infrastructure operators, by selectively undervolting nodes, at the controlled risk of inducing failures and activating service-level agreement (SLA) violation penalties. We model the problem in a formal way, capturing the most important aspects that drive VM management and system configuration decisions. Then, we introduce XM-VFS policy that reduces infrastructure operator costs by reducing voltage margins, and compare it with the state-of-the-art which employs dynamic voltage-frequency scaling (DVFS) and workload consolidation. We perform simulations to quantify the cost reduction, considering the energy consumption and potential SLA violations. Our results show significant gains, up to 17.35\% and 16.32\% for the energy and cost reduction respectively. In our simulations, we use realistic assumptions for voltage margins, energy consumption and performance degradation of applications due to frequency scaling, based on the characterization of commercial Intel-and ARM-based machines. Our model and scheduling policy are generic and scalable.},
	booktitle = {2019 19th {IEEE}/{ACM} {International} {Symposium} on {Cluster}, {Cloud} and {Grid} {Computing} ({CCGRID})},
	author = {Kalogirou, Christos and Koutsovasilis, Panos and Antonopoulos, Christos D. and Bellas, Nikolaos and Lalis, Spyros and Venugopal, Srikumar and Pinto, Christian},
	month = may,
	year = {2019},
	keywords = {CPU manufacturers, CPU voltage margins, Web services, cloud computing, cloud infrastructure providers, computer centres, contracts, cost reduction, customer services, datacenter nodes power consumption, energy conservation, energy consumption, energy efficiency, extended margins, undervolting, energy efficiency, scheduling, cost effectiveness, infrastructure operator costs, infrastructure operators, potential SLA violations, power consumption, quality of service, real-world execution scenarios, service-level agreement, service-oriented architecture, system configuration decisions, virtual machines, workload consolidation},
	pages = {302--311}
}

@article{wang_enabling_2020,
	title = {Enabling {Energy}-{Efficient} and {Reliable} {Neural} {Network} via {Neuron}-{Level} {Voltage} {Scaling}},
	issn = {1557-9956},
	doi = {10.1109/TC.2020.2973150},
	abstract = {With the platforms of running deep neural networks (DNNs) move from large-scale data centers to handheld devices, power emerge as one of the most significant obstacles. Voltage scaling is a promising technique that enables power saving. Nevertheless, it raises reliability and performance concerns that may undesirably deteriorate the NNs accuracy and performance. Consequently, an energy-efficient and reliable scheme is required for NNs to balance the above three aspects with satisfied user experience. To this end, we propose a neuron-level voltage scaling framework called NN-APP to model the impact of supply voltages on NNs from output accuracy (A), power (P), and performance (P) perspectives. We analyze the error propagation characteristics in NNs at both inter- and intra- network layers to precisely model the impact of voltage scaling on the final output accuracy at neuron-level. Furthermore, we combine a voltage clustering method and the multi-objective optimization to identify the optimal voltage islands and apply the same voltage to neurons with similar fault tolerance capability. We perform three case studies to demonstrate the efficacy of the proposed techniques.},
	journal = {IEEE Transactions on Computers},
	author = {Wang, Jing and Fu, Xin and Wang, Xu and Liu, Shubo and Gao, Lan and Zhang, Weigong},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Artificial neural networks, Computer network reliability, Energy efficiency, Fault tolerance, Fault tolerant systems, Neurons, and reliability, energy-efficiency, fault tolerance, neural networks, voltage scaling},
	pages = {1--1}
}

@inproceedings{salami_comprehensive_2018,
	title = {Comprehensive {Evaluation} of {Supply} {Voltage} {Underscaling} in {FPGA} on-{Chip} {Memories}},
	doi = {10.1109/MICRO.2018.00064},
	abstract = {In this work, we evaluate aggressive undervolting, i.e., voltage scaling below the nominal level to reduce the energy consumption of Field Programmable Gate Arrays (FPGAs). Usually, voltage guardbands are added by chip vendors to ensure the worst-case process and environmental scenarios. Through experimenting on several FPGA architectures, we measure this voltage guardband to be on average 39\% of the nominal level, which in turn, delivers more than an order of magnitude power savings. However, further undervolting below the voltage guardband may cause reliability issues as the result of the circuit delay increase, i.e., start to appear faults. We extensively characterize the behavior of these faults in terms of the rate, location, type, as well as sensitivity to environmental temperature, with a concentration of on-chip memories, or Block RAMs (BRAMs). Finally, we evaluate a typical FPGA-based Neural Network (NN) accelerator under low-voltage BRAM operations. In consequence, the substantial NN energy savings come with the cost of NN accuracy loss. To attain power savings without NN accuracy loss, we propose a novel technique that relies on the deterministic behavior of undervolting faults and can limit the accuracy loss to 0.1\% without any timing-slack overhead.},
	booktitle = {2018 51st {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Salami, Behzad and S. Unsal, Osman and Cristal Kestelman, Adrian},
	month = oct,
	year = {2018},
	keywords = {Artificial neural networks, Circuit faults, Energy, FPGA, FPGA architectures, FPGA on-chip memories, Field programmable gate arrays, NN accuracy loss, Neural Network, Power demand, Reliability, System-on-chip, Voltage Scaling, Voltage control, chip vendors, comprehensive evaluation, energy consumption, environmental scenarios, field programmable gate arrays, low-voltage BRAM operations, magnitude power savings, nominal level, power aware computing, random-access storage, substantial NN energy savings, supply voltage underscaling, typical FPGA-based neural network accelerator, undervolting faults, voltage guardband, voltage scaling, worst-case process},
	pages = {724--736}
}

@inproceedings{salami_fault_2018,
	title = {Fault {Characterization} {Through} {FPGA} {Undervolting}},
	doi = {10.1109/FPL.2018.00023},
	abstract = {The power and energy efficiency of Field Programmable Gate Arrays (FPGAs) are estimated to be up to 20X less than Application Specific Integrated Circuits (ASICs). What is needed to close this gap is aggressive power/energy savings techniques. Such a potentially effective approach is undervolting, which can directly deliver an order of magnitude static and dynamic power savings. However, aggressive undervolting, without accompanying frequency scaling leads to timing related faults, potentially undermining the power savings. Understanding the behavior of these faults and efficiently mitigating them can deliver further power and energy savings in low-voltage designs. In this paper, we conduct a detailed analysis of undervolting FPGA on-chip memories (BRAMs). Through experimental analysis, we find that lowering the supply voltage until a certain conservative level, Vmin does not introduce any observable fault. For the studied platforms, we measure this voltage guardband gap to be 39\% of the nominal level (Vnom= 1V, Vmin= 0.61V). Further undervolting corrupts some of the data bits stored in BRAMs; however, it also reduces the BRAMs power consumption a further 36.1\%. When the voltage is lowered below Vmin, the rate of these faults exponentially increases to 0.06\%, by a fully non-uniform distribution over various BRAMs. This paper comprehensively analyzes the behavior of these faults, in terms of rate, type, location, and environmental temperature.},
	booktitle = {2018 28th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	author = {Salami, Behzad and Unsal, Osman and Cristal, Adrian},
	month = aug,
	year = {2018},
	note = {ISSN: 1946-1488},
	keywords = {BRAM, Circuit faults, FPGA, FPGA on-chip memories, FPGA undervolting, Faults, Field programmable gate arrays, Hardware, Power Consumption, Power demand, Standards, Supply Voltage Scaling, Temperature, Voltage measurement, aggressive power/energy savings techniques, application specific integrated circuits, dynamic power savings, energy efficiency, fault characterization, field programmable gate arrays, frequency scaling, logic design, low-power electronics, low-voltage designs, observable fault, power aware computing, power consumption, supply voltage, voltage 0.61 V, voltage 1.0 V, voltage guardband gap},
	pages = {85--853}
}

@inproceedings{tang_impact_2019,
	address = {Phoenix, AZ, USA},
	series = {e-{Energy} '19},
	title = {The {Impact} of {GPU} {DVFS} on the {Energy} and {Performance} of {Deep} {Learning}: an {Empirical} {Study}},
	isbn = {978-1-4503-6671-7},
	shorttitle = {The {Impact} of {GPU} {DVFS} on the {Energy} and {Performance} of {Deep} {Learning}},
	doi = {10.1145/3307772.3328315},
	abstract = {Over the past years, great progress has been made in improving the computing power of general-purpose graphics processing units (GPGPUs), which facilitates the prosperity of deep neural networks (DNNs) in multiple fields like computer vision and natural language processing. A typical DNN training process repeatedly updates tens of millions of parameters, which not only requires huge computing resources but also consumes significant energy. In order to train DNNs in a more energy-efficient way, we empirically investigate the impact of GPU Dynamic Voltage and Frequency Scaling (DVFS) on the energy consumption and performance of deep learning. Our experiments cover a wide range of GPU architectures, DVFS settings, and DNN configurations. We observe that, compared to the default core frequency settings of three tested GPUs, the optimal core frequency can help conserve 8.7\%{\textasciitilde}23.1\% energy consumption for different DNN training cases. Regarding the inference, the benefits vary from 19.6\%{\textasciitilde}26.4\%. Our findings suggest that GPU DVFS has great potentials to help develop energy efficient DNN training/inference schemes.},
	urldate = {2020-05-20},
	booktitle = {Proceedings of the {Tenth} {ACM} {International} {Conference} on {Future} {Energy} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Zhenheng and Wang, Yuxin and Wang, Qiang and Chu, Xiaowen},
	month = jun,
	year = {2019},
	keywords = {Deep Convolutional Neural Network, Dynamic Voltage and Frequency Scaling, Graphics Processing Units},
	pages = {315--325}
}

@misc{noauthor_deep_nodate,
	title = {Deep learning state of the art 2020 ({MIT} {Deep} {Learning} {Series}) - {Part} 1 · {Buomsoo} {Kim}},
	url = {https://buomsoo-kim.github.io/learning/2020/04/02/deep-learning-sota-part-1.md/},
	urldate = {2020-05-20}
}

@article{mittal_survey_2019,
	title = {A survey of techniques for optimizing deep learning on {GPUs}},
	volume = {99},
	issn = {1383-7621},
	doi = {10.1016/j.sysarc.2019.101635},
	abstract = {The rise of deep-learning (DL) has been fuelled by the improvements in accelerators. Due to its unique features, the GPU continues to remain the most widely used accelerator for DL applications. In this paper, we present a survey of architecture and system-level techniques for optimizing DL applications on GPUs. We review techniques for both inference and training and for both single GPU and distributed system with multiple GPUs. We bring out the similarities and differences of different works and highlight their key attributes. This survey will be useful for both novice and experts in the field of machine learning, processor architecture and high-performance computing.},
	language = {en},
	urldate = {2020-05-20},
	journal = {Journal of Systems Architecture},
	author = {Mittal, Sparsh and Vaishay, Shraiysh},
	month = oct,
	year = {2019},
	note = {Original Publisher: Elsevier},
	keywords = {Accelerator, Allreduce, Distributed training, GPU, Hardware architecture for deep learning, Parameter server, Pruning, Review, Tiling},
	pages = {101635}
}

@article{khan_miopen_2019,
	title = {{MIOpen}: {An} {Open} {Source} {Library} {For} {Deep} {Learning} {Primitives}},
	shorttitle = {{MIOpen}},
	abstract = {Deep Learning has established itself to be a common occurrence in the business lexicon. The unprecedented success of deep learning in recent years can be attributed to: abundance of data, availability of gargantuan compute capabilities offered by GPUs, and adoption of open-source philosophy by the researchers and industry. Deep neural networks can be decomposed into a series of different operators. MIOpen, AMD's open-source deep learning primitives library for GPUs, provides highly optimized implementations of such operators, shielding researchers from internal implementation details and hence, accelerating the time to discovery. This paper introduces MIOpen and provides details about the internal workings of the library and supported features. MIOpen innovates on several fronts, such as implementing fusion to optimize for memory bandwidth and GPU launch overheads, providing an auto-tuning infrastructure to overcome the large design space of problem configurations, and implementing different algorithms to optimize convolutions for different filter and input sizes. MIOpen is one of the first libraries to publicly support the bfloat16 data-type for convolutions, allowing efficient training at lower precision without the loss of accuracy.},
	urldate = {2020-05-11},
	journal = {arXiv:1910.00078 [cs, stat]},
	author = {Khan, Jehandad and Fultz, Paul and Tamazov, Artem and Lowell, Daniel and Liu, Chao and Melesse, Michael and Nandhimandalam, Murali and Nasyrov, Kamil and Perminov, Ilya and Shah, Tejash and Filippov, Vasilii and Zhang, Jing and Zhou, Jing and Natarajan, Bragadeesh and Daga, Mayank},
	month = sep,
	year = {2019},
	note = {arXiv: 1910.00078},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@incollection{dutot_high-performance_2016,
	address = {Cham},
	title = {High-{Performance} {Matrix}-{Matrix} {Multiplications} of {Very} {Small} {Matrices}},
	volume = {9833},
	isbn = {978-3-319-43658-6 978-3-319-43659-3},
	abstract = {The use of the general dense matrix-matrix multiplication (GEMM) is fundamental for obtaining high performance in many scientiﬁc computing applications. GEMMs for small matrices (of sizes less than 32) however, are not suﬃciently optimized in existing libraries. In this paper we consider the case of many small GEMMs on either CPU or GPU architectures. This is a case that often occurs in applications like big data analytics, machine learning, high-order FEM, and others. The GEMMs are grouped together in a single batched routine. We present specialized for these cases algorithms and optimization techniques to obtain performance that is within 90\% of the optimal. We show that these results outperform currently available state-of-the-art implementations and vendor-tuned math libraries.},
	language = {en},
	urldate = {2020-05-11},
	booktitle = {Euro-{Par} 2016: {Parallel} {Processing}},
	publisher = {Springer International Publishing},
	author = {Masliah, Ian and Abdelfattah, Ahmad and Haidar, A. and Tomov, S. and Baboulin, Marc and Falcou, J. and Dongarra, J.},
	editor = {Dutot, Pierre-François and Trystram, Denis},
	year = {2016},
	doi = {10.1007/978-3-319-43659-3_48},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {659--671}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2020-05-11},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444}
}

@misc{noauthor_approxann_nodate,
	title = {{ApproxANN}: {An} approximate computing framework for artificial neural network - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/7092478},
	urldate = {2020-05-11}
}

@misc{noauthor_baidu-researchdeepbench_2020,
	title = {baidu-research/{DeepBench}},
	copyright = {Apache-2.0},
	url = {https://github.com/baidu-research/DeepBench},
	abstract = {Benchmarking Deep Learning operations on different hardware},
	urldate = {2020-05-11},
	publisher = {Baidu Research},
	month = may,
	year = {2020},
	note = {original-date: 2016-09-23T23:57:05Z}
}

@misc{noauthor_rocmsoftwareplatformrocblas_2020,
	title = {{ROCmSoftwarePlatform}/{rocBLAS}},
	copyright = {MIT},
	url = {https://github.com/ROCmSoftwarePlatform/rocBLAS},
	abstract = {Next generation BLAS implementation for ROCm platform},
	urldate = {2020-05-11},
	publisher = {ROCm Software Platform},
	month = may,
	year = {2020},
	note = {original-date: 2015-10-08T18:48:02Z},
	keywords = {blas, hip, rocm}
}

@misc{noauthor_radeonopencomputeroc-smi_2020,
	title = {{RadeonOpenCompute}/{ROC}-smi},
	url = {https://github.com/RadeonOpenCompute/ROC-smi},
	abstract = {ROC System Management Interface. Contribute to RadeonOpenCompute/ROC-smi development by creating an account on GitHub.},
	urldate = {2020-05-11},
	publisher = {ROCm Core Technology},
	month = may,
	year = {2020},
	note = {original-date: 2016-05-11T20:56:10Z}
}

@misc{noauthor_rocmsoftwareplatformmiopen_2020,
	title = {{ROCmSoftwarePlatform}/{MIOpen}},
	copyright = {MIT},
	url = {https://github.com/ROCmSoftwarePlatform/MIOpen},
	abstract = {AMD's Machine Intelligence Library. Contribute to ROCmSoftwarePlatform/MIOpen development by creating an account on GitHub.},
	urldate = {2020-05-11},
	publisher = {ROCm Software Platform},
	month = may,
	year = {2020},
	note = {original-date: 2017-06-27T17:51:22Z}
}

@article{jiang_optimizing_2020,
	title = {Optimizing energy efficiency of {CNN}-based object detection with dynamic voltage and frequency scaling},
	volume = {41},
	issn = {1674-4926, 2058-6140},
	url = {https://iopscience.iop.org/article/10.1088/1674-4926/41/2/022406},
	doi = {10.1088/1674-4926/41/2/022406},
	number = {2},
	urldate = {2020-05-11},
	journal = {Journal of Semiconductors},
	author = {Jiang, Weixiong and Yu, Heng and Zhang, Jiale and Wu, Jiaxuan and Luo, Shaobo and Ha, Yajun},
	month = feb,
	year = {2020},
	pages = {022406}
}

@inproceedings{li_evaluating_2016,
	title = {Evaluating the {Energy} {Efficiency} of {Deep} {Convolutional} {Neural} {Networks} on {CPUs} and {GPUs}},
	doi = {10.1109/BDCloud-SocialCom-SustainCom.2016.76},
	abstract = {In recent years convolutional neural networks (CNNs) have been successfully applied to various applications that are appropriate for deep learning, from image and video processing to speech recognition. The advancements in both hardware (e.g. more powerful GPUs) and software (e.g. deep learning models, open-source frameworks and supporting libraries) have significantly improved the accuracy and training time of CNNs. However, the high speed and accuracy are at the cost of energy consumption, which has been largely ignored in previous CNN design. With the size of data sets grows exponentially, the energy demand for training such data sets increases rapidly. It is highly desirable to design deep learning frameworks and algorithms that are both accurate and energy efficient. In this paper, we conduct a comprehensive study on the power behavior and energy efficiency of numerous well-known CNNs and training frameworks on CPUs and GPUs, and we provide a detailed workload characterization to facilitate the design of energy efficient deep learning solutions.},
	booktitle = {2016 {IEEE} {International} {Conferences} on {Big} {Data} and {Cloud} {Computing} ({BDCloud}), {Social} {Computing} and {Networking} ({SocialCom}), {Sustainable} {Computing} and {Communications} ({SustainCom}) ({BDCloud}-{SocialCom}-{SustainCom})},
	author = {Li, Da and Chen, Xinbo and Becchi, Michela and Zong, Ziliang},
	month = oct,
	year = {2016},
	keywords = {Biological neural networks, CPU, Energy consumption, Energy efficiency, GPU, GPUs, Graphics processing units, Hardware, Machine learning, Training, central processing unit, deep convolutional neural networks, deep learning, deep learning framework, energy conservation, energy efficiency, energy-efficiency, graphics processing unit, graphics processing units, learning (artificial intelligence), neural nets, neural networks},
	pages = {477--484}
}

@inproceedings{karki_detailed_2019,
	address = {Providence, RI, USA},
	series = {{GPGPU} '19},
	title = {Detailed {Characterization} of {Deep} {Neural} {Networks} on {GPUs} and {FPGAs}},
	isbn = {978-1-4503-6255-9},
	url = {https://doi.org/10.1145/3300053.3319418},
	doi = {10.1145/3300053.3319418},
	abstract = {Deep neural networks (DNNs) have been proving the effectiveness in various computing fields. To provide more efficient computing platforms for DNN applications, it is essential to have evaluation environments that include assorted benchmark workloads. Though a few DNN benchmark suites have been recently released, most of them require to install proprietary DNN libraries or resource-intensive DNN frameworks, which can run only on certain architectures. Also, some of the benchmark suites only support a few per-layer functions where the interactions between layers can not be measured. To provide a more scalable evaluation environment, we present a new DNN benchmark suite, Tango, that can run on any platform that supports CUDA and OpenCL. Tango includes the most widely used five convolution neural networks and two recurrent neural networks. We provide in-depth architectural statistics of these networks while running them on an architecture simulator, a server- and a mobile-GPU, and a mobile FPGA.},
	urldate = {2020-02-26},
	booktitle = {Proceedings of the 12th {Workshop} on {General} {Purpose} {Processing} {Using} {GPUs}},
	publisher = {Association for Computing Machinery},
	author = {Karki, Aajna and Keshava, Chethan Palangotu and Shivakumar, Spoorthi Mysore and Skow, Joshua and Hegde, Goutam Madhukeshwar and Jeon, Hyeran},
	month = apr,
	year = {2019},
	keywords = {Benchmark Suite, CUDA, Deep neural network, OpenCL},
	pages = {12--21}
}

@article{nabavinejad_coordinated_2019,
	title = {Coordinated {DVFS} and {Precision} {Control} for {Deep} {Neural} {Networks}},
	volume = {18},
	issn = {2473-2575},
	doi = {10.1109/LCA.2019.2942020},
	abstract = {Traditionally, DVFS has been the main mechanism to trade-off performance and power. We observe that Deep Neural Network (DNN) applications offer the possibility to trade-off performance, power, and accuracy using both DVFS and numerical precision levels. Our proposed approach, Power-Inference accuracy Trading (PIT), monitors the server's load, and accordingly adjusts the precision of the DNN model and the DVFS setting of GPU to trade-off the accuracy and power consumption with response time. At high loads and tight request arrivals, PIT leverages INT8-precision instructions of GPU to dynamically change the precision of deployed DNN models and boosts GPU frequency to execute the requests faster at the expense of accuracy reduction and high power consumption. However, when the requests' arrival rate is relaxed and there is slack time for requests, PIT deploys high precision version of models to improve the accuracy and reduces GPU frequency to decrease power consumption. We implement and deploy PIT on a state-of-the-art server equipped with a Tesla P40 GPU. Experimental results demonstrate that depending on the load, PIT can improve response time up to 11 percent compared to a job scheduler that uses only FP32 precision. It also improves the energy consumption by up to 28 percent, while achieving around 99.5 percent accuracy of sole FP32-precision.},
	number = {2},
	journal = {IEEE Computer Architecture Letters},
	author = {Nabavinejad, Seyed Morteza and Hafez-Kolahi, Hassan and Reda, Sherief},
	month = jul,
	year = {2019},
	keywords = {DNN model, Deep neural network, FP32 precision, Graphics processing units, Neural networks, PIT, Power demand, Runtime, Servers, Tesla P40 GPU, Time factors, Time-frequency analysis, accuracy, coordinated DVFS, deep neural network applications, energy consumption, graphics processing units, hardware accelerator, high power consumption, job scheduler, neural nets, power, power aware computing, power consumption, power-inference accuracy trading, precision control, response time, scheduling},
	pages = {136--140}
}

@article{nabavinejad_coordinated_2019-1,
	title = {Coordinated {DVFS} and {Precision} {Control} for {Deep} {Neural} {Networks}},
	volume = {18},
	issn = {2473-2575},
	doi = {10.1109/LCA.2019.2942020},
	abstract = {Traditionally, DVFS has been the main mechanism to trade-off performance and power. We observe that Deep Neural Network (DNN) applications offer the possibility to trade-off performance, power, and accuracy using both DVFS and numerical precision levels. Our proposed approach, Power-Inference accuracy Trading (PIT), monitors the server's load, and accordingly adjusts the precision of the DNN model and the DVFS setting of GPU to trade-off the accuracy and power consumption with response time. At high loads and tight request arrivals, PIT leverages INT8-precision instructions of GPU to dynamically change the precision of deployed DNN models and boosts GPU frequency to execute the requests faster at the expense of accuracy reduction and high power consumption. However, when the requests' arrival rate is relaxed and there is slack time for requests, PIT deploys high precision version of models to improve the accuracy and reduces GPU frequency to decrease power consumption. We implement and deploy PIT on a state-of-the-art server equipped with a Tesla P40 GPU. Experimental results demonstrate that depending on the load, PIT can improve response time up to 11 percent compared to a job scheduler that uses only FP32 precision. It also improves the energy consumption by up to 28 percent, while achieving around 99.5 percent accuracy of sole FP32-precision.},
	number = {2},
	journal = {IEEE Computer Architecture Letters},
	author = {Nabavinejad, Seyed Morteza and Hafez-Kolahi, Hassan and Reda, Sherief},
	month = jul,
	year = {2019},
	keywords = {DNN model, Deep neural network, FP32 precision, Graphics processing units, Neural networks, PIT, Power demand, Runtime, Servers, Tesla P40 GPU, Time factors, Time-frequency analysis, accuracy, coordinated DVFS, deep neural network applications, energy consumption, graphics processing units, hardware accelerator, high power consumption, job scheduler, neural nets, power, power aware computing, power consumption, power-inference accuracy trading, precision control, response time, scheduling},
	pages = {136--140}
}

@article{seongki_gpgpu-perf:_2015,
	title = {{GPGPU}-{Perf}: efficient, interval-based {DVFS} algorithm for mobile {GPGPU} applications},
	issn = {1432-2315},
	doi = {https://doi.org/10.1007/s00371-015-1111-1},
	abstract = {Although general purpose computations on graphics processing unit (GPGPU) technologies are available
even on GPUs, their performance has been seriously affected
by the underlying dynamic voltage and frequency scaling
(DVFS) mechanism of GPU. In order to save the energy,
eventually prolonging the battery life, the DVFS adjusts the
GPU’s frequency according to the past utilization. When the
GPU processes graphic tasks only, it is enough to process
them within a fixed time (typically 30–60 frames per second), so the DVFS parameters can be conservatively set.
However, in GPGPU case, the GPU should process them
at much higher rates depending on applications. Although a
modification of DVFS parameters may improve the GPGPU
performance, the energy efficiency is sacrificed, and the performance of graphic tasks is affected, as these parameters
are shared by both graphic and GPGPU tasks. In order to
improve the GPGPU performance without influencing the
graphic performance, we devise the new GPGPU-Perf algorithm that adjusts the DVFS parameters such as thresholds
and an interval. The new algorithm controls the frequency
more intelligently for mobile GPGPU applications, and thus
the performance over energy increases by 1.44 times with no
influences on graphic tasks and any modifications of GPGPU
algorithms. To the best of our knowledge, this paper is the
first work that proposes a GPU-DVFS algorithm for GPGPU
applications.},
	language = {en},
	journal = {Springer Berlin Heidelberg},
	author = {SeongKi, Kim and Young, Kim},
	month = apr,
	year = {2015}
}

@misc{noauthor_jon_2018,
	title = {Jon {Peddie} {Research} releases its {Q2}, 2019 global add-in board report},
	url = {https://www.jonpeddie.com/press-releases/jon-peddie-research-releases-its-q2-2019-global-add-in-board-report/},
	abstract = {AMD increases market share again in the quarter},
	urldate = {2019-12-03},
	year = {2018}
}

@misc{nvidia_cuda_2017,
	title = {{CUDA} {Zone}},
	url = {https://developer.nvidia.com/cuda-zone},
	abstract = {CUDA® is a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs). With CUDA, developers are able to dramatically speed up computing applications by harnessing the power of GPUs.},
	language = {en},
	urldate = {2019-12-08},
	journal = {NVIDIA Developer},
	author = {{NVIDIA}},
	month = jul,
	year = {2017}
}

@misc{nvidia_cuda_2008,
	type = {concept},
	title = {{CUDA} {C}++ {Programming} {Guide}},
	url = {http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html},
	abstract = {The programming guide to the CUDA model and interface.},
	language = {en-us},
	urldate = {2019-11-28},
	author = {{NVIDIA}},
	year = {2008}
}

@inproceedings{kanduri_approximation_2016,
	title = {Approximation {Knob}: {Pwer} {Capping} {Meets} {Energy} {Efficieny}},
	isbn = {978-1-4503-4466-1},
	url = {http://delivery.acm.org/10.1145/2970000/2967002/a122-kanduri.pdf?ip=193.136.132.10&id=2967002&acc=ACTIVE%20SERVICE&key=2E5699D25B4FE09E%2E454625C777251F56%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1571844946_5c810906c6450effb0c8bf49a8820b1e},
	abstract = {Power Capping techniques are used to restrict power consumption of computer systems to a thermally safe limit. Current many-core systems employ dynamic voltage and frequency scaling (DVFS), power gating (PG) and scheduling methods as actuators for power capping. These knobs arc oriented towards power actuation, while the need for performance and energy savings are increasing in the dark silicon era. To address this, we propose approximation (APPX) as another knob for close-looped power management, lending performance and energy efficiency to existing power capping techniques. We use approximation in a pro-active way for long-term performance-energy objectives, complementing the short-term reactive power objectives. We implement an approximation-enabled power management framework, APPEND, that dynamically chooses an application with appropriate level of approximation from a set of variable accuracy implementations. Subject to the system dynamics, our power manager chooses an effective combination of knobs - APPX, DVFS and PG, in a hierarchical way to ensure power capping with performance and energy gains. Our proposed approach yields 1.5× higher throughput, improved latency upto 5×, better performance per energy and dark silicon mitigation compared to state-of-the-art power management techniques over a set of applications ranging from high to no error resilience.},
	urldate = {2019-10-23},
	publisher = {IEEE},
	author = {Kanduri, Anil},
	month = nov,
	year = {2016}
}

@misc{amd_amd_2008,
	title = {{AMD} {Accelerated} {Parallel} {Processing}: {OpenCL} {Programming} {Guide}},
	url = {http://developer.amd.com/wordpress/media/2013/07/AMD_Accelerated_Parallel_Processing_OpenCL_Programming_Guide-rev-2.7.pdf},
	language = {en-us},
	urldate = {2019-11-28},
	author = {AMD},
	year = {2008}
}

@misc{noauthor_gpu_2011,
	title = {{GPU} {Wattch} {Manual}},
	url = {http://www.gpgpu-sim.org/gpuwattch/},
	urldate = {2019-12-03},
	year = {2011}
}

@article{amd_polaris_2017,
	title = {The {Polaris} {Architecture}  {\textbar}},
	language = {en},
	author = {AMD},
	year = {2017},
	pages = {22}
}

@misc{noauthor_cifar-10_2010,
	title = {{CIFAR}-10 and {CIFAR}-100 datasets},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	urldate = {2019-12-25},
	year = {2010}
}

@misc{noauthor_pytorch_2016,
	title = {{PyTorch}},
	url = {https://pytorch.org/},
	urldate = {2019-12-26},
	year = {2016}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2019-12-25},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2019-12-25},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	urldate = {2019-12-25},
	journal = {arXiv:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{jiao_assessment_2017,
	title = {An assessment of vulnerability of hardware neural networks to dynamic voltage and temperature variations},
	doi = {10.1109/ICCAD.2017.8203882},
	abstract = {As a problem solving method, neural networks have shown broad applicability from medical applications, speech recognition, and natural language processing. This success has even led to implementation of neural network algorithms into hardware. In this paper, we explore two questions: (a) to what extent microelectronic variations affects the quality of results by neural networks; and (b) if the answer to first question represents an opportunity to optimize the implementation of neural network algorithms. Regarding first question, variations are now increasingly common in aggressive process nodes and typically manifest as an increased frequency of timing errors. Combating variations - due to process and/or operating conditions - usually results in increased guardbands in circuit and architectural design, thus reducing the gains from process technology advances. Given the inherent resilience of neural networks due to adaptation of their learning parameters, one would expect the quality of results produced by neural networks to be relatively insensitive to the rising timing error rates caused by increased variations. On the contrary, using two frequently used neural networks (MLP and CNN), our results show that variations can significantly affect the inference accuracy. This paper outlines our assessment methodology and use of a cross-layer evaluation approach that extracts hardware-level errors from twenty different operating conditions and then inject such errors back to the software layer in an attempt to answer the second question posed above.},
	booktitle = {2017 {IEEE}/{ACM} {International} {Conference} on {Computer}-{Aided} {Design} ({ICCAD})},
	author = {Jiao, Xun and Luo, Mulong and Lin, Jeng-Hau and Gupta, Rajesh K.},
	month = nov,
	year = {2017},
	note = {ISSN: 1558-2434},
	keywords = {Adders, Biological neural networks, CNN, Hardware, Logic gates, MLP, Resilience, Timing, dynamic voltage, hardware neural networks, learning (artificial intelligence), medical applications, multilayer perceptrons, natural language processing, neural nets, neural network algorithms, problem solving, speech recognition, temperature variations},
	pages = {945--950}
}

@inproceedings{thomas_core_2016,
	title = {Core tunneling: {Variation}-aware voltage noise mitigation in {GPUs}},
	shorttitle = {Core tunneling},
	doi = {10.1109/HPCA.2016.7446061},
	abstract = {Voltage noise and manufacturing process variation represent significant reliability challenges for modern microprocessors. Voltage noise is caused by rapid changes in processor activity that can lead to timing violations and errors. Process variation is caused by manufacturing challenges in low-nanometer technologies and can lead to significant heterogeneity in performance and reliability across the chip. To ensure correct execution under worst-case conditions, chip designers generally add operating margins that are often unnecessarily conservative for most use cases, which results in wasted energy. This paper investigates the combined effects of process variation and voltage noise on modern GPU architectures. A distributed power delivery and process variation model at functional unit granularity was developed and used to simulate supply voltage behavior in a multicore GPU system. We observed that, just like in CPUs, large changes in power demand can lead to significant voltage droops. We also note that process variation makes some cores much more vulnerable to noise than others in the same GPU. Therefore, protecting the chip against large voltage droops by using fixed and uniform voltage guardbands is costly and inefficient. This paper presents core tunneling, a variation-aware solution for dynamically reducing voltage margins. The system relies on hardware critical path monitors to detect voltage noise conditions and quickly reacts by clock-gating vulnerable cores to prevent timing violations. This allows a substantial reduction in voltage margins. Since clock gating is enabled infrequently and only on the most vulnerable cores, the performance impact of core tunneling is very low. On average, core tunneling reduces energy consumption by 15\%.},
	booktitle = {2016 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Thomas, Renji and Barber, Kristin and Sedaghati, Naser and Zhou, Li and Teodorescu, Radu},
	month = mar,
	year = {2016},
	note = {ISSN: 2378-203X},
	keywords = {Delays, GPU, Graphics processing units, Histograms, Kernel, Monitoring, Power demand, Tunneling, clock gating, core tunneling, distributed power delivery, energy conservation, energy consumption reduction, graphics processing unit, graphics processing units, low-nanometer technologies, microprocessors, power aware computing, process variation model, variation-aware voltage noise mitigation, voltage droops, voltage guardbands, voltage margin reduction},
	pages = {151--162}
}

@inproceedings{isci_runtime_2003,
	address = {San Diego, CA, USA},
	title = {Runtime power monitoring in high-end processors: methodology and empirical data},
	isbn = {978-0-7695-2043-8},
	shorttitle = {Runtime power monitoring in high-end processors},
	url = {http://ieeexplore.ieee.org/document/1253186/},
	doi = {10.1109/MICRO.2003.1253186},
	abstract = {With power dissipation becoming an increasingly vexing problem across many classes of computer systems, measuring power dissipation of real, running systems has become crucial for hardware and software system research and design. Live power measurements are imperative for studies requiring execution times too long for simulation, such as thermal analysis. Furthermore, as processors become more complex and include a host of aggressive dynamic power management techniques, per-component estimates of power dissipation have become both more challenging as well as more important.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {22nd {Digital} {Avionics} {Systems} {Conference}. {Proceedings} ({Cat}. {No}.{03CH37449})},
	publisher = {IEEE Comput. Soc},
	author = {Isci, C. and Martonosi, M.},
	year = {2003},
	pages = {93--104}
}

@article{thomas_application_2018,
	title = {Application aware {Scalable} {Architecture} for {GPGPU}},
	volume = {89},
	issn = {1383-7621},
	url = {http://www.sciencedirect.com/science/article/pii/S1383762117304241},
	doi = {10.1016/j.sysarc.2018.07.003},
	abstract = {Modern General Purpose Graphic Processing Units (GPGPU) offer high throughput for parallel applications with their hundreds of integrated cores. However, there are applications that experience performance saturation and even degradation with increasing number of cores. At present the scheduler in the GPU hardware allocates all the available resources to maximize their utilization. We observed that applications have preference towards specific set of resources. The utilization of other redundant resources can reduce the throughput of the applications. To overcome this problem, in this paper we first classify the applications into two types; type-I that dominantly require processing cores and type-II that rely on the performance of the memory-system. We propose an Application aware Scalable Architecture (ApSA) for GPGPU based on classified applications which performs run-time tailoring of the GPU resources to present an optimal set of resources to the running application. The results are analyzed and compared in terms of instructions per cycle, bandwidth utilization and branch divergence. We found that if the application is identified to be of type-I with the proposed technique the average profiling overhead is 1.6\%. Type-II applications experience average profiling overhead of 1.15\%. The average power saved by clock-gating redundant resources in the case of type-II applications is 20.08\%.},
	language = {en},
	urldate = {2019-12-14},
	journal = {Journal of Systems Architecture},
	author = {Thomas, Winnie and Daruwala, Rohin D.},
	month = sep,
	year = {2018},
	keywords = {Bandwidth utilization, Branch divergence, CTA scheduler, CUDA, GPGPU, Instructions per cycle},
	pages = {73--83}
}

@book{orr_neural_1998,
	address = {Berlin Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Networks}: {Tricks} of the {Trade}},
	isbn = {978-3-540-49430-0},
	shorttitle = {Neural {Networks}},
	url = {https://www.springer.com/gp/book/9783540494300},
	abstract = {It is our belief that researchers and practitioners acquire, through experience and word-of-mouth, techniques and heuristics that help them successfully apply neural networks to di cult real world problems. Often these {\textbackslash}tricks" are theo- tically well motivated. Sometimes they are the result of trial and error. However, their most common link is that they are usually hidden in people’s heads or in the back pages of space-constrained conference papers. As a result newcomers to the eld waste much time wondering why their networks train so slowly and perform so poorly. This book is an outgrowth of a 1996 NIPS workshop called Tricks of the Trade whose goal was to begin the process of gathering and documenting these tricks. The interest that the workshop generated motivated us to expand our collection and compile it into this book. Although we have no doubt that there are many tricks we have missed, we hope that what we have included will prove to be useful, particularly to those who are relatively new to the eld. Each chapter contains one or more tricks presented by a given author (or authors). We have attempted to group related chapters into sections, though we recognize that the di erent sections are far from disjoint. Some of the chapters (e.g., 1, 13, 17) contain entire systems of tricks that are far more general than the category they have been placed in.},
	language = {en},
	urldate = {2019-12-10},
	publisher = {Springer-Verlag},
	editor = {Orr, Genevieve B. and Müller, Klaus-Robert},
	year = {1998},
	doi = {10.1007/3-540-49430-8}
}

@misc{noauthor_googles_nodate,
	title = {Google’s {AlphaGo} {AI} wins three-match series against the world’s best {Go} player – {TechCrunch}},
	url = {https://techcrunch.com/2017/05/24/alphago-beats-planets-best-human-go-player-ke-jie/amp/},
	urldate = {2019-12-10}
}

@article{marblestone_toward_2016,
	title = {Toward an {Integration} of {Deep} {Learning} and {Neuroscience}},
	volume = {10},
	issn = {1662-5188},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5021692/},
	doi = {10.3389/fncom.2016.00094},
	abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
	urldate = {2019-12-10},
	journal = {Frontiers in Computational Neuroscience},
	author = {Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
	month = sep,
	year = {2016},
	pmid = {27683554},
	pmcid = {PMC5021692}
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {0893-6080},
	shorttitle = {Deep learning in neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	language = {en},
	urldate = {2019-12-10},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	keywords = {Deep learning, Evolutionary computation, Reinforcement learning, Supervised learning, Unsupervised learning},
	pages = {85--117}
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	issn = {1939-3539},
	shorttitle = {Representation {Learning}},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = aug,
	year = {2013},
	keywords = {AI, Abstracts, Algorithms, Artificial Intelligence, Boltzmann machine, Deep learning, Feature extraction, Humans, Learning systems, Machine learning, Manifolds, Neural Networks (Computer), Neural networks, Speech recognition, artificial intelligence, autoencoder, autoencoders, data representation, data structures, density estimation, feature learning, geometrical connections, machine learning algorithms, manifold learning, neural nets, probabilistic models, probability, representation learning, unsupervised feature learning, unsupervised learning},
	pages = {1798--1828}
}

@misc{noauthor_mnist_nodate,
	title = {{MNIST} handwritten digit database, {Yann} {LeCun}, {Corinna} {Cortes} and {Chris} {Burges}},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2019-12-10}
}

@inproceedings{wu_gpgpu_2015,
	title = {{GPGPU} performance and power estimation using machine learning},
	doi = {10.1109/HPCA.2015.7056063},
	abstract = {Graphics Processing Units (GPUs) have numerous configuration and design options, including core frequency, number of parallel compute units (CUs), and available memory bandwidth. At many stages of the design process, it is important to estimate how application performance and power are impacted by these options. This paper describes a GPU performance and power estimation model that uses machine learning techniques on measurements from real GPU hardware. The model is trained on a collection of applications that are run at numerous different hardware configurations. From the measured performance and power data, the model learns how applications scale as the GPU's configuration is changed. Hardware performance counter values are then gathered when running a new application on a single GPU configuration. These dynamic counter values are fed into a neural network that predicts which scaling curve from the training data best represents this kernel. This scaling curve is then used to estimate the performance and power of the new application at different GPU configurations. Over an 8× range of the number of CUs, a 3.3× range of core frequencies, and a 2.9× range of memory bandwidth, our model's performance and power estimates are accurate to within 15\% and 10\% of real hardware, respectively. This is comparable to the accuracy of cycle-level simulators. However, after an initial training phase, our model runs as fast as, or faster than the program running natively on real hardware.},
	booktitle = {2015 {IEEE} 21st {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Wu, Gene and Greathouse, Joseph L. and Lyashevsky, Alexander and Jayasena, Nuwan and Chiou, Derek},
	month = feb,
	year = {2015},
	note = {ISSN: 2378-203X},
	keywords = {CU, Engines, GPGPU, Graphics processing units, Hardware, Kernel, Predictive models, Radiation detectors, Training, graphics processing units, learning (artificial intelligence), machine learning, memory bandwidth, parallel compute unit, performance estimation, performance evaluation, power estimation},
	pages = {564--576}
}

@inproceedings{song_simplified_2013,
	title = {A {Simplified} and {Accurate} {Model} of {Power}-{Performance} {Efficiency} on {Emergent} {GPU} {Architectures}},
	doi = {10.1109/IPDPS.2013.73},
	abstract = {Emergent heterogeneous systems must be optimized for both power and performance at exascale. Massive parallelism combined with complex memory hierarchies form a barrier to efficient application and architecture design. These challenges are exacerbated with GPUs as parallelism increases orders of magnitude and power consumption can easily double. Models have been proposed to isolate power and performance bottlenecks and identify their root causes. However, no current models combine simplicity, accuracy, and support for emergent GPU architectures (e.g. NVIDIA Fermi). We combine hardware performance counter data with machine learning and advanced analytics to model power-performance efficiency for modern GPU-based systems. Our performance counter based approach is simpler than previous approaches and does not require detailed understanding of the underlying architecture. The resulting model is accurate for predicting power (within 2.1\%) and performance (within 6.7\%) for application kernels on modern GPUs. Our model can identify power-performance bottlenecks and their root causes for various complex computation and memory access patterns (e.g. global, shared, texture). We measure the accuracy of our power and performance models on a NVIDIA Fermi C2075 GPU for more than a dozen CUDA applications. We show our power model is more accurate and robust than the best available GPU power models - multiple linear regression models MLR and MLR+. We demonstrate how to use our models to identify power-performance bottlenecks and suggest optimization strategies for high-performance codes such as GEM, a biomolecular electrostatic analysis application. We verify our power-performance model is accurate on clusters of NVIDIA Fermi M2090s and useful for suggesting optimal runtime configurations on the Keeneland supercomputer at Georgia Tech.},
	booktitle = {2013 {IEEE} 27th {International} {Symposium} on {Parallel} and {Distributed} {Processing}},
	author = {Song, Shuaiwen and Su, Chunyi and Rountree, Barry and Cameron, Kirk W.},
	month = may,
	year = {2013},
	note = {ISSN: 1530-2075},
	keywords = {Adaptation models, CUDA application, Graphics processing units, Kernel, NVIDIA Fermi M2090s, Predictive models, Radiation detectors, Runtime, Training, complex memory hierarchy, emergent GPU architecture, emergent heterogeneous system, graphics processing units, high-performance code, machine learning, memory access pattern, multiple linear regression model, optimisation, optimization strategy, parallel architectures, power consumption, power-performance efficiency, regression analysis},
	pages = {673--686}
}
@inproceedings{ghosh_statistical_2013,
	title = {Statistical modeling of power/energy of scientific kernels on a multi-{GPU} system},
	doi = {10.1109/IGCC.2013.6604488},
	abstract = {Energy efficiency of GPUs has facilitated the usage of GPUs in many complex scientific applications. Nodes with multi-GPUs along with multi-core CPUs are quite common in today's HPC landscape. This gives the flexibility to utilize CPUs or accelerators or even both according to the workload characteristics. It is not possible to measure power and energy accurately in all the cases, an alternate approach is to estimate power and energy using statistical methods. Apart from saving time and money, reasonable prediction of power/energy would lead to power saving optimizations for certain applications, without compromising performance. In this paper we employ parametric and non-parametric regression analysis to model power and energy consumption of some of the common high performance kernels (DGEMM, FFT, PRNG and FD stencils) on a multi-GPU platform. Our experiments show that using a minimal set of hardware counters and performance attributes, the average error between the measured and the predicted values of power and energy is only {\textasciitilde} 4\%.},
	author = {Ghosh, Sayan and Chandrasekaran, Sunita and Chapman, Barbara},
	month = jun,
	year = {2013},
	pages = {1--6}
}

@inproceedings{abe_power_2014,
	address = {Phoenix, AZ, USA},
	title = {Power and {Performance} {Characterization} and {Modeling} of {GPU}-{Accelerated} {Systems}},
	isbn = {978-1-4799-3800-1 978-1-4799-3799-8},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6877247},
	doi = {10.1109/IPDPS.2014.23},
	abstract = {Graphics processing units (GPUs) provide an order-of-magnitude improvement on peak performance and performance-per-watt as compared to traditional multicore CPUs. However, GPU-accelerated systems currently lack a generalized method of power and performance prediction, which prevents system designers from an ultimate goal of dynamic power and performance optimization. This is due to the fact that their power and performance characteristics are not well captured across architectures, and as a result, existing power and performance modeling approaches are only available for a limited range of particular GPUs. In this paper, we present power and performance characterization and modeling of GPUaccelerated systems across multiple generations of architectures. Characterization and modeling both play a vital role in optimization and prediction of GPU-accelerated systems. We quantify the impact of voltage and frequency scaling on each architecture with a particularly intriguing result that a cutting-edge Kepler-based GPU achieves energy saving of 75\% by lowering GPU clocks in the best scenario, while Fermi- and Tesla-based GPUs achieve no greater than 40\% and 13\%, respectively. Considering these characteristics, we provide statistical power and performance modeling of GPU-accelerated systems simpliﬁed enough to be applicable for multiple generations of architectures. One of our ﬁndings is that even simpliﬁed statistical models are able to predict power and performance of cutting-edge GPUs within errors of 20\% to 30\% for any set of voltage and frequency pair.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {2014 {IEEE} 28th {International} {Parallel} and {Distributed} {Processing} {Symposium}},
	publisher = {IEEE},
	author = {Abe, Yuki and Sasaki, Hiroshi and Kato, Shinpei and Inoue, Koji and Edahiro, Masato and Peres, Martin},
	month = may,
	year = {2014},
	pages = {113--122}
}

@inproceedings{chen_statistical_2011,
	title = {Statistical {GPU} power analysis using tree-based methods},
	doi = {10.1109/IGCC.2011.6008582},
	abstract = {Graphics Processing Units (GPUs) have emerged as a promising platform for parallel computation. With a large number of scalar processors and abundant memory bandwidth, GPUs provide substantial computation power. While delivering high computation performance, the GPU also consumes high power and needs to be equipped with sufficient power supplies and cooling systems. Therefore, it is essential to institute an efficient mechanism for evaluating and understanding the power consumption requirement when running real applications on high-end GPUs. In this paper, we present a high-level GPU power consumption model using sophisticated tree-based random forest methods which can correlate the power consumption with a set of independent performance variables. This statistical model not only predicts the GPU runtime power consumption accurately, but more importantly, it also provides sufficient insights for understanding the dependence between the GPU runtime power consumption and the individual performance metrics. In order to gain more insights, we use a GPU simulator that can collect more runtime performance metrics than hardware counters. We measure the power consumption of a wide-range of CUDA kernels on an experimental system with GTX 280 GPU as statistical samples for our power analysis. This methodology can certainly be applied to any other CUDA GPU.},
	author = {Chen, Jianmin and Li, Bin and Zhang, Ying and Peng, Lu and Peir, Jih-Kwon},
	month = aug,
	year = {2011},
	pages = {1--6}
}

@inproceedings{nagasaka_statistical_2010,
	address = {Washington, DC, USA},
	series = {{GREENCOMP} '10},
	title = {Statistical {Power} {Modeling} of {GPU} {Kernels} {Using} {Performance} {Counters}},
	isbn = {978-1-4244-7612-1},
	url = {http://dx.doi.org/10.1109/GREENCOMP.2010.5598315},
	doi = {10.1109/GREENCOMP.2010.5598315},
	abstract = {We present a statistical approach for estimating power consumption of GPU kernels. We use the GPU performance counters that are exposed for CUDA applications, and train a linear regression model where performance counters are used as independent variables and power consumption is the dependent variable. For model training and evaluation, we use publicly available CUDA applications, consisting of 49 kernels in the CUDA SDK and the Rodinia benchmark suite. Our regression model achieves highly accurate estimates for many of the tested kernels, where the average error ratio is 4.7\%. However, we also find that it fails to yield accurate estimates for kernels with texture reads because of the lack of performance counters for monitoring texture accesses, resulting in significant underestimation for such kernels.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the {International} {Conference} on {Green} {Computing}},
	publisher = {IEEE Computer Society},
	author = {Nagasaka, Hitoshi and Maruyama, Naoya and Nukada, Akira and Endo, Toshio and Matsuoka, Satoshi},
	year = {2010},
	pages = {115--122}
}

@misc{noauthor_polybench/c_nodate,
	title = {{PolyBench}/{C} -- {Homepage} of {Louis}-{Noël} {Pouchet}},
	url = {http://web.cs.ucla.edu/~pouchet/software/polybench/},
	urldate = {2019-12-09}
}

@article{stratton_parboil:_nodate,
	title = {Parboil: {A} {Revised} {Benchmark} {Suite} for {Scientiﬁc} and {Commercial} {Throughput} {Computing}},
	abstract = {The Parboil benchmarks are a set of throughput computing applications useful for studying the performance of throughput computing architecture and compilers. The name comes from the culinary term for a partial cooking process, which represents our belief that useful throughput computing benchmarks must be “cooked”, or preselected to implement a scalable algorithm with ﬁne-grained parallel tasks. But useful benchmarks for this ﬁeld cannot be “fully cooked”, because the architectures and programming models and supporting tools are evolving rapidly enough that static benchmark codes will lose relevance very quickly.},
	language = {en},
	author = {Stratton, John A and Rodrigues, Christopher and Sung, I-Jui and Obeid, Nady and Chang, Li-Wen and Anssari, Nasser and Liu, Geng Daniel and Hwu, Wen-mei W},
	pages = {12}
}

@inproceedings{sethia_equalizer:_2014,
	title = {Equalizer: {Dynamic} {Tuning} of {GPU} {Resources} for {Efficient} {Execution}},
	shorttitle = {Equalizer},
	doi = {10.1109/MICRO.2014.16},
	abstract = {GPUs use thousands of threads to provide high performance and efficiency. In general, if one thread of a kernel uses one of the resources (compute, bandwidth, data cache) more heavily, there will be significant contention for that resource due to the large number of identical concurrent threads. This contention will eventually saturate the performance of the kernel due to contention for the bottleneck resource, while at the same time leaving other resources underutilized. To overcome this problem, a runtime system that can tune the hardware to match the characteristics of a kernel can effectively mitigate the imbalance between resource requirements of kernels and the hardware resources present on the GPU. We propose Equalizer, a low overhead hardware runtime system, that dynamically monitors the resource requirements of a kernel and manages the amount of on-chip concurrency, core frequency and memory frequency to adapt the hardware to best match the needs of the running kernel. Equalizer provides efficiency in two modes. Firstly, it can save energy without significant performance degradation by GPUs use thousands of threads to provide high performance and efficiency. In general, if one thread of a kernel uses one of the resources (compute, bandwidth, data cache) more heavily, there will be significant contention for that resource due to the large number of identical concurrent threads. This contention will eventually saturate the performance of the kernel due to contention for the bottleneck resource, while at the same time leaving other resources underutilized. To overcome this problem, a runtime system that can tune the hardware to match the characteristics of a kernel can effectively mitigate the imbalance between resource requirements of kernels and the hardware resources present on the GPU. We propose Equalizer, a low overhead hardware runtime system, that dynamically monitors the resource requirements of a kernel and manages the amount of on-chip concurrency, core frequency and memory frequency to adapt the hardware to best match the needs of the running kernel. Equalizer provides efficiency in two modes. Firstly, it can save energy without significant performance degradation by throttling under-utilized resources. Secondly, it can boost bottleneck resources to reduce contention and provide higher performance without significant energy increase. Across a spectrum of 27 kernels, Equalizer achieves 15\% savings in energy mode and 22\% speedup in performance mode. Throttling under-utilized resources. Secondly, it can boost bottleneck resources to reduce contention and provide higher performance without significant energy increase. Across a spectrum of 27 kernels, Equalizer achieves 15\% savings in energy mode and 22\% speedup in performance mode.},
	booktitle = {2014 47th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	author = {Sethia, Ankit and Mahlke, Scott},
	month = dec,
	year = {2014},
	note = {ISSN: 2379-3155},
	keywords = {Dynamic Voltage and Frequency Scaling, Equalizer, Equalizers, GPGPUs, GPU resource dynamic tuning, Graphics processing units, Instruction sets, Kernel, Memory management, Pipelines, Resource Utilization, Runtime, Runtime System, bottleneck resource, concurrent threads, core frequency, energy conservation, energy saving, graphics processing units, hardware runtime system, kernel characteristics, kernel resource requirements, memory frequency, on-chip concurrency, resource allocation},
	pages = {647--658}
}

@inproceedings{lee_dynamic_2014,
	address = {New York, NY, USA},
	series = {{ISLPED} '14},
	title = {Dynamic {Thermal} {Management} for {FinFET}-based {Circuits} {Exploiting} the {Temperature} {Effect} {Inversion} {Phenomenon}},
	isbn = {978-1-4503-2975-0},
	url = {http://doi.acm.org/10.1145/2627369.2627608},
	doi = {10.1145/2627369.2627608},
	abstract = {Due to limits on the availability of the energy source in many mobile user platforms (ranging from handheld devices to portable electronics to deeply embedded devices) and concerns about how much heat can effectively be removed from chips, minimizing the power consumption has become a primary driver for system-on-chip designers. Because of their superb characteristics, FinFETs have emerged as a promising replacement for planar CMOS devices in sub-20nm CMOS technology nodes. However, based on extensive simulations, we have observed that the delay vs. temperature characteristics of FinFET-based circuits are fundamentally different from that of the conventional bulk CMOS circuits, i.e., the delay of a FinFET circuit decreases with increasing temperature even in the super-threshold supply voltage regime. Unfortunately, the leakage power dissipation of the FinFET-based circuits increases exponentially with the temperature. These two trends give rise to a tradeoff between delay and leakage power as a function of the chip temperature, and hence, lead to the definition of an optimum chip temperature operating point (i.e., one that balances concerns about the circuit speed and power efficiency.) This paper presents the results of our investigations into the aforesaid temperature effect inversion (TEI) and proposes a novel dynamic thermal management (DTM) algorithm, which exploits this phenomenon to minimize the energy consumption of FinFET-based circuits without any appreciable performance penalty. Experimental results demonstrate 40\% energy saving (with no performance penalty) can be achieved by the proposed TEI-aware DTM approach compared to the best-in-class DTMs that are unaware of this phenomenon.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2014 {International} {Symposium} on {Low} {Power} {Electronics} and {Design}},
	publisher = {ACM},
	author = {Lee, Woojoo and Wang, Yanzhi and Cui, Tiansong and Nazarian, Shahin and Pedram, Massoud},
	year = {2014},
	note = {event-place: La Jolla, California, USA},
	keywords = {finfet, low-power designs, thermal mangement},
	pages = {105--110}
}

@inproceedings{che_rodinia:_2009,
	address = {Austin, TX, USA},
	title = {Rodinia: {A} benchmark suite for heterogeneous computing},
	isbn = {978-1-4244-5156-2},
	shorttitle = {Rodinia},
	url = {http://ieeexplore.ieee.org/document/5306797/},
	doi = {10.1109/IISWC.2009.5306797},
	abstract = {This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley’s dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	publisher = {IEEE},
	author = {Che, Shuai and Boyer, Michael and Meng, Jiayuan and Tarjan, David and Sheaffer, Jeremy W. and Lee, Sang-Ha and Skadron, Kevin},
	month = oct,
	year = {2009},
	pages = {44--54}
}

@inproceedings{ge_effects_2013,
	title = {Effects of {Dynamic} {Voltage} and {Frequency} {Scaling} on a {K20} {GPU}},
	doi = {10.1109/ICPP.2013.98},
	abstract = {Improving energy efficiency is an ongoing challenge in HPC because of the ever-increasing need for performance coupled with power and economic constraints. Though GPU-accelerated heterogeneous computing systems are capable of delivering impressive performance, it is necessary to explore all available power-aware technologies to meet the inevitable energy efficiency challenge. In this paper, we experimentally study the impacts of DVFS on application performance and energy efficiency for GPU computing and compare them with those of DVFS for CPU computing. Based on a power-aware heterogeneous system that includes dual Intel Sandy Bridge CPUs and the latest Nvidia K20c Kepler GPU, the study provides numerous new insights, general trends and exceptions of DVFS for GPU computing. In general, the effects of DVFS on a GPU differ from those of DVFS on a CPU. For example, on a GPU running compute-bound high-performance and high-throughput workloads, the system performance and the power consumption are approximately proportional to the GPU frequency. Hence, with a permissible power limit, increasing the GPU frequency leads to better performance without incurring a noticeable increase in energy. This paper further provides detailed analytical explanations of the causes of the observed trends and exceptions. The findings presented in this paper have the potential to impact future CPU and GPU architectures to achieve better energy efficiency and point out directions for designing effective DVFS schedulers for heterogeneous systems.},
	author = {Ge, Rong and Vogt, Ryan and Majumder, AKM Jahangir and Alam, Mohammad  Arif Ul and Burtscher, Martin and Zong, Ziliang},
	month = oct,
	year = {2013},
	pages = {826--833}
}

@misc{noauthor_opencl_2013,
	title = {{OpenCL} - {The} open standard for parallel programming of heterogeneous systems},
	url = {https://www.khronos.org/opencl/},
	abstract = {OpenCL™ (Open Computing Language) is the open, royalty-free standard for cross-platform, parallel programming of diverse processors found in personal computers, servers, mobile devices and embedded platforms. OpenCL greatly improves the…},
	language = {en},
	urldate = {2019-12-08},
	journal = {The Khronos Group},
	month = jul,
	year = {2013}
}

@misc{noauthor_gpgpu-sim/gpgpu-sim_distribution_2019,
	title = {gpgpu-sim/gpgpu-sim\_distribution},
	url = {https://github.com/gpgpu-sim/gpgpu-sim_distribution},
	abstract = {GPGPU-Sim provides a detailed simulation model of a contemporary GPU running CUDA and/or OpenCL workloads and now includes an integrated (and validated) energy model, GPUWattch.},
	urldate = {2019-12-03},
	publisher = {gpgpu-sim},
	month = dec,
	year = {2019},
	note = {original-date: 2014-08-09T15:55:39Z}
}

@misc{mujtaba_amd_2019,
	title = {{AMD} {Radeon} {Graphics} {Cards} {Gain} {Market} {Share} {Versus} {NVIDIA} {GeForce}},
	url = {https://wccftech.com/amd-radeon-nvidia-geforce-graphics-card-gpu-market-share-q2-2019/},
	abstract = {AMD Radeon RX managed to gain major desktop discrete graphics share compared to NVIDIA GeForce RTX / GTX cards in Q2 2019.},
	language = {en-US},
	urldate = {2019-12-03},
	journal = {Wccftech},
	author = {Mujtaba, Hassan},
	month = sep,
	year = {2019}
}

@misc{hibben_nvidia:_2019,
	title = {Nvidia: {Gaining} {Share} {In} {A} {Declining} {Market}},
	shorttitle = {Nvidia},
	url = {https://seekingalpha.com/article/4248871-nvidia-gaining-share-in-declining-market},
	abstract = {Nvidia graphics card market share over 80\%, according to Jon Peddie Research. Explanations for the graphics card market decline. Motivating the Mellanox acquisi},
	language = {en},
	urldate = {2019-12-03},
	journal = {Seeking Alpha},
	author = {Hibben, Mark},
	month = mar,
	year = {2019}
}

@article{gonzalez_supply_1997,
	title = {Supply and threshold voltage scaling for low power {CMOS}},
	volume = {32},
	issn = {1558-173X},
	doi = {10.1109/4.604077},
	abstract = {This paper investigates the effect of lowering the supply and threshold voltages on the energy efficiency of CMOS circuits. Using a first-order model of the energy and delay of a CMOS circuit, we show that lowering the supply and threshold voltage is generally advantageous, especially when the transistors are velocity saturated and the nodes have a high activity factor, In fact, for modern submicron technologies, this simple analysis suggests optimal energy efficiency at supply voltages under 0.5 V. Other process and circuit parameters have almost no effect on this optimal operating point. If there is some uncertainty in the value of the threshold or supply voltage, however, the power advantage of this very low voltage operation diminishes. Therefore, unless active feedback is used to control the uncertainty, in the future the supply and threshold voltage will not decrease drastically, but rather will continue to scale down to maintain constant electric fields.},
	number = {8},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Gonzalez, R. and Gordon, B.M. and Horowitz, M.A.},
	month = aug,
	year = {1997},
	keywords = {0.25 mum, 120 mV, 250 mV, CMOS integrated circuits, CMOS process, CMOS technology, Circuits, Delay effects, Energy efficiency, Feedback, HSPICE models, Low voltage, SPICE, Semiconductor device modeling, Threshold voltage, Uncertainty, active feedback, circuit feedback, circuit optimisation, delay, delays, energy efficiency, energy-delay product, first-order model, high activity factor nodes, integrated circuit modelling, low power CMOS, optimal operating point, sleep mode, submicron technologies, supply voltage scaling, threshold voltage scaling, velocity saturated transistors},
	pages = {1210--1216}
}

@misc{noauthor_libsensors3:_nodate,
	title = {libsensors(3): publicly accessible functions - {Linux} man page},
	url = {https://linux.die.net/man/3/libsensors},
	urldate = {2019-11-28}
}

@misc{noauthor_rocm-developer-tools/rocprofiler_2019,
	title = {{ROCm}-{Developer}-{Tools}/rocprofiler},
	copyright = {MIT},
	url = {https://github.com/ROCm-Developer-Tools/rocprofiler},
	abstract = {ROC profiler library. Profiling with perf-counters and derived metrics.},
	urldate = {2019-11-28},
	publisher = {ROCm Developer Tools},
	month = nov,
	year = {2019},
	note = {original-date: 2018-07-17T15:49:55Z}
}

@misc{noauthor_radeonopencompute/roc-smi_2019,
	title = {{RadeonOpenCompute}/{ROC}-smi},
	url = {https://github.com/RadeonOpenCompute/ROC-smi},
	abstract = {ROC System Management Interface. Contribute to RadeonOpenCompute/ROC-smi development by creating an account on GitHub.},
	urldate = {2019-11-28},
	publisher = {ROCm Core Technology},
	month = nov,
	year = {2019},
	note = {original-date: 2016-05-11T20:56:10Z}
}

@misc{amd_radeons_nodate,
	title = {Radeon’s next-generation {Vega} architecture - {Whitepaper}},
	url = {https://www.techpowerup.com/gpu-specs/docs/amd-vega-architecture.pdf},
	language = {en-us},
	urldate = {2019-11-28},
	author = {{AMD}}
}

@book{hwu_heterogeneous_2015,
	address = {Amsterdam : Waltham, MA},
	edition = {1 edition},
	title = {Heterogeneous {System} {Architecture}: {A} {New} {Compute} {Platform} {Infrastructure}},
	isbn = {978-0-12-800386-2},
	shorttitle = {Heterogeneous {System} {Architecture}},
	abstract = {Heterogeneous Systems Architecture - a new compute platform infrastructure presents a next-generation hardware platform, and associated software, that allows processors of different types to work efficiently and cooperatively in shared memory from a single source program. HSA also defines a virtual ISA for parallel routines or kernels, which is vendor and ISA independent thus enabling single source programs to execute across any HSA compliant heterogeneous processer from those used in smartphones to supercomputers. The book begins with an overview of the evolution of heterogeneous parallel processing, associated problems, and how they are overcome with HSA. Later chapters provide a deeper perspective on topics such as the runtime, memory model, queuing, context switching, the architected queuing language, simulators, and tool chains. Finally, three real world examples are presented, which provide an early demonstration of how HSA can deliver significantly higher performance thru C++ based applications. Contributing authors are HSA Foundation members who are experts from both academia and industry. Some of these distinguished authors are listed here in alphabetical order: Yeh-Ching Chung, Benedict R. Gaster, Juan Gómez-Luna, Derek Hower, Lee Howes, Shih-Hao HungThomas B. Jablin, David Kaeli,Phil Rogers, Ben Sander, I-Jui (Ray) Sung.Provides clear and concise explanations of key HSA concepts and fundamentals by expert HSA Specification contributors Explains how performance-bound programming algorithms and application types can be significantly optimized by utilizing HSA hardware and software features Presents HSA simply, clearly, and concisely without reading the detailed HSA Specification documentsDemonstrates ideal mapping of processing resources from CPUs to many other heterogeneous processors that comply with HSA Specifications},
	language = {English},
	publisher = {Morgan Kaufmann},
	author = {Hwu, Wen-mei W.},
	month = dec,
	year = {2015}
}

@misc{noauthor_radeonopencompute/rocm_2019,
	title = {{RadeonOpenCompute}/{ROCm}},
	url = {https://github.com/RadeonOpenCompute/ROCm},
	abstract = {ROCm},
	urldate = {2019-11-28},
	publisher = {ROCm Core Technology},
	month = nov,
	year = {2019},
	note = {original-date: 2016-03-18T00:24:29Z}
}

@inproceedings{esmaeilzadeh_dark_2011,
	title = {Dark silicon and the end of multicore scaling},
	abstract = {Since 2005, processor designers have increased core counts to exploit Moore's Law scaling, rather than focusing on single-core performance. The failure of Dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single-core scaling has been curtailed. This paper models multicore scaling limits by combining device scaling, single-core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. For device scaling, we use both the ITRS projections and a set of more conservative device scaling parameters. To model single-core scaling, we combine measurements from over 150 processors to derive Pareto-optimal frontiers for area/performance and power/performance. Finally, to model multicore scaling, we build a detailed performance model of upper-bound performance and lower-bound core power. The multicore designs we study include single-threaded CPU-like and massively threaded GPU-like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. The study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. Even at 22 nm (just one year from now), 21\% of a fixed-size chip must be powered off, and at 8 nm, this number grows to more than 50\%. Through 2024, only 7.9× average speedup is possible across commonly used parallel workloads, leaving a nearly 24-fold gap from a target of doubled performance per generation.},
	booktitle = {2011 38th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Esmaeilzadeh, Hadi and Blem, Emily and Amant, Renée St. and Sankaralingam, Karthikeyan and Burger, Doug},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6897},
	keywords = {Dark Silicon, Dennard scaling, ITRS projections, Instruction sets, Microarchitecture, Modeling, Moore's law scaling, Multicore, Multicore processing, Organizations, Pareto-optimal frontiers, Performance evaluation, Power, Technology Scaling, Topology, Transistors, asymmetric topology, chip topology, composed topology, computing community, dark silicon, device scaling parameters, dynamic topology, fixed-size chip, graphics processing units, lower-bound core power, massively threaded GPU-like multicore chip organizations, microprocessor chips, multicore designs, multicore parts, multicore scaling limits, multiprocessing systems, network topology, parallel processing, parallel workloads, performance model, processor designers, single-core performance, single-core scaling, single-threaded CPU-like multicore chip organizations, speedup potential, technology generations, upper-bound performance},
	pages = {365--376}
}

@inproceedings{choo_understanding_2014,
	title = {Understanding and {Optimizing} {GPU} {Cache} {Memory} {Performance} for {Compute} {Workloads}},
	doi = {10.1109/ISPDC.2014.29},
	abstract = {Processing elements such as CPUs and GPUs depend on cache technology to bridge the classic processor memory subsystem performance gap. As GPUs evolve into general purpose co-processors with CPUs sharing the load, good cache design and use becomes increasingly important. While both CPUs and GPUs must cooperate and perform well, their memory access patterns are very different. On CPUs only a few threads access memory simultaneously. On GPUs, there is significantly higher memory access contention among thousands of threads. Despite such different behavior, there is little research that investigates the behavior and performance of GPU caches in depth. In this paper, we present our extensive study on the characterization and improvement of GPU cache behavior and performance for general-purpose workloads using a cycle-accurate ISA level GPU architectural simulator that models one of the latest GPU architectures, Graphics Core Next (GCN) from AMD. Our study makes the following observations and improvements. First, we observe that L1 vector data cache hit rate is substantially lower when compared to CPU caches. The main culprit is compulsory misses caused by lack of data reuse among massively simultaneous threads. Second, there is significant memory access contention in shared L2 data cache, accounting for up to 19\% of total access for some benchmarks. This high contention remains a main performance barrier in L2 data cache even though its hit rate is high. Third, we demonstrate that memory access coalescing plays a critical role in reducing memory traffic. Finally we found that there exists inter-workgroup locality which can affect the cache behavior and performance. Our experimental results show memory performance can be improved by 1) shared L1 vector data cache where multiple compute units share a single cache to exploit inter-workgroup locality and increase data reusability, and 2) clustered workgroup scheduling where workgroups with consecutive IDs are assigned on the same compute unit.},
	booktitle = {2014 {IEEE} 13th {International} {Symposium} on {Parallel} and {Distributed} {Computing}},
	author = {Choo, Kyoshin and Panlener, William and Jang, Byunghyun},
	month = jun,
	year = {2014},
	note = {ISSN: 2379-5352},
	keywords = {AMD, Benchmark testing, CPUs, Computer architecture, Discrete cosine transforms, GCN, GPU architectures, GPU cache behavior, GPU cache memory performance optimization, Graphics processing units, Hardware, Instruction sets, L1 vector data cache hit rate, Vectors, cache design, cache storage, cache technology, clustered workgroup scheduling, compute workloads, coprocessors, cycle-accurate ISA level GPU architectural simulator, data reusability, data reuse, general-purpose workloads, graphics core next, graphics processing units, interworkgroup locality, load sharing, memory access contention, memory access patterns, memory traffic reduction, processing elements, processor memory subsystem performance gap, scheduling, shared L1 vector data cache, shared L2 data cache},
	pages = {189--196}
}

@inproceedings{guerreiro_multi-kernel_2015,
	address = {Turku, Finland},
	title = {Multi-kernel {Auto}-{Tuning} on {GPUs}: {Performance} and {Energy}-{Aware} {Optimization}},
	isbn = {978-1-4799-8491-6},
	shorttitle = {Multi-kernel {Auto}-{Tuning} on {GPUs}},
	url = {http://ieeexplore.ieee.org/document/7092758/},
	doi = {10.1109/PDP.2015.44},
	language = {en},
	urldate = {2019-11-11},
	booktitle = {2015 23rd {Euromicro} {International} {Conference} on {Parallel}, {Distributed}, and {Network}-{Based} {Processing}},
	publisher = {IEEE},
	author = {Guerreiro, Joao and Ilic, Aleksandar and Roma, Nuno and Tomas, Pedro},
	month = mar,
	year = {2015},
	pages = {438--445}
}

@inproceedings{nishikawa_throughput_2013,
	title = {Throughput and {Power} {Efficiency} {Evaluations} of {Block} {Ciphers} on {Kepler} and {GCN} {GPUs}},
	doi = {10.1109/CANDAR.2013.65},
	abstract = {Computer systems with GPUs are expected to become a strong methodology for high-speed encryption processing. Moreover, power consumption is a primary deterrent for data center security on cloud services and handheld devices such as smartphones and tablet PCs. On the other hand, GPU vendors currently announce their future roadmaps of GPU architecture development, Nvidia Corp. accentuates Kepler architecture and AMD Corp. does GCN architecture. That's why in this paper we evaluated throughput and power efficiency of three 128-bit block ciphers on GPUs with recent Nvidia Kepler and AMD GCN architectures. In accordance with our experiments, whereas the throughput and per-watt throughput of AES-128 on Radeon HD 7970 (2048 cores) with GCN architecture is respectively extremely high 219.9 Gbps and 1310.7 Mbps/W, those on Geforce GTX 680 (1536 cores) with Kepler architecture be respectively considerably low 68.6 Gbps and 471.7 Mbps/W. Next, in order to investigate this mysterious experimental result, we used our micro-benchmark suites. They cleared up the reason, arithmetic logical instructions are required by encryption processing but are eliminated from some of the processing cores in Kepler architecture, unlike GCN's.},
	booktitle = {2013 {First} {International} {Symposium} on {Computing} and {Networking}},
	author = {Nishikawa, Naoki and Iwai, Keisuke and Tanaka, Hidema and Kurokawa, Takakazu},
	month = dec,
	year = {2013},
	note = {ISSN: 2379-1896},
	keywords = {AES, AES-128, AMD Corp, AMD GCN architectures, Camellia, Ciphers, Computer architecture, Encryption, GCN GPUs, GPU, GPU architecture development, Geforce GTX 680, Graphics Core Next, Graphics processing units, High definition video, Kepler, Kepler architecture, Nvidia Corp, OpenCL, Power demand, Power efficiency, Radeon HD 7970, SC2000, Throughput, arithmetic logical instructions, bit rate 219.9 Gbit/s, bit rate 68.6 Gbit/s, block ciphers, cloud services, computer centres, computer systems, cryptography, data center security, graphics processing units, handheld devices, high-speed encryption processing, microbenchmark suites, power consumption, power efficiency evaluations, smartphones, tablet PCs, throughput evaluation, word length 128 bit},
	pages = {366--372}
}

@inproceedings{dutta_gpu_2018,
	address = {New York, NY, USA},
	series = {{CF} '18},
	title = {{GPU} {Power} {Prediction} via {Ensemble} {Machine} {Learning} for {DVFS} {Space} {Exploration}},
	isbn = {978-1-4503-5761-6},
	url = {http://doi.acm.org/10.1145/3203217.3203273},
	doi = {10.1145/3203217.3203273},
	abstract = {A software-based approach to achieve high performance within a power budget often involves dynamic voltage and frequency scaling (DVFS). Thus, accurately predicting the power consumption of an application at different DVFS levels (or more generally, different processor configurations) is paramount for the energy-efficient functioning of a high-performance computing (HPC) system. The increasing prevalence of graphics processing units (GPUs) in HPC systems presents new challenges in power management, and machine learning presents an unique way to improve the software-based power management of these systems. As such, we explore the problem of GPU power prediction at different DVFS states via machine learning. Specifically, we propose a new ensemble technique that incorporates three machine-learning techniques --- sequential minimal optimization regression, simple linear regression, and decision tree --- to reduce the mean absolute error (MAE) to 3.5\%.},
	urldate = {2019-11-21},
	booktitle = {Proceedings of the 15th {ACM} {International} {Conference} on {Computing} {Frontiers}},
	publisher = {ACM},
	author = {Dutta, Bishwajit and Adhinarayanan, Vignesh and Feng, Wu-chun},
	year = {2018},
	note = {event-place: Ischia, Italy},
	pages = {240--243}
}

@article{wang_benchmarking_2019,
	title = {Benchmarking the {Performance} and {Power} of {AI} {Accelerators} for {AI} {Training}},
	url = {http://arxiv.org/abs/1909.06842},
	abstract = {Deep learning has become widely used in complex AI applications. Yet, training a deep neural network (DNNs) model requires a huge amount of calculations, taking a long running time and consuming a lot of energy. Nowadays, many-core AI accelerators (e.g., GPUs and TPUs) are designed to improve the AI training performance. However, different processors from different vendors perform very differently in terms of performance and power consumption. To investigate the differences among several popular off-the-shelf processors (i.e., Intel CPU, NVIDIA GPU, AMD GPU and Google TPU) in training DNNs, we carry out a detailed benchmark study on the performance and power (when possible) of these processors when training a representative set of DNNs, including three classical convolutional neural networks (CNNs), a recurrent neural network (LSTM), Deep Speech 2, and Transformer. We try to understand the impact of hardware, vendor's software library, and deep learning framework on the final training performance. Our evaluation results make two valuable directions for end-users and vendors. For the end-users, the evaluation results provide a guide for selecting a proper accelerator for training DNN models. For the vendors, some advantages and disadvantages revealed in our evaluation results could be useful for future architecture design and software library optimization.},
	urldate = {2019-11-21},
	journal = {arXiv:1909.06842 [cs]},
	author = {Wang, Yuxin and Wang, Qiang and Shi, Shaohuai and He, Xin and Tang, Zhenheng and Zhao, Kaiyong and Chu, Xiaowen},
	month = nov,
	year = {2019},
	note = {arXiv: 1909.06842},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning}
}

@article{guerreiro_modeling_2019,
	title = {Modeling and {Decoupling} the {GPU} {Power} {Consumption} for {Cross}-{Domain} {DVFS}},
	volume = {30},
	issn = {1045-9219, 1558-2183, 2161-9883},
	doi = {10.1109/TPDS.2019.2917181},
	abstract = {Dynamic voltage and frequency scaling (DVFS) is a popular technique to improve the energy-efficiency of high-performance computing systems. It allows placing the devices into lower performance states when the computational demands are lower, opening the possibility for significant power/energy savings. This work presents a GPU power consumption model, used to predict the GPU power consumption of any application at different frequency levels. To obtain this model, an estimation algorithm is proposed, relying on careful benchmarking of the GPU architecture. The model can estimate the contribution of twelve different GPU components (FP32-ADD/MUL/FMA, FP64-ADD/MUL/FMA, INT, SF, CF units, shared memory, L2-cache, and DRAM) to the GPU power consumption. Different model use cases are evaluated (fixed-frequency, DVFS, and scaling-factors), which can obtain both the total or the per-component GPU power consumption. A technique to export models to a distinct GPU from the one it was estimated on is also proposed. These approaches were extensively validated on five different GPUs from the three most recent microarchitectures with a set of 42 standard benchmarks, achieving very accurate predictions. In particular, the scaling-factor power model achieves an average prediction error of 3.5 percent (Titan Xp), 4.6 percent (GTX Titan X), 3.1 percent (GTX 980) and 2.4 percent (Tesla K40c).},
	number = {11},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Guerreiro, João and Ilic, Aleksandar and Roma, Nuno and Tomás, Pedro},
	month = nov,
	year = {2019},
	keywords = {Benchmark testing, Computational modeling, DVFS, Frequency-domain analysis, GPGPU, Graphics processing units, Memory management, Power demand, Predictive models, power modeling, scaling-factors},
	pages = {2494--2506}
}

@article{mittal_survey_2014,
	title = {A {Survey} of {Methods} {For} {Analyzing} and {Improving} {GPU} {Energy} {Efficiency}},
	volume = {47},
	doi = {10.1145/2636342},
	abstract = {Recent years have witnessed a phenomenal growth in the computational capabilities and applications of GPUs. However, this trend has also led to dramatic increase in their power consumption. This paper surveys research works on analyzing and improving energy efficiency of GPUs. It also provides a classification of these techniques on the basis of their main research idea. Further, it attempts to synthesize research works which compare energy efficiency of GPUs with other computing systems, e.g. FPGAs and CPUs. The aim of this survey is to provide researchers with knowledge of state-of-the-art in GPU power management and motivate them to architect highly energy-efficient GPUs of tomorrow.},
	journal = {ACM Computing Surveys},
	author = {Mittal, Sparsh and Vetter, Jeffrey},
	month = apr,
	year = {2014}
}

@inproceedings{tan_combating_2016,
	address = {Haifa, Israel},
	title = {Combating the {Reliability} {Challenge} of {GPU} {Register} {File} at {Low} {Supply} {Voltage}},
	isbn = {978-1-4503-4121-9},
	url = {http://dl.acm.org/citation.cfm?doid=2967938.2967951},
	doi = {10.1145/2967938.2967951},
	abstract = {Supply voltage reduction is an eﬀective approach to significantly reduce GPU energy consumption. As the largest on-chip storage structure, the GPU register ﬁle becomes the reliability hotspot that prevents further supply voltage reduction below the safe limit (Vmin) due to process variation eﬀects. This work addresses the reliability challenge of the GPU register ﬁle at low supply voltages, which is an essential ﬁrst step for aggressive supply voltage reduction of the entire GPU chip. To better understand the reliability issues posed by undervolting and its energy-saving potential, we ﬁrst rigorously model and analyze the process variation impact on the GPU register ﬁle at diﬀerent voltages. By further analyzing the GPU architecture, we make a key observation that the time GPU registers contain useless data (i.e., dead time) is long, providing a unique opportunity to enhance register reliability. We then propose GR-Guard, an architectural solution that leverages long register dead time to enable reliable operations from unreliable register ﬁle at low voltages. GR-Guard is both eﬀective and lowcost, and does not aﬀect normal (i.e., non-faulty) register accesses. Experimental results show that for a 28nm baseline GPU under aggressive voltage reduction, GR-Guard can maintain the register ﬁle reliability with less than 2\% overall performance degradation, while achieving an average of 31\% energy reduction across various applications.},
	language = {en},
	urldate = {2019-11-18},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Parallel} {Architectures} and {Compilation} - {PACT} '16},
	publisher = {ACM Press},
	author = {Tan, Jingweijia and Song, Shuaiwen Leon and Yan, Kaige and Fu, Xin and Marquez, Andres and Kerbyson, Darren},
	year = {2016},
	pages = {3--15}
}

@article{park_aggressive_2018,
	title = {Aggressive {Voltage} and {Temperature} {Control} for {Power} {Saving} in {Mobile} {Application} {Processors}},
	volume = {17},
	issn = {1536-1233, 1558-0660, 2161-9875},
	doi = {10.1109/TMC.2017.2762670},
	abstract = {DVFS is a widely used methodology for reducing the power consumption of mobile devices. This scheme involves frequency scaling in accordance with a specific governor and the establishment of an operating voltage to be paired with frequency. Incorporated into the settings for operating voltage is a guardband that ensures safe processor operation even at the worst conditions of on-chip temperature. Typically, the processor temperature remains at a normal range (i.e., not the worst), hence the voltage guardband set to guarantee safe operation is overly protected. In this paper, we propose a temperature-aware DVS (T-DVS) that aggressively reduces voltage guardband. We explore the opportunity to provide minimum operating voltages for frequencies at different temperatures and realize a dynamic voltage control scheme that reduces power consumption. The T-DVS manages temperature so that it remains in the “green zone” where maximum voltage gain is enabled for power-efficient operation. We validate the effectiveness of the T-DVS under various thermal conditions by using mobile application processors and different operating scenarios. Experimental results show that the T-DVS leads to power gain without degrading performance regardless of thermal conditions and chip characteristics. By examining the real-world applications of and off-the-shelf smartphone, we show that the voltage gains generated by the T-DVS results in battery lifetime increment.},
	number = {6},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Park, JInsoo and Cha, Hojung},
	month = jun,
	year = {2018},
	keywords = {DVFS, Power demand, System-on-chip, T-DVS, Temperature control, Temperature distribution, Temperature measurement, Voltage control, aggressive voltage, battery lifetime increment, chip characteristics, dynamic voltage control scheme, frequency scaling, green zone, low-power electronics, maximum voltage gain, microprocessor chips, minimum operating voltages, mobile application processors, mobile devices, on-chip temperature, operating voltage, power aware computing, power consumption, power gain, power-efficient operation, processor temperature, safe processor operation, smart phones, temperature, temperature control, temperature-aware DVS, thermal conditions, voltage control, voltage gains, voltage guardband},
	pages = {1233--1246}
}
@inproceedings{leng_safe_2015,
	title = {Safe limits on voltage reduction efficiency in {GPUs}: {A} direct measurement approach},
	shorttitle = {Safe limits on voltage reduction efficiency in {GPUs}},
	doi = {10.1145/2830772.2830811},
	abstract = {Energy efficiency of GPU architectures has emerged as an important aspect of computer system design. In this paper, we explore the energy benefits of reducing the GPU chip's voltage to the safe limit, i.e. Vmin point. We perform such a study on several commercial off-the-shelf GPU cards. We find that there exists about 20\% voltage guardband on those GPUs spanning two architectural generations, which, if "eliminated" completely, can result in up to 25\% energy savings on one of the studied GPU cards. The exact improvement magnitude depends on the program's available guardband, because our measurement results unveil a program dependent Vmin behavior across the studied programs. We make fundamental observations about the program-dependent Vmin behavior. We experimentally determine that the voltage noise has a larger impact on Vmin compared to the process and temperature variation, and the activities during the kernel execution cause large voltage droops. From these findings, we show how to use a kernel's microarchitectural performance counters to predict its Vmin value accurately. The average and maximum prediction errors are 0.5\% and 3\%, respectively. The accurate Vmin prediction opens up new possibilities of a cross-layer dynamic guardbanding scheme for GPUs, in which software predicts and manages the voltage guardband, while the functional correctness is ensured by a hardware safety net mechanism.},
	booktitle = {2015 48th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Leng, Jingwen and Buyuktosunoglu, Alper and Bertran, Ramon and Bose, Pradip and Reddi, Vijay Janapa},
	month = dec,
	year = {2015},
	note = {ISSN: 2379-3155},
	keywords = {GPU architectures, GPU cards, GPU chip voltage, GPU spanning, Graphics processing units, Kernel, Power measurement, Radiation detectors, Semiconductor device measurement, Temperature measurement, Voltage measurement, computer system design, cross-layer dynamic guardbanding, graphics processing units, hardware safety net mechanism, kernel execution, kernel microarchitectural performance counters, temperature variation, voltage guardband, voltage reduction efficiency},
	pages = {294--307}
}

@article{mahdiani_efficient_2017,
	title = {Efficient utilization of imprecise computational blocks for hardware implementation of imprecision tolerant applications},
	volume = {61},
	issn = {0026-2692},
	url = {http://www.sciencedirect.com/science/article/pii/S0026269217300058},
	doi = {10.1016/j.mejo.2017.01.002},
	abstract = {Recently, it has been reported that exploiting imprecise arithmetic building blocks such as adders and multipliers significantly improves digital implementation costs as well as performance of an important category of systems named as Imprecision Tolerant (IT) applications. We have categorized this new type of functional units as Bio-inspired Imprecise Computational (BIC) blocks. To efficiently exploit BICs in an IT application, however, the traditional hardware design flow should also be customized based on unique features of this new type of computing blocks. The most significant modification on traditional design flow to maximize the cost-performance of BIC implementations is to verify that the application is capable of tolerating those types of errors which are inherently introduced by the selected BIC based on its internal structure. We call this “error-behavior compatibility matching” between the system and the selected BIC building blocks. In this paper, we introduce and explain a customized hardware design flow for BICs with the main focus on error-behavior compatibility matching process as the main difference between traditional and BIC design flows. Two different error-behavior compatibility matching strategies are also introduced and their applicability is verified by applying them to exploit BICs for hardware implementation of some significant case studies including a general multiply-accumulate (MAC) block as the basic building block of many signal processing applications as well as an Artificial Neural Network (ANN) as a critical instance of MAC-based IT applications.},
	language = {en},
	urldate = {2019-11-18},
	journal = {Microelectronics Journal},
	author = {Mahdiani, Hamid Reza and Haji Seyed Javadi, Mohammad and Fakhraie, Sied Mehdi},
	month = mar,
	year = {2017},
	keywords = {Bio-inspired Imprecise Computational (BIC) block, Error-behavior compatibility matching, Imprecision Tolerant (IT) application, Multiply-accumulate, VLSI design flow},
	pages = {57--66}
}

@article{ibrahim_overview_2016,
	series = {12th {International} {Conference} on {Application} of {Fuzzy} {Systems} and {Soft} {Computing}, {ICAFS} 2016, 29-30 {August} 2016, {Vienna}, {Austria}},
	title = {An {Overview} of {Soft} {Computing}},
	volume = {102},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050916325467},
	doi = {10.1016/j.procs.2016.09.366},
	abstract = {Soft computing, as opposed to traditional computing, deals with approximate models and gives solutions to complex real-life problems. Unlike hard computing, soft computing is tolerant of imprecision, uncertainty, partial truth, and approximations. In effect, the role model for soft computing is the human mind. Soft computing is based on techniques such as fuzzy logic, genetic algorithms, artificial neural networks, machine learning, and expert systems. Although soft computing theory and techniques were first introduced in 1980s, it has now become a major research and study area in automatic control engineering. The techniques of soft computing are nowadays being used successfully in many domestic, commercial, and industrial applications. With the advent of the low-cost and very high performance digital processors and the reduction of the cost of memory chips it is clear that the techniques and application areas of soft computing will continue to expand. This paper gives an overview of the current state of soft computing techniques and describes the advantages and disadvantages of soft computing compared to traditional hard computing techniques.},
	language = {en},
	urldate = {2019-11-18},
	journal = {Procedia Computer Science},
	author = {Ibrahim, Dogan},
	month = jan,
	year = {2016},
	keywords = {Soft computing, expert system, fuzzy logic, genetic algorithms, neural networks},
	pages = {34--38}
}

@article{huang_gpu_2019,
	title = {{GPU} {Energy} {Consumption} {Optimization} {With} a {Global}-{Based} {Neural} {Network} {Method}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2915380},
	abstract = {With the widespread use of smart technologies, graphics processing unit (GPU) power-optimization issues are becoming increasingly important. Many researchers have tried to use dynamic voltage and frequency scaling (DVFS) technology to optimize a GPU's internal energy consumption. However, DVFS energy management often has difficulty balancing GPU performance and energy efficiency. This paper aims to implement a DVFS energy management strategy. We constructed a new type of neural network to a GPU-based energy management scheme, implemented the global-based DVFS model, and explored its implementation details. Using a master-slave model, we built a global energy control solution strategy. This strategy performs global collaborative DVFS adjustments on the GPU's energy consumption module based on task characteristics. Through the software construction and implementation of the global-based DVFS model, we proved that the strategy improves the GPU performance while improving the GPU's energy efficiency. We conducted performance and energy tests on three GPUs on the Tesla, Fermi, and Kepler platforms. The experiments showed that this strategy improved the performance and power consumption of GPUs based on each of the platforms.},
	journal = {IEEE Access},
	author = {Huang, Yanhui and Guo, Bing and Shen, Yan},
	year = {2019},
	keywords = {Collaborative control, DVFS energy management strategy, DVFS method, Energy consumption, Energy management, GPU energy consumption module, GPU energy consumption optimization, GPU energy efficiency, GPU internal energy consumption, GPU-based energy management scheme, Graphics processing units, Neurons, Optimization, Power demand, Task analysis, difficulty balancing GPU performance, dynamic voltage, energy conservation, energy consumption, energy management systems, energy tests, frequency scaling, global collaborative DVFS adjustments, global energy control solution strategy, global-based DVFS model, global-based energy optimization, global-based neural network method, graphics processing unit power-optimization issues, graphics processing units, master-slave model, neural nets, optimisation, performance evaluation, power aware computing, power consumption, smart technologies, software construction, task feature},
	pages = {64303--64314}
}

@article{nakhaee_lifetime_2018,
	title = {Lifetime improvement by exploiting aggressive voltage scaling during runtime of error-resilient applications},
	volume = {61},
	issn = {0167-9260},
	url = {http://www.sciencedirect.com/science/article/pii/S0167926017306363},
	doi = {10.1016/j.vlsi.2017.10.013},
	abstract = {In this paper, we present an accuracy-aware operating voltage management unit to improve the lifetime of processors by considering the error-resilient nature of some applications. This unit is placed in the power management unit of the processor and its operation is based on the aggressive operating voltage reduction during the runtime of error resilient applications. This unit determines the operating voltage of the processor based on the type of the running application, the predefined minimum acceptable quality, and the operating voltage level specified by the dynamic voltage frequency scaling controller of the processor power management unit. The determined operating voltage by this unit results in lifetime improvement and power reduction at the cost of timing violations that are tolerable by error-resilient applications. In addition, the proposed unit dynamically adjusts the minimum acceptable operating voltage based on the impact of aging mechanisms. The aging mechanisms considered in this work include Negative Bias Temperature Instability, Hot Carrier Injection, Time Dependent Dielectric Breakdown, Thermal Cycling, Electro-Migration, and Stress Migration. The efficacy of the proposed operating voltage management is investigated by applying it to some exact and error-resilient applications. The results show that the proposed unit lead to, on average, 38.82\% lifetime improvement as well as 41.8\% power consumption reduction.},
	language = {en},
	urldate = {2019-11-18},
	journal = {Integration},
	author = {Nakhaee, Farzaneh and Kamal, Mehdi and Afzali-Kusha, Ali and Pedram, Massoud and Fakhraie, Sied Mehdi and Dorosti, Hamed},
	month = mar,
	year = {2018},
	keywords = {Aging mechanisms, Dynamic voltage scaling, Lifetime improvement, Quality degradation},
	pages = {29--38}
}

@inproceedings{lew_analyzing_2019,
	title = {Analyzing {Machine} {Learning} {Workloads} {Using} a {Detailed} {GPU} {Simulator}},
	doi = {10.1109/ISPASS.2019.00028},
	abstract = {Machine learning (ML) has recently emerged as an important application driving future architecture design. Traditionally, architecture research has used detailed simulators to model and measure the impact of proposed changes. However, current open-source, publicly available simulators lack support for running a full ML stack like PyTorch. High-confidence, cycle-accurate simulations are crucial for architecture research and without them, it is difficult to rapidly prototype new ideas. In this paper, we describe changes we made to GPGPU-Sim, a popular, widely used GPU simulator, to run ML applications that use cuDNN and PyTorch, two widely used frameworks for running Deep Neural Networks (DNNs). This work has the potential to enable significant microarchitectural research into GPUs for DNNs. Our results show that the modified simulator, which has been made publicly available with this paper 1Source code available at https://github.com/gpgpu-sim/gpgpu-sim\_distribution (dev branch), provides execution time results within 18\% of real hardware. We further use it to study other ML workloads and demonstrate how the simulator identifies opportunities for architectural optimization that prior tools are unable to provide.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software} ({ISPASS})},
	author = {Lew, Jonathan and Shah, Deval A. and Pati, Suchita and Cattell, Shaylin and Zhang, Mengchi and Sandhupatla, Amruth and Ng, Christopher and Goli, Negar and Sinclair, Matthew D. and Rogers, Timothy G. and Aamodt, Tor M.},
	month = mar,
	year = {2019},
	keywords = {Computer architecture, Convolution, DNNs, GPGPU-Sim, GPU simulator, Graphics processing units, Hardware, Libraries, ML applications, ML stack, ML workloads, Machine Learning, Machine learning, Py-Torch, Tools, architectural optimization, architecture design, computer architecture, cuDNN, cycle-accurate simulations, deep neural networks, graphics processing units, learning (artificial intelligence), machine learning workloads, microarchitectural research, neural nets},
	pages = {151--152}
}

@inproceedings{sun_evaluating_2018,
	title = {Evaluating {Performance} {Tradeoffs} on the {Radeon} {Open} {Compute} {Platform}},
	doi = {10.1109/ISPASS.2018.00034},
	abstract = {GPUs have been shown to deliver impressive computing performance, while also providing high energy efficiency, across a wide range of high-performance and embedded system workloads. However, limited support for efficient communication and synchronization between the CPU and the GPU impacts our ability to fully exploit the benefits of heterogeneous systems. Recently, the Heterogeneous System Architecture (HSA) was introduced to address these issues with synchronization and communication, but given the low-level nature of HSA, it was not easily adopted by the broader programming community. In 2016, AMD described the Radeon Open Compute (ROC) platform that brings high-level programming frameworks such as OpenCL, HC++, and HIP to end users. These high-level programming frameworks offer a simpler programming experience by wrapping complex HSA APIs, while still delivering the power of HSA. To date, there has been little evaluation of the potential performance benefits and trade-offs of leveraging the ROC platform. In this work, we evaluate the performance of the ROC platform using the Hetero-Mark and DNNMark benchmark suites. Equipped with Hetero-Mark, we compare the performance of different programming frameworks, including OpenCL, HC++, and HIP on both integrated APUs and discrete GPUs. We also present three new CPU-GPU collaborative patterns and employ three new benchmarks to evaluate system-level atomics. With DNNMark and a new DNN Face Detection benchmark, we evaluate the performance of ROC libraries including rocBLAS and MIOpen. We also provide guidance on best practices to programmers when developing applications leveraging the ROC platform.},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software} ({ISPASS})},
	author = {Sun, Yifan and Mukherjee, Saoni and Baruah, Trinayan and Dong, Shi and Gutierrez, Julian and Mohan, Prannoy and Kaeli, David},
	month = apr,
	year = {2018},
	keywords = {Benchmark testing, CPU-GPU collaborative patterns, DNN Face Detection benchmark, DNNMark benchmark suites, GPU, Graphics processing units, HC++, HSA, HSA API, Hetero-Mark, Heterogeneous System Architecture, Hip, Kernel, OpenCL, Performance, Performance evaluation, Programming, ROC, ROC platform, Runtime, application program interfaces, coprocessors, embedded system workloads, embedded systems, energy conservation, energy efficiency, graphics processing units, high-level programming frameworks, multiprocessing systems, power aware computing, radeon open compute platform, synchronisation, system-level atomics},
	pages = {209--218}
}

@inproceedings{zhang_thundervolt:_2018,
	address = {San Francisco, California},
	title = {Thundervolt: enabling aggressive voltage underscaling and timing error resilience for energy efficient deep learning accelerators},
	isbn = {978-1-4503-5700-5},
	shorttitle = {Thundervolt},
	url = {http://dl.acm.org/citation.cfm?doid=3195970.3196129},
	doi = {10.1145/3195970.3196129},
	abstract = {Hardware accelerators are being increasingly deployed to boost the performance and energy efficiency of deep neural network (DNN) inference. In this paper we propose Thundervolt, a new framework that enables aggressive voltage underscaling of high-performance DNN accelerators without compromising classification accuracy even in the presence of high timing error rates. Using post-synthesis timing simulations of a DNN accelerator modeled on the Google TPU, we show that Thundervolt enables between 34\%-57\% energy savings on state-of-the-art speech and image recognition benchmarks with less than 1\% loss in classification accuracy and no performance loss. Further, we show that Thundervolt is synergistic with and can further increase the energy efficiency of commonly used run-time DNN pruning techniques like Zero-Skip.},
	language = {en},
	urldate = {2019-11-18},
	booktitle = {Proceedings of the 55th {Annual} {Design} {Automation} {Conference} on - {DAC} '18},
	publisher = {ACM Press},
	author = {Zhang, Jeff and Rangineni, Kartheek and Ghodsi, Zahra and Garg, Siddharth},
	year = {2018},
	pages = {1--6}
}

@article{jorda_performance_2019,
	title = {Performance {Evaluation} of {cuDNN} {Convolution} {Algorithms} on {NVIDIA} {Volta} {GPUs}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2918851},
	abstract = {Convolutional neural networks (CNNs) have recently attracted considerable attention due to their outstanding accuracy in applications, such as image recognition and natural language processing. While one advantage of the CNNs over other types of neural networks is their reduced computational cost, faster execution is still desired for both training and inference. Since convolution operations pose most of the execution time, multiple algorithms were and are being developed with the aim of accelerating this type of operations. However, due to the wide range of convolution parameter configurations used in the CNNs and the possible data type representations, it is not straightforward to assess in advance which of the available algorithms will be the best performing in each particular case. In this paper, we present a performance evaluation of the convolution algorithms provided by the cuDNN, the library used by most deep learning frameworks for their GPU operations. In our analysis, we leverage the convolution parameter configurations from widely used the CNNs and discuss which algorithms are better suited depending on the convolution parameters for both 32 and 16-bit floating-point (FP) data representations. Our results show that the filter size and the number of inputs are the most significant parameters when selecting a GPU convolution algorithm for 32-bit FP data. For 16-bit FP, leveraging specialized arithmetic units (NVIDIA Tensor Cores) is key to obtain the best performance.},
	journal = {IEEE Access},
	author = {Jordà, Marc and Valero-Lara, Pedro and Peña, Antonio J.},
	year = {2019},
	keywords = {16-bit floating-point data representations, 32-bit FP data, CNN, Convolution, GPU, GPU convolution algorithm, GPU operations, Graphics processing units, NVIDIA Tensor Cores, NVIDIA volta GPU, Neural network, Neural networks, Performance evaluation, Three-dimensional displays, Training, Two dimensional displays, convolution, convolution parameter configurations, convolution parameters, convolutional neural nets, convolutional neural networks, cuDNN, cuDNN convolution algorithms, data type representations, deep learning, deep learning frameworks, graphics processing units, image recognition, inference, learning (artificial intelligence), natural language processing, performance evaluation, reduced computational cost, volta},
	pages = {70461--70473}
}

@inproceedings{dong_dnnmark:_2017,
	address = {New York, NY, USA},
	series = {{GPGPU}-10},
	title = {{DNNMark}: {A} {Deep} {Neural} {Network} {Benchmark} {Suite} for {GPUs}},
	isbn = {978-1-4503-4915-4},
	shorttitle = {{DNNMark}},
	url = {http://doi.acm.org/10.1145/3038228.3038239},
	doi = {10.1145/3038228.3038239},
	abstract = {Deep learning algorithms have been growing in popularity in the machine learning community based on their ability to accurately perform clustering and classification in a number of domains. One commonly used class of deep learning techniques is deep neural networks (DNNs). They are composed of a massive number of artificial neurons and many hidden layers. As a complex scientific computing problem, deep neural networks encompass a rich set of computing-intensive and data-intensive workloads including convolution, pooling, and inner products. All of these workloads can be used as standalone programs to benchmark hardware performance. As the GPU develops into a popular platform used to run deep learning algorithms, hardware architects should be equipped with a representative set of benchmarks that can be used to explore design tradeoffs. This suite of workloads can be constructed from a number of primitive operations commonly found in deep neural networks. In this paper, we present DNNMark, a GPU benchmark suite that consists of a collection of deep neural network primitives, covering a rich set of GPU computing patterns. This suite is designed to be a highly configurable, extensible, and flexible framework, in which benchmarks can run either individually or collectively. The goal is to provide hardware and software developers with a set of kernels that can be used to develop increasingly complex workload scenarios. We also evaluate selected benchmarks in the suite and showcase their execution behavior on a Nvidia K40 GPU.},
	urldate = {2019-11-18},
	booktitle = {Proceedings of the {General} {Purpose} {GPUs}},
	publisher = {ACM},
	author = {Dong, Shi and Kaeli, David},
	year = {2017},
	note = {event-place: Austin, TX, USA},
	keywords = {Benchmark Suite, Deep Neural Network, GPU, cuDNN},
	pages = {63--72}
}

@article{yokoyama_survey_2019,
	title = {The survey on {ARM} processors for {HPC}},
	volume = {75},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-019-02911-9},
	doi = {10.1007/s11227-019-02911-9},
	abstract = {The ongoing effort to reach the exascale computing barrier has led to a myriad of research and publications in the topic of alternative energy-efficient architectures, such as ARM, for HPC systems. The staggering pace at which ARM architectures have evolved has increased the volume of publications on this topic even more. A complex subject as the race to exascale touches on several aspects such as floating-point performance, scalability issues in coupled workloads, net energy consumption and ratio of energy to performance. In this context, we see the opportunity to contribute to this subject by: (1) analyzing the state of the art to identify essential papers; (2) highlighting important developments of ARM architecture in support to HPC; (3) discussing both positive and negative trends observed regarding the use of ARM for HPC; and (4) listing key topics concerning the use of ARM for exascale computing, along with distinguished references for each one.},
	language = {en},
	number = {10},
	urldate = {2019-11-11},
	journal = {The Journal of Supercomputing},
	author = {Yokoyama, Daniel and Schulze, Bruno and Borges, Fábio and Mc Evoy, Giacomo},
	month = oct,
	year = {2019},
	keywords = {ARM, Energy efficiency, Exascale, Heterogeneous computing, High-performance computing},
	pages = {7003--7036}
}

@article{vieira_transactional_nodate,
	title = {Transactional {Memory} for {Heterogeneous} {CPU}-{GPU} {Systems}},
	language = {en},
	author = {Vieira, Ricardo Manuel Nunes},
	pages = {84}
}

@inproceedings{lopes_exploring_2017,
	address = {Santa Rosa, CA, USA},
	title = {Exploring {GPU} performance, power and energy-efficiency bounds with {Cache}-aware {Roofline} {Modeling}},
	isbn = {978-1-5386-3890-3},
	url = {http://ieeexplore.ieee.org/document/7975297/},
	doi = {10.1109/ISPASS.2017.7975297},
	language = {en},
	urldate = {2019-11-11},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software} ({ISPASS})},
	publisher = {IEEE},
	author = {Lopes, Andre and Pratas, Frederico and Sousa, Leonel and Ilic, Aleksandar},
	month = apr,
	year = {2017},
	pages = {259--268}
}

@article{cameirinha_exploiting_nodate,
	title = {Exploiting {DVFS} for {GPU} {Energy} {Management}},
	language = {en},
	author = {Cameirinha, Diogo},
	pages = {87}
}

@inproceedings{chang_learning-directed_2011,
	title = {Learning-{Directed} {Dynamic} {Voltage} and {Frequency} {Scaling} for {Computation} {Time} {Prediction}},
	doi = {10.1109/TrustCom.2011.140},
	abstract = {Dynamic voltage and frequency scaling (DVFS) is an effective technique for reducing power consumption. A number of DVFS researches apply learning methods in an attempt to approach the DVFS prediction model instead of using complicated mathematical models. In this paper, we propose a lightweight learning-directed DVFS technique using Counter Propagation Networks (CPN) to identify the task behavior and predict the corresponding voltage/frequency setting precisely. An adjustable performance mechanism is also provided to users that have diverse performance requirement. The algorithm has been implemented on the Linux operating system and used a PXA270 development board. The results show that the learning-directed DVFS method could accurately predict the suitable frequency, given runtime statistics information of a running program. In this way, the user can easily control the energy consumption by specifying allowable performance loss factor.},
	booktitle = {{2011IEEE} 10th {International} {Conference} on {Trust}, {Security} and {Privacy} in {Computing} and {Communications}},
	author = {Chang, Ming-Feng and Liang, Wen-Yew},
	month = nov,
	year = {2011},
	note = {ISSN: 2324-898X, 2324-9013},
	keywords = {CPN, Classification algorithms, DVFS, Embedded System, Energy consumption, Linux, Linux operating system, Low Power Software Design, PXA270 development board, Prediction algorithms, Radiation detectors, Time frequency analysis, Training, Vectors, computation time prediction, counter propagation networks, electronic engineering computing, learning directed dynamic voltage and frequency scaling, mathematical analysis, mathematical models, microprocessor chips, neural network, power aware computing, power consumption, statistical analysis, statistics information},
	pages = {1023--1029}
}

@article{nunez-yanez_energy_2019,
	title = {Energy {Proportional} {Neural} {Network} {Inference} with {Adaptive} {Voltage} and {Frequency} {Scaling}},
	volume = {68},
	issn = {0018-9340, 1557-9956, 2326-3814},
	doi = {10.1109/TC.2018.2879333},
	abstract = {This research presents the extension and application of a voltage and frequency scaling framework called Elongate to a high-performance and reconfigurable binarized neural network. The neural network is created in the FPGA reconfigurable fabric and coupled to a multiprocessor host that controls the operational point to obtain energy proportionality. Elongate instruments a design netlist by inserting timing detectors to enable the exploitation of the operating margins of a device reliably. The elongated neural network is re-targeted to devices with different nominal operating voltages and fabricated with 28 nm (i.e., Zynq) and 16nm (i.e., Zynq Ultrascale) feature sizes showing the portability of the framework to advanced process nodes. New hardware and software components are created to support the 16nm fabric microarchitecture and a comparison in terms of power, energy and performance with the older 28 nm process is performed. The results show that Elongate can obtain new performance and energy points that are up to 86 percent better than nominal at the same level of classification accuracy. Trade-offs between energy and performance are also possible with a large dynamic range of valid working points available. The results also indicate that the built-in neural network robustness allows operation beyond the first point of error while maintaining the classification accuracy largely unaffected.},
	number = {5},
	journal = {IEEE Transactions on Computers},
	author = {Nunez-Yanez, Jose},
	month = may,
	year = {2019},
	keywords = {Biological neural networks, Computer architecture, DVFS, Elongate instruments, FPGA, FPGA reconfigurable fabric, Field programmable gate arrays, Hardware, Performance evaluation, Program processors, Zynq Ultrascale, adaptive voltage, convolutional neural network, elongated neural network, energy efficiency, energy proportional neural network inference, energy proportionality, fabric microarchitecture, field programmable gate arrays, frequency scaling, inserting timing detectors, logic design, low-power electronics, multiprocessor host, neural nets, neural network robustness, reconfigurable binarized neural network, size 16.0 nm, size 28.0 nm, software components},
	pages = {676--687}
}

@article{you_quality_2015,
	title = {Quality of {Service}-{Aware} {Dynamic} {Voltage} and {Frequency} {Scaling} for {Embedded} {GPUs}},
	volume = {14},
	issn = {1556-6056, 1556-6064, 2473-2575},
	doi = {10.1109/LCA.2014.2319079},
	abstract = {Dynamic voltage and frequency scaling (DVFS) is a key technique for reducing processor power consumption in mobile devices. In recent years, mobile system-on-chips (SoCs) has supported DVFS for embedded graphics processing units (GPUs) as the processing power of embedded GPUs has been increasing steadlily. The major challenge of applying DVFS to a processing unit is to meet the quality of service (QoS) requirement while achieving a reasonable power reduction. In the case of GPUs, the QoS requirement can be specified as the frame-per-second (FPS) which the target GPU should achieve. The proposed DVFS technique ensures a consistent GPU performance by scaling the operating clock frequency in a way that it maintains a uniform FPS.},
	number = {1},
	journal = {IEEE Computer Architecture Letters},
	author = {You, Daecheol and Chung, Ki-Seok},
	month = jan,
	year = {2015},
	keywords = {Benchmark testing, Clocks, Correlation, DVFS, Energy consumption, Graphics processing units, Graphics processors, Quality of service, SoC, System-on-chip, dynamic voltage scaling, embedded GPU, energy-aware systems, frequency scaling, graphics processing unit, graphics processing units, hardware/software interfaces, low-power design, mobile device, mobile system-on-chips, operating clock frequency, power aware computing, processor power consumption, quality of service, system-on-chip},
	pages = {66--69}
}

@inproceedings{ahmad_green_2015,
	title = {Green smartphone {GPUs}: {Optimizing} energy consumption using {GPUFreq} scaling governors},
	shorttitle = {Green smartphone {GPUs}},
	doi = {10.1109/WiMOB.2015.7348036},
	abstract = {Modern smartphones are limited by their short battery life. The advancement of the graphical performance is considered as one of the main reasons behind the massive battery drainage in smartphones. In this paper we present a novel implementation of the GPUFreq Scaling Governors, a Dynamic Voltage and Frequency Scaling (DVFS) model implemented in the Android Linux kernel for dynamically scaling smartphone Graphical Processing Units (GPUs). The GPUFreq governors offer users multiple variations and alternatives in controlling the power consumption and performance of their GPUs. We implemented and evaluated our model on a smartphone GPU and measured the energy performance using an external power monitor. The results show that the energy consumption of smartphone GPUs can be significantly reduced with a minor effect on the GPU performance.},
	booktitle = {2015 {IEEE} 11th {International} {Conference} on {Wireless} and {Mobile} {Computing}, {Networking} and {Communications} ({WiMob})},
	author = {Ahmad, Enas and Shihada, Basem},
	month = oct,
	year = {2015},
	keywords = {Android Linux kernel, Batteries, CPUFreq Governors, DVFS, DVFS model, Energy Efficiency, Energy consumption, GPU, GPUFreq scaling governors, Graphics processing units, Kernel, Linux, Smartphones, Three-dimensional displays, battery life, dynamic voltage and frequency scaling model, energy consumption, energy measurement, graphical performance, graphical processing units, graphics processing units, green smartphone GPU, massive battery drainage, optimisation, power consumption, power monitor, smart phones, telecommunication control, telecommunication power management},
	pages = {740--747}
}

@inproceedings{nachiappan_domain_2015,
	title = {Domain knowledge based energy management in handhelds},
	doi = {10.1109/HPCA.2015.7056029},
	abstract = {Energy management in handheld devices is becoming a daunting task with the growing number of accelerators, increasing memory demands and high computing capacities required to support applications with stringent QoS needs. Current DVFS techniques that modulate power states of a single hardware component, or even recent proposals that manage multiple components, can lose out opportunities for attaining high energy efficiencies that may be possible by leveraging application domain knowledge. Thus, this paper proposes a coordinated multi-component energy optimization mechanism for handheld devices, where the energy profile of different components such as CPU, memory, GPU and IP cores are considered in unison to trigger the appropriate DVFS state by exploiting the application domain knowledge. Specifically, we show that for the important class of frame-based applications, the domain knowledge - frame processing rates, component utilization and available slack - can be used to decide effective DVFS states for each component from among the numerous choices. With such knowledge, rather than a brute force search of all speed setting choices, we propose two simpler heuristics, called Greedy policy and Kaldor-Hicks compensation policy, to make the decisions at frame boundaries. Our evaluations with 7 commonly-used Android apps show that our domain-aware coordinated DVFS policies have 23\% better energy efficiency than the conventionally used Android governors, and are within 9\% of an optimal policy that does not drop any frames.},
	booktitle = {2015 {IEEE} 21st {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Nachiappan, Nachiappan Chidambaram and Yedlapalli, Praveen and Soundararajan, Niranjan and Sivasubramaniam, Anand and Kandemir, Mahmut T. and Iyer, Ravi and Das, Chita R.},
	month = feb,
	year = {2015},
	note = {ISSN: 1530-0897, 2378-203X},
	keywords = {Android apps, Computer aided manufacturing, DVFS technique, Energy management, Games, Hardware, IP networks, Kaldor-Hicks compensation policy, Power measurement, QoS, System-on-chip, accelerator, coordinated multicomponent energy optimization, domain knowledge based energy management, energy conservation, energy efficiency, frame-based application, greedy policy, handheld device, notebook computers, power aware computing},
	pages = {150--160}
}

@article{price_optimizing_2016,
	title = {Optimizing performance per watt on {GPUs} in {High} {Performance} {Computing}: temperature, frequency and voltage effects},
	volume = {31},
	issn = {1865-2034, 1865-2042},
	shorttitle = {Optimizing performance per watt on {GPUs} in {High} {Performance} {Computing}},
	url = {http://arxiv.org/abs/1407.8116},
	doi = {10.1007/s00450-015-0300-5},
	abstract = {The magnitude of the real-time digital signal processing challenge attached to large radio astronomical antenna arrays motivates use of high performance computing (HPC) systems. The need for high power efficiency (performance per watt) at remote observatory sites parallels that in HPC broadly, where efficiency is an emerging critical metric. We investigate how the performance per watt of graphics processing units (GPUs) is affected by temperature, core clock frequency and voltage. Our results highlight how the underlying physical processes that govern transistor operation affect power efficiency. In particular, we show experimentally that GPU power consumption grows non-linearly with both temperature and supply voltage, as predicted by physical transistor models. We show lowering GPU supply voltage and increasing clock frequency while maintaining a low die temperature increases the power efficiency of an NVIDIA K20 GPU by up to 37-48\% over default settings when running xGPU, a compute-bound code used in radio astronomy. We discuss how temperature-aware power models could be used to reduce power consumption for future HPC installations. Automatic temperature-aware and application-dependent voltage and frequency scaling (T-DVFS and A-DVFS) may provide a mechanism to achieve better power efficiency for a wider range of codes running on GPUs},
	number = {4},
	urldate = {2019-10-23},
	journal = {Computer Science - Research and Development},
	author = {Price, D. C. and Clark, M. A. and Barsdell, B. R. and Babich, R. and Greenhill, L. J.},
	month = nov,
	year = {2016},
	note = {arXiv: 1407.8116},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {185--193}
}

@inproceedings{mei_energy_2017,
	title = {Energy efficient real-time task scheduling on {CPU}-{GPU} hybrid clusters},
	doi = {10.1109/INFOCOM.2017.8057205},
	abstract = {Conserving the energy consumption of large data centers is of critical significance, where a few percent in consumption reduction translates into millions-dollar savings. This work studies energy conservation on emerging CPU-GPU hybrid clusters through dynamic voltage and frequency scaling (DVFS). We aim at minimizing the total energy consumption of processing a sequence of real-time tasks under deadline constraints. We compute the appropriate voltage/frequency setting for each task through mathematical optimization, and assign multiple tasks to the cluster with heuristic scheduling algorithms. In performance evaluation driven by real-world power measurement traces, our scheduling algorithm shows comparable energy savings to the theoretical upper bound. With a GPU scaling interval where analytically at most 38\% of energy can be saved, we record 30-36\% of energy savings. Our results are applicable to energy management on modern heterogeneous clusters. In particular, our model stresses the nonlinear relationship between task execution time and processor speed for GPU-accelerated applications, for more accurately capturing real-world GPU energy consumption.},
	booktitle = {{IEEE} {INFOCOM} 2017 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Mei, Xinxin and Chu, Xiaowen and Liu, Hai and Leung, Yiu-Wing and Li, Zongpeng},
	month = may,
	year = {2017},
	keywords = {CPU-GPU hybrid clusters, Energy consumption, GPU energy consumption, GPU scaling interval, GPU-accelerated applications, Graphics processing units, Power demand, Runtime, Scheduling algorithms, Servers, data centers, dynamic voltage and frequency scaling, energy efficient real-time task scheduling, graphics processing units, heuristic scheduling algorithms, multiprocessing systems, performance evaluation, power aware computing, processor scheduling, real-world power measurement traces, task execution time, total energy consumption reduction, upper bound},
	pages = {1--9}
}

@article{calore_evaluation_2017,
	title = {Evaluation of {DVFS} techniques on modern {HPC} processors and accelerators for energy-aware applications},
	volume = {29},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4143},
	doi = {10.1002/cpe.4143},
	abstract = {Energy efficiency is becoming increasingly important for computing systems, in particular for large scale High Performance Computing (HPC) facilities. In this work, we evaluate, from a user perspective, the use of Dynamic Voltage and Frequency Scaling techniques, assisted by the power and energy monitoring capabilities of modern processors to tune applications for energy efficiency. We run selected kernels and a full HPC application on 2 high-end processors widely used in the HPC context, namely, an NVIDIA K80 GPU and an Intel Haswell CPU. We evaluate the available trade-offs between energy-to-solution and time-to-solution, attempting a function-by-function frequency tuning. We finally estimate the benefits obtainable running the full code on an HPC multi-GPU node, with respect to default clock frequency governors. We instrument our code to accurately monitor power consumption and execution time without the need of any additional hardware, and we enable it to change CPUs and GPUs clock frequencies while running. We analyze our results on the different architectures using a simple energy-performance model and derive a number of energy saving strategies, which can be easily adopted on recent high-end HPC systems for generic applications.},
	language = {en},
	number = {12},
	urldate = {2019-10-23},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Calore, Enrico and Gabbana, Alessandro and Schifano, Sebastiano Fabio and Tripiccione, Raffaele},
	year = {2017},
	keywords = {DVFS, GPU, HPC, application, energy-aware, user},
	pages = {e4143}
}

@inproceedings{akiki_energy-aware_2018,
	title = {Energy-{Aware} {Automatic} {Tuning} of {Many}-{Core} {Platform} via {Gradient} {Descent}},
	doi = {10.1109/SmartWorld.2018.00209},
	abstract = {Even though attaining high performance has been the user's pursuit traditionally, in the many-core era, the emphasis has shifted towards controlling the power and energy consumption, so as to maintain a satisfying performance while consuming an acceptable amount of energy. This paper describes an auto-tuning algorithm for the energy efficiency optimization of many-core platform, in this case, a Graphic Processing Unit (GPU). We employed gradient descent algorithm as the basis for this optimization. Metrics such as energy and energy delay product (EDP) are examined using programs representing different types of workloads such as sequential, parallel and hybrid. Based on the experimental results, our method achieves the level of savings over 15\% in terms of energy consumption when compared with the default on-board governors that also adjust the voltage and frequency of the GPU. Our approach shows an advantage when optimizing towards EDP as well. This shows the effectiveness of our proposed approach.},
	booktitle = {2018 {IEEE} {SmartWorld}, {Ubiquitous} {Intelligence} {Computing}, {Advanced} {Trusted} {Computing}, {Scalable} {Computing} {Communications}, {Cloud} {Big} {Data} {Computing}, {Internet} of {People} and {Smart} {City} {Innovation} ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	author = {Akiki, Samer and Yang, Zhiliu and Liu, Chen and Tang, Jie and Liu, Shaoshan},
	month = oct,
	year = {2018},
	keywords = {Energy-Aware Computing, Gradient Descent, Many core Processors, DVFS, Auto tuning, GPU, Graphic Processing Unit, auto-tuning algorithm, circuit optimisation, circuit tuning, energy conservation, energy consumption, energy delay product, energy efficiency optimization, energy-aware automatic tuning, gradient descent algorithm, gradient methods, graphics processing units, many-core platform, multiprocessing systems, optimization, power aware computing},
	pages = {1199--1203}
}

@inproceedings{he_online_2018,
	title = {Online {Demand} {Response} of {GPU} {Cloud} {Computing} with {DVFS}},
	doi = {10.1109/IWQoS.2018.8624136},
	abstract = {GPU cloud computing is emerging as a new type of cloud service that drives computation-extensive jobs, such as big data analytics and distributed machine learning. The introduction of GPU brings parallel processing power at the cost of excessive energy consumption. Dynamic Voltage and Frequency Scaling (DVFS) is a promising method to control energy consumption of GPU VMs. This work focuses on using DVFS to reduce energy of cloud computing in datacenter demand response. We first consider an online demand response scenario where users arrive stochastically, aiming at maximizing social welfare and meeting energy reduction goals by employing DVFS. We address the challenge posed by DVFS through a new technique of compact infinite optimization. A more practical scenario where both energy and resource limitations present is further studied. We design a primal-dual approximation algorithm that can compute a feasible solution in polynomial time with guaranteed approximation ratio, and a payment scheme that works in concert to form a truthful cloud job auction.},
	booktitle = {2018 {IEEE}/{ACM} 26th {International} {Symposium} on {Quality} of {Service} ({IWQoS})},
	author = {He, Yu and Ma, Lin and Huang, Chuanhe},
	month = jun,
	year = {2018},
	note = {ISSN: 1548-615X},
	keywords = {Cloud computing, DVFS, Energy consumption, GPU VMs, GPU cloud computing, Graphics processing units, Load management, Optimization, Parallel processing, Task analysis, approximation theory, big data analytics, cloud computing, cloud job auction, cloud service, computation-extensive jobs, computational complexity, computer centres, datacenter demand response, distributed machine learning, dynamic voltage-and-frequency scaling, energy consumption, energy reduction goals, graphics processing units, infinite optimization, learning (artificial intelligence), online demand response scenario, optimisation, parallel processing, parallel processing power, payment scheme, polynomial time, power aware computing, primal-dual approximation algorithm, resource limitations, social welfare},
	pages = {1--10}
}

@inproceedings{cha_core-level_2018,
	title = {Core-level {DVFS} for {Spatial} {Multitasking} {GPUs}},
	doi = {10.1109/TENCON.2018.8650072},
	abstract = {DVFS (Dynamic voltage frequency scaling) is one of the most widely used power management technologies employed to improve the performance or minimize the power consumption by controlling voltages and frequencies in real time. When applying device-level DVFS in graphics processing units (GPUs) that support spatial multitasking, it is difficult to determine the optimal DVFS status when multiple running kernels have different characteristics. To solve the problem, we created a GPU simulator that can operate at different streaming multiprocessor frequencies according to the characteristics of the assigned kernel and compared it with a single-clock-based spatial multitasking GPU simulator.},
	booktitle = {{TENCON} 2018 - 2018 {IEEE} {Region} 10 {Conference}},
	author = {Cha, Jehee and Kim, Jiho and Park, Yongjun},
	month = oct,
	year = {2018},
	note = {ISSN: 2159-3450, 2159-3442},
	keywords = {DVFS, GPU, GPU simulator, Generators, Graphics processing units, Kernel, Metadata, Multitasking, Registers, Task analysis, core-level DVFS, device-level DVFS, dynamic voltage frequency scaling, graphics processing unit, graphics processing units, multiprocessing systems, optimal DVFS status, power aware computing, power consumption, power management technologies, spatial multitasking GPUs, streaming multiprocessor frequencies},
	pages = {1525--1528}
}

@article{meinerzhagen_energy-efficient_2019,
	title = {An {Energy}-{Efficient} {Graphics} {Processor} in 14-nm {Tri}-{Gate} {CMOS} {Featuring} {Integrated} {Voltage} {Regulators} for {Fine}-{Grain} {DVFS}, {Retentive} {Sleep}, and \${V}\_{\textbackslash}{textMIN}\$ {Optimization}},
	volume = {54},
	issn = {0018-9200, 1558-173X},
	doi = {10.1109/JSSC.2018.2875097},
	abstract = {Graphics workloads make highly dynamic use of resources such as execution units (EUs), and thus can benefit from fast, fine-grain dynamic voltage and frequency scaling (DVFS) and retentive sleep. This paper presents a 14-nm graphics processing unit (GPU) prototype with modified EUs which include an integrated voltage regulator (IVR). The IVR enables energy-efficient EU turbo operation, data retention, and VMIN optimization per EU. Silicon measurements show that IVR-enabled EU turbo operation offers up to 32\% (average 29\%) energy reduction at constant performance.},
	number = {1},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Meinerzhagen, Pascal A. and Tokunaga, Carlos and Malavasi, Andrés and Vaidya, Vaibhav and Mendon, Ashwin and Mathaikutty, D. and Kulkarni, Jaydeep and Augustine, Charles and Cho, Minki and Kim, Stephen T. and Matthew, George E. and Jain, Rinkle and Ryan, Joseph and Peng, Chung-Ching and Paul, Somnath and Vangal, Sriram and Perez Esparza, Brando and Cuellar, L. and Woodman, M. and Iyer, Bala and Maiyuran, Subramaniam and Chinya, G. and Zou, Xiang and Liao, Yuyun and Ravichandran, Krishnan and Wang, H. and Khellah, Muhammad M. and Tschanz, James W. and De, Vivek},
	month = jan,
	year = {2019},
	keywords = {CMOS integrated circuits, Clocks, Dynamic scheduling, Energy-efficient graphics processing unit (GPU), Graphics processing units, Media, Optimization, Prototypes, VMIN optimization, Voltage control, circuit optimisation, data retention, energy conservation, energy-efficient graphics processor, execution units turbo operation, fine-grain DVFS, fine-grain dynamic voltage and frequency scaling (DVFS), fine-grain dynamic voltage scaling, frequency scaling, graphics processing unit, graphics processing units, integrated voltage regulator, integrated voltage regulators (IVRs), low-power electronics, power aware computing, retentive sleep, trigate CMOS, voltage optimization, voltage regulators},
	pages = {144--157}
}

@inproceedings{stavrinides_energy-aware_2018,
	title = {Energy-{Aware} {Scheduling} of {Real}-{Time} {Workflow} {Applications} in {Clouds} {Utilizing} {DVFS} and {Approximate} {Computations}},
	doi = {10.1109/FiCloud.2018.00013},
	abstract = {As cloud services become more ubiquitous, green cloud computing attracts significant attention from both academia and industry. Towards this direction, in this paper we propose an energy-aware heuristic for the scheduling of real-time workflow applications in a cloud environment. Our approach utilizes per-core Dynamic Voltage and Frequency Scaling (DVFS) on the underlying heterogeneous multi-core processors and approximate computations, in order to fill in schedule gaps. Our goal is to provide timeliness and energy efficiency by trading off result precision, while keeping the average result precision of the completed jobs at an acceptable level. The proposed scheduling heuristic is compared to two other baseline policies. The simulation experiments reveal that our approach outperforms the other examined policies, providing promising results. To the best of our knowledge, such a technique that combines per-core DVFS and approximate computations in order to utilize schedule gaps in a virtualized environment with real-time workflow applications has never been discussed in the literature before.},
	booktitle = {2018 {IEEE} 6th {International} {Conference} on {Future} {Internet} of {Things} and {Cloud} ({FiCloud})},
	author = {Stavrinides, Georgios L. and Karatza, Helen D.},
	month = aug,
	year = {2018},
	keywords = {Cloud computing, DVFS, Dynamic Voltage and Frequency Scaling, Energy efficiency, Job shop scheduling, Multicore processing, Processor scheduling, Program processors, Real-time systems, Task analysis, approximate computations, cloud computing, cloud environment, cloud services, clouds, energy efficiency, energy-aware scheduling, green cloud computing, heterogeneous multicore processors, multiprocessing systems, per-core DVFS, power aware computing, real-time workflow applications, real-time workflows, scheduling, scheduling heuristic},
	pages = {33--40}
}

@inproceedings{noauthor_energy-efficient_2019,
	title = {An energy-efficient, {QoS}-aware and cost-effective scheduling approach for real-time workflow applications in cloud computing systems utilizing {DVFS} and approximate computations},
	url = {https://reader.elsevier.com/reader/sd/pii/S0167739X18327353?token=3E4D6E67539D5839710EA3EBA7D76219220C733C3F3E7338AC38AA4BCF799A7ABADF217E5544FB83A92E8EF3C327DB61},
	doi = {10.1016/j.future.2019.02.019},
	abstract = {Greencloudcomputingattractssignificantattentionfrombothacademiaandindustry.Oneofthemajorchallenges involved, is to provide a high level of Quality of Service (QoS) in a cost-effective way for theend users and in an energy-efficient manner for the cloud providers. Towards this direction, this paperpresents an energy-efficient, QoS-aware and cost-effective scheduling strategy for real-time workflowapplicationsincloudcomputingsystems.Theproposedapproachutilizesper-coreDynamicVoltageandFrequencyScaling(DVFS)ontheunderlyingheterogeneousmulti-coreprocessors,aswellasapproximatecomputations, in order to fill in schedule gaps. At the same time, it takes into account the effects ofinputerrorontheprocessingtimeofthecomponenttasks.Ourgoalistoprovidetimelinessandenergyefficiency by trading off result precision, while keeping the result quality of the completed jobs at anacceptable standard and the monetary cost required for the execution of the jobs at a reasonable level.Theproposedschedulingheuristiciscomparedtotwootherbaselinepolicies,undertheimpactofvariousQoSrequirements.Thesimulationexperimentsrevealthatourapproachoutperformstheotherexaminedpolicies,providingpromisingresults},
	language = {en},
	urldate = {2019-10-23},
	month = jan,
	year = {2019},
	keywords = {Approximate Computations, Energy Efficiency, Per-Core DVFS, Quality of Service, Real-time workflows, Schedualing}
}

@inproceedings{koga_optimal_2014,
	title = {Optimal {Reliability} {Design} for {Real}-{Time} {Systems} with {Dynamic} {Voltage} and {Frequency} {Scaling}},
	doi = {10.1109/PRDC.2014.35},
	abstract = {In designing information communication devices such as real-time embedded systems, it is quite important to maximize the system performance under some hard energy constraints. As a useful technology to reduce the energy consumption in computer-based systems, the dynamic voltage and frequency scaling (DVFS) is becoming very popular. In this paper, we consider an optimal DVFS allocation problem by maximizing the system reliability subject to the hard real-time and energy constraints. More specifically, we formulate a reliability maximization problem when the task processing is probabilistic and is described by a discrete-time Markov chain. Two approximate formulas are proposed to calculate the system reliability efficiently. We perform the sensitivity analysis of model parameters in numerical examples, and also give a case study to design a Wi-Fi subsystem in terms of the DVFS allocation.},
	booktitle = {2014 {IEEE} 20th {Pacific} {Rim} {International} {Symposium} on {Dependable} {Computing}},
	author = {Koga, Toshitaka and Dohi, Tadashi and Okamura, Hiroyuki},
	month = nov,
	year = {2014},
	keywords = {Approximation methods, DTMC, DVFS, DVFS allocation, Energy consumption, Frequency modulation, Markov processes, Optimization, Real-time systems, Reliability, Resource management, Wi-Fi subsystem, discrete-time Markov chain, dynamic voltage and frequency scaling, energy constraint, energy constraints, green computing, hard real-time constraints, optimal reliability design, power aware computing, real-time system, real-time systems, reliability maximization, reliability maximization problem, sensitivity analysis, system reliability, wireless LAN},
	pages = {213--222}
}

@inproceedings{han_thermal-aware_2015,
	title = {Thermal-aware energy-efficient task scheduling for {DVFS}-enabled data centers},
	doi = {10.1109/ICCNC.2015.7069401},
	abstract = {In this paper we study the problem of optimal task scheduling that minimizes the computation and AC energy consumption of a data center. Different from existing studies that assume a linear relationship between computation power consumption and CPU frequency, our model considers a nonlinear cube-function computation power model. Compared with the linear model, this model better describes the behavior of the dynamic voltage and frequency scaling (DVFS) technology that has been widely supported by modern CPUs to improve their energy efficiency. Moreover, our optimization formulation explicitly accounts for the heterogeneous thermal correlation among different servers in the data center, so that tasks are carefully scheduled to offset the spatially-uneven temperature distribution caused by the heterogeneity of thermal correlation. This process makes the AC cooling efficiency better. We show that the energy-efficient task scheduling under the above settings can be formulated as a mixed-integer convex (MIC) optimization problem, in which approximate solution can be computed efficiently. Extensive simulations are conducted to verify the energy benefit of the proposed optimization by comparing with those proposed in previous studies.},
	booktitle = {2015 {International} {Conference} on {Computing}, {Networking} and {Communications} ({ICNC})},
	author = {Han, Dong and Shu, Tao},
	month = feb,
	year = {2015},
	keywords = {AC cooling efficiency, AC energy consumption, CPU frequency, Computational modeling, Correlation, DVFS-enabled data centers, Heating, Optimization, Power demand, Processor scheduling, Servers, computation power consumption, convex programming, dynamic voltage and frequency scaling technology, heterogeneous thermal correlation, integer programming, mixed-integer convex optimization problem, nonlinear cube-function computation power model, nonlinear functions, optimization formulation, power aware computing, thermal-aware energy-efficient task scheduling},
	pages = {536--540}
}
@inproceedings{huang_double-q_2017,
	title = {Double-{Q} {Learning}-{Based} {DVFS} for {Multi}-core {Real}-{Time} {Systems}},
	doi = {10.1109/iThings-GreenCom-CPSCom-SmartData.2017.83},
	abstract = {The Q-learning based DVFS selection algorithm has been used to lower energy consumption for system-level power management. However, this algorithm normally suffers from overestimation since it greedily uses maximum action value to approximate expected value, consequently fails to select the most appropriate DVFS method for real-time systems. In this article, we propose a Double-Q learning based DVFS selection algorithm to reduce energy consumption. In our scheme, instead of approximating actual action values with only one estimator, it implements Double-Q learning that applies two estimators to efficiently reduce overestimation, leading to an energy-aware scheme that can maintain a relatively stable and sufficient performance in action selection. We evaluate the performance of the proposed scheme through simulated data sets. Results demonstrate that our scheme can save more energy than the Q-learning based scheme while adapting to various system conditions and provide a more stable and accurate DVFS policy selection mechanism for multi-core real-time systems.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Internet} of {Things} ({iThings}) and {IEEE} {Green} {Computing} and {Communications} ({GreenCom}) and {IEEE} {Cyber}, {Physical} and {Social} {Computing} ({CPSCom}) and {IEEE} {Smart} {Data} ({SmartData})},
	author = {Huang, Hui and Lin, Man and Zhang, Qingchen},
	month = jun,
	year = {2017},
	keywords = {Approximation algorithms, DVFS, DVFS policy selection mechanism, DVFS selection algorithm, Double-Q Learning, Energy consumption, Heuristic algorithms, Power demand, Real-Time Systems, Real-time systems, Task analysis, Time-frequency analysis, action selection, approximate expected value, double-Q learning-based DVFS, energy consumption, energy-aware scheme, learning (artificial intelligence), maximum action value, multicore real-time systems, multiprocessing systems, power aware computing, real-time systems, system conditions, system-level power management},
	pages = {522--529}
}