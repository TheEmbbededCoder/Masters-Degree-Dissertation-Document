%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                      %
%     File: Thesis_Introduction.tex                                    %
%     Tex Master: Thesis.tex                                           %
%                                                                      %
%     Author: Francisco Mendes                                           %
%     Last modified :  31 Jul 2020                                      %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{chapter:introduction}

In the last few years, Deep Neural Networks (\acrshort{dnn}s) have had a significant impact in industry and society by allowing for important breakthroughs in many application domains, such as computer vision, speech recognition, natural language processing, drug discovery, genomics, etc \cite{shrestha_review_2019}.

However, \acrshort{dnn}s are usually characterized by significant computational burdens, particularly when considering the training of very deep and complex networks, dealing with high dimensional data, such as images and videos. For such purpose, researchers (and data scientists, in general) often rely on accelerators, such as Graphical Processing Units (\acrshort{gpu}s), to cope with the associated computational burden and reduce the training time. \acrshort{gpu}s differ from conventional processors by including thousands of computing cores (Compute Units - \acrshort{cu}s) and a large bandwidth memory module. As a result, they are able to execute the same instruction over massive amounts of data. Due to their versatility and compute power, \acrshort{gpu}s are now commonly deployed on most supercomputers, data centers, and other computational infrastructures related to artificial intelligence algorithms' development.



Additionally, several software frameworks, algorithms and techniques have been proposed to manage and optimize the execution of \acrshort{dnn} on \acrshort{gpu} (e.g., the work of Mittal~\cite{mittal_survey_2019}). However, most optimization techniques neglect the training phase's energy impact, usually resulting in considerable costs. 

To overcome this problem, researchers have also explored other solutions that allow mitigating the energy impact of neural network training. One particular and common approach relies on the use of low-precision arithmetic (e.g., demonstrated by Nabavinejad~ \cite{nabavinejad_coordinated_2019}), eventually trading network accuracy with increased processing performance and lower energy consumption.

Researchers have also looked at alternative approaches, such as exploiting Dynamic Voltage and Frequency Scaling (\acrshort{dvfs}) on both the inference and training phases. In fact, by carefully selecting the used voltage-frequency (V-F) levels, significant energy savings can be obtained, although depending on the considered \acrshort{dnn} architecture and computing  device~\cite{tang_impact_2019}. This is achieved through a careful balance between the different GPU components' performance and power consumption (particularly the core and global memory) to minimize stalls in the compute cores. In fact, not only can \acrshort{dvfs} be used to decrease the power consumption, but it can also boost the system performance~\cite{tang_impact_2019}, by increasing the voltage and frequency levels (as long as the GPU total power envelope and thermal limits are not surpassed).

Nevertheless, most state-of-the-art works only consider tightly coupled V-F levels, often predefined by \acrshort{gpu} manufacturers and neglecting the voltage margin that is usually introduced to guarantee fail-safe designs, as well as its variation with the kernel instruction sequence and the corresponding use of specific \acrshort{gpu} components. Supported on this observation, this work tries to increase the energy-efficiency of \acrshort{gpu}s by understanding and characterizing their behavior when subject to non-conventional V-F scaling. 
It also tries to go one step further by creating an optimization mechanism that automatically selects the V-F pair that better suits the running application as well as the specific characteristics of the computing device.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Objectives}
\label{section:objectives}

To uncover the use of non-conventional V-F scaling, this thesis focuses on the following objectives:

\begin{itemize}
\item Access the viability of using non-conventional V-F pairs on regular \acrshort{gpu}s.
\item Characterize the behaviour of the \acrshort{gpu} architecture to non-conventional V-F pairs.
\item Develop an automatic non-conventional V-F controlling and optimization mechanism that improves the \textit{performance}, \textit{energy consumption} or \textit{energy-efficiency} of \acrshort{gpu}s.
\item Safely apply non-conventional V-F scaling on Deep Learning applications, characterizing the behaviour of training procedure.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Main Contributions}
\label{section:main_contri}


\textcolor{red}{FALAR DO QUE FOI FEITO}

The scientiﬁc contributions of this work have been published for communication in the following conference:

\begin{itemize}
    \item F. Mendes, P. Tómas and N. Roma, "Exploiting non-conventional DVFS on GPUs: application to Deep Learning", IEEE 32nd International Symposium on Computer Architecture and High Performance Computing, 2020.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dissertation Outline}
\label{section:outline}

This dissertation is organized in 5 chapters with the following outline:
\begin{itemize}
    \item \textbf{Chapter 2 - Background:} This chapter presents a summary of the current state-of-the-art related to the subject in focus on this dissertation. First, it introduces an overview of general-purpose computing on \acrshort{gpu}s, providing a summary of their architecture and programming model. It presents the key concepts that should be present when developing for this kind of device and the standard programming interfaces between the \acrshort{gpu} and the host  \acrshort{cpu}. A bottom-up approach is drawn to present the fundamental concepts of frequency and voltage scaling on \acrshort{cmos} devices, and how those translate to the current \acrshort{dvfs} systems presented in out-of-the-shelf \acrshort{gpu}s. It finalizes by granting a sketch of how better exploring and going against the conventional voltage-frequency scaling allows for better energy efficiency of \acrshort{gpu}s, being that the stepping stone of this dissertation. 
    \item \textbf{Chapter 3 - GPU architectural characterization to decoupled V-F:} In this chapter, the methodology to characterize the use of non-conventional V-F scaling is presented. The chapter starts by exploring each developed stressing component benchmark's motivation and objectives, and it follows by testing the methodology in two different architecture \acrshort{gpu} devices. The methodology is used to: determine the minimum voltage allowed by each architectural component and evaluate the performance, energy consumption and energy efficiency of the devices, when prompting on them, non-conventional V-F pairs. It finishes by experimentally testing the effects of temperature on the undervoltage capabilities of the \acrshort{gpu} architecture.
    \item \textbf{Chapter 4 - V-F Optimization Mechanism:} This chapter analyses and proposes a solution to dynamically adjust frequency and voltage using the newly tested non-conventional V-F pairs. Programmers can execute their algorithms using this tool, while the most energy-efficient V-F configuration is being discovered, targetting the current \acrshort{gpu} state and performance metrics, and application output.
    \item \textbf{Chapter 5 - Application to Deep Learning:} In this chapter, the previous two chapters' results are combined and used to perform an application for deep learning. This chapter as three objectives: showcase the practical advantages of extending the default frequency-voltage scaling with non-conventional V-F pairs on a target application; evaluate the use of the developed optimization mechanism, and, overall, the chapter indicates how a user should proceed to safely and beneficially use the work developed in this dissertation.
    \item \textbf{Chapter 6 - Conclusions:} This final chapter presents the accomplished results from this work and the possible directions to take in future work.
\end{itemize}

